{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrate from `v7.8.0` to `v8.0.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### 1. Determine impacted Mongo collections\n",
    "\n",
    "Determine which Mongo collections will be transformed during this migration.\n",
    "\n",
    "That involves reading each transformation function written specifically for this migration—[in the `nmdc-schema` repository](https://github.com/microbiomedata/nmdc-schema/blob/main/nmdc_schema/migration_recursion.py); checking the object it returns; and mapping that to a Mongo collection (e.g. a dictionary named `transformed_study` → the Mongo collection named `study_set`).\n",
    "\n",
    "In this case, those Mongo collections are:\n",
    "\n",
    "- `extraction_set`\n",
    "- `omics_processing_set`\n",
    "- `biosample_set`\n",
    "- `study_set`\n",
    "\n",
    "> Note: The list above is specific to this migration.\n",
    "\n",
    "### 2. Determine impacted system components\n",
    "\n",
    "Determine which components of the NMDC system will be impacted by the transformations.\n",
    "\n",
    "That involves mapping each Mongo collection listed in the previous step, to the component of the NMDC system that write to it.\n",
    "\n",
    "For reference, here's a table of Mongo collections and the components of the NMDC system that write to them (according to [a conversation that occurred on September 11, 2023](https://nmdc-group.slack.com/archives/C01SVTKM8GK/p1694465755802979?thread_ts=1694216327.234519&cid=C01SVTKM8GK)):\n",
    "\n",
    "| Mongo collection                            | Components that writes to it                             |\n",
    "|---------------------------------------------|----------------------------------------------------------|\n",
    "| `biosample_set`                             | Workflows (via manual entry via `nmdc-runtime` HTTP API) |\n",
    "| `data_object_set`                           | Workflows (via `nmdc-runtime` HTTP API)                  |\n",
    "| `mags_activity_set`                         | Workflows (via `nmdc-runtime` HTTP API)                  |\n",
    "| `metagenome_annotation_activity_set`        | Workflows (via `nmdc-runtime` HTTP API)                  |\n",
    "| `metagenome_assembly_set`                   | Workflows (via `nmdc-runtime` HTTP API)                  |\n",
    "| `read_based_taxonomy_analysis_activity_set` | Workflows (via `nmdc-runtime` HTTP API)                  |\n",
    "| `read_qc_analysis_activity_set`             | Workflows (via `nmdc-runtime` HTTP API)                  |\n",
    "| `jobs`                                      | Scheduler (via Mongo directly)                           |\n",
    "| `*`                                         | `nmdc-runtime` (via Mongo directly)                      |\n",
    "\n",
    "> Note: The table above is not specific to any given migration. It may still change over time, though.\n",
    "\n",
    "In this case, those parts of the NMDC system are:\n",
    "\n",
    "- Workflows (due to `biosample_set`)\n",
    "- `nmdc-runtime` (due to `*`)\n",
    "\n",
    "### 3. Coordinate with component owners\n",
    "\n",
    "Coordinate with the owners of the impacted components of the NMDC system.\n",
    "\n",
    "TODO: Elaborate on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup the environment\n",
    "\n",
    "1. Start a MongoDB server on your local machine (or in a Docker container) and ensure it does **not** contain a database named `nmdc`.\n",
    "2. Create and populate a **notebook configuration file** named `.notebook.env`.\n",
    "    1. You can use the `.notebook.env.example` file as a template:\n",
    "       ```shell\n",
    "       $ cp .notebook.env.example .notebook.env\n",
    "       ```\n",
    "3. Create and populate **MongoDB configuration files** for connecting to the origin and transformer MongoDB servers.\n",
    "    1. You can use the `.mongo.yaml.example` file as a template:\n",
    "       ```shell\n",
    "       $ cp .mongo.yaml.example .mongo.origin.yaml\n",
    "       $ cp .mongo.yaml.example .mongo.transformer.yaml\n",
    "       ```\n",
    "       > When populating the file for the origin MongoDB server, use root credentials since this notebook will be manipulating user roles on that server. You can get those root credentials from Rancher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "Install the third-party Python packages upon which this notebook depends.\n",
    "\n",
    "Reference: https://saturncloud.io/blog/what-is-the-difference-between-and-in-jupyter-notebooks/\n",
    "\n",
    "> Note: You may need to restart the notebook kernel to use updated packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%pip install nmdc-schema==8.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Python objects upon which this notebook depends.\n",
    "\n",
    "> Note: You may need to restart the notebook kernel to use updated packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library packages:\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "from shutil import rmtree\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "# Third-party packages:\n",
    "import pymongo\n",
    "from nmdc_schema.migration_recursion import Migrator\n",
    "\n",
    "# First-party packages:\n",
    "from helpers import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse configuration files\n",
    "\n",
    "Parse the notebook and MongoDB configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "\n",
    "# Define some aliases we can use to make the shell commands in this notebook easier to read.\n",
    "mongodump = cfg.mongodump_path\n",
    "mongorestore = cfg.mongorestore_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MongoDB clients\n",
    "\n",
    "Create MongoDB clients you can use to access the \"origin\" MongoDB server (i.e. the one containing the database you want to migrate) and the \"transformer\" MongoDB server (i.e. the one you want to use to perform the data transformations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB client for origin MongoDB server.\n",
    "origin_mongo_client = pymongo.MongoClient(host=cfg.origin_mongo_server_uri, directConnection=True)\n",
    "\n",
    "# MongoDB client for transformer MongoDB server.\n",
    "transformer_mongo_client = pymongo.MongoClient(host=cfg.transformer_mongo_server_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back up user data from origin MongoDB server\n",
    "\n",
    "Before temporarily disabling write access to the `nmdc` database on the origin MongoDB server, we will back up the current user data.\n",
    "\n",
    "> That way, in case something goes wrong later, we can refer to this backup (e.g. to manually restore the original access levels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result: dict = origin_mongo_client[\"admin\"].command(\"usersInfo\")\n",
    "users_initial = result[\"users\"]\n",
    "\n",
    "# Create temporary file in the notebook's folder, containing the initial users.\n",
    "users_file = NamedTemporaryFile(delete=False, dir=str(Path.cwd()), prefix=\"tmp.origin_users_initial.\")\n",
    "users_file.write(bytes(pformat(users_initial), \"utf-8\"))\n",
    "users_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable non-admin writing to the origin database\n",
    "\n",
    "To disable non-admin writing to the `nmdc` database on the origin MongoDB server, we will set all users' roles (except the root user) to `read` (i.e. read-only) with respect to that database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users_initial:\n",
    "\n",
    "    break  # Abort! TODO: Remove me when I'm ready to run this notebook for real.\n",
    "\n",
    "    if any((role[\"db\"] == \"nmdc\") for role in user[\"roles\"]):\n",
    "        origin_mongo_client[\"admin\"].command(\"grantRolesToUser\", user[\"user\"], roles=[{ \"role\": \"read\", \"db\": \"nmdc\" }])\n",
    "        origin_mongo_client[\"admin\"].command(\"revokeRolesFromUser\", user[\"user\"], roles=[{ \"role\": \"readWrite\", \"db\": \"nmdc\" }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump those collections from the database on the origin MongoDB server\n",
    "\n",
    "In a previous step, you determined which collections would be transformed during this migration.\n",
    "\n",
    "Here, you'll dump those collections from the database on the origin MongoDB server.\n",
    "\n",
    "Since `mongodump` doesn't offer an option to specify more than one collection to dump (it's either one—via the `--collection` option—or all), we use its `--excludeCollection` option multiple times to specify all the collections we _don't_ want to dump.\n",
    "\n",
    "> If we accidentally dump more collections than necessary; the dump process will take longer than necessary and the dump files will, together, take up more space than necessary. You may be OK with that (I am).\n",
    "\n",
    "You can get the full list of collections by running the following commands on the origin Mongo server: \n",
    "```shell\n",
    "use nmdc;\n",
    "db.getCollectionNames();\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date\n",
    "\n",
    "# Dump collections from the origin database.\n",
    "#\n",
    "# I removed these options from the command because I want to dump these collections.\n",
    "#   --excludeCollection='biosample_set' \\\n",
    "#   --excludeCollection='study_set' \\\n",
    "#   --excludeCollection='omics_processing_set' \\\n",
    "#   --excludeCollection='extraction_set' \\\n",
    "#\n",
    "!{mongodump} \\\n",
    "  --config=\"{cfg.origin_mongo_config_file_path}\" \\\n",
    "  --db=\"nmdc\" \\\n",
    "  --gzip \\\n",
    "  --excludeCollection='notes' \\\n",
    "  --excludeCollection='ids_nmdc_fk0' \\\n",
    "  --excludeCollection='fs.files' \\\n",
    "  --excludeCollection='schema_classes' \\\n",
    "  --excludeCollection='capabilities' \\\n",
    "  --excludeCollection='_runtime.api.allow' \\\n",
    "  --excludeCollection='minter.id_records' \\\n",
    "  --excludeCollection='run_events' \\\n",
    "  --excludeCollection='query_runs' \\\n",
    "  --excludeCollection='metagenome_assembly_set' \\\n",
    "  --excludeCollection='object_types' \\\n",
    "  --excludeCollection='metabolomics_analysis_activity_set' \\\n",
    "  --excludeCollection='ids_nmdc_mga0' \\\n",
    "  --excludeCollection='read_qc_analysis_activity_set' \\\n",
    "  --excludeCollection='date_created' \\\n",
    "  --excludeCollection='pooling_set' \\\n",
    "  --excludeCollection='field_research_site_set' \\\n",
    "  --excludeCollection='typecodes' \\\n",
    "  --excludeCollection='read_QC_analysis_activity_set' \\\n",
    "  --excludeCollection='ids_nmdc_sys0' \\\n",
    "  --excludeCollection='material_sample_set' \\\n",
    "  --excludeCollection='triggers' \\\n",
    "  --excludeCollection='nom_analysis_activity_set' \\\n",
    "  --excludeCollection='system.views' \\\n",
    "  --excludeCollection='sites' \\\n",
    "  --excludeCollection='fs.chunks' \\\n",
    "  --excludeCollection='jobs' \\\n",
    "  --excludeCollection='functional_annotation_agg' \\\n",
    "  --excludeCollection='ids_nmdc_gfs0' \\\n",
    "  --excludeCollection='minter.requesters' \\\n",
    "  --excludeCollection='ids' \\\n",
    "  --excludeCollection='workflows' \\\n",
    "  --excludeCollection='requesters' \\\n",
    "  --excludeCollection='operations' \\\n",
    "  --excludeCollection='etl_software_version' \\\n",
    "  --excludeCollection='read_based_analysis_activity_set' \\\n",
    "  --excludeCollection='processed_sample_set' \\\n",
    "  --excludeCollection='minter.schema_classes' \\\n",
    "  --excludeCollection='metap_gene_function_aggregation' \\\n",
    "  --excludeCollection='read_based_taxonomy_analysis_activity_set' \\\n",
    "  --excludeCollection='_tmp__get_file_size_bytes' \\\n",
    "  --excludeCollection='users' \\\n",
    "  --excludeCollection='ids_nmdc_fk4' \\\n",
    "  --excludeCollection='queries' \\\n",
    "  --excludeCollection='metaproteomics_analysis_activity_set' \\\n",
    "  --excludeCollection='txn_log' \\\n",
    "  --excludeCollection='shoulders' \\\n",
    "  --excludeCollection='activity_set' \\\n",
    "  --excludeCollection='library_preparation_set' \\\n",
    "  --excludeCollection='page_tokens' \\\n",
    "  --excludeCollection='data_object_set' \\\n",
    "  --excludeCollection='mags_activity_set' \\\n",
    "  --excludeCollection='metagenome_annotation_activity_set' \\\n",
    "  --excludeCollection='file_type_enum' \\\n",
    "  --excludeCollection='id_records' \\\n",
    "  --excludeCollection='metatranscriptome_activity_set' \\\n",
    "  --excludeCollection='metagenome_sequencing_activity_set' \\\n",
    "  --excludeCollection='collecting_biosamples_from_site_set' \\\n",
    "  --excludeCollection='services' \\\n",
    "  --excludeCollection='nmdc_schema_version' \\\n",
    "  --excludeCollection='objects' \\\n",
    "  --excludeCollection='ids_nmdc_mta0' \\\n",
    "  --excludeCollection='minter.services' \\\n",
    "  --excludeCollection='_runtime.healthcheck' \\\n",
    "  --excludeCollection='minter.typecodes' \\\n",
    "  --excludeCollection='minter.shoulders' \\\n",
    "  --excludeCollection='EMP_soil_project_run_counts' \\\n",
    "  --out=\"{cfg.origin_dump_folder_path}\"\n",
    "\n",
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the dump into the transformer MongoDB server\n",
    "\n",
    "Load the collections contained in the dump (dumped from the origin MongoDB server) into the transformer MongoDB server, so we can start transforming them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the dumped collections to the transformer MongoDB server.\n",
    "!{mongorestore} \\\n",
    "  --config=\"{cfg.transformer_mongo_config_file_path}\" \\\n",
    "  --gzip \\\n",
    "  --nsInclude=\"nmdc.extraction_set\" \\\n",
    "  --nsInclude=\"nmdc.omics_processing_set\" \\\n",
    "  --nsInclude=\"nmdc.biosample_set\" \\\n",
    "  --nsInclude=\"nmdc.study_set\" \\\n",
    "  --drop --preserveUUID \\\n",
    "  --dir=\"{cfg.origin_dump_folder_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the database\n",
    "\n",
    "Now that the transformer database contains a copy of each relevant collection from the origin database, we can transform those copies.\n",
    "\n",
    "The transformation functions are provided by the `nmdc-schema` Python package.\n",
    "> You can examine the transformation functions at: https://github.com/microbiomedata/nmdc-schema/blob/main/nmdc_schema/migration_recursion.py\n",
    "\n",
    "In this step, we will retrieve each documents from each collection, pass it to a transformation function(s), then store the transformed document in place of the original one—all within the transformation database only. **The origin database is not involved with this step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migrator = Migrator()\n",
    "\n",
    "# Define a mapping from collection name to transformation function(s).\n",
    "# TODO: Consider defining this mapping in the `nmdc-schema` repository/package instead.\n",
    "transformation_pipelines = dict(\n",
    "    extraction_set=[migrator.migrate_extractions_7_8_0_to_8_0_0],\n",
    "    omics_processing_set=[migrator.migrate_uc_gold_sequencing_project_identifiers_7_8_0_to_8_0_0],\n",
    "    biosample_set=[migrator.migrate_uc_gold_biosample_identifiers_7_8_0_to_8_0_0],\n",
    "    study_set=[migrator.migrate_uc_gold_study_identifiers_7_8_0_to_8_0_0],\n",
    ")\n",
    "\n",
    "# Apply the transformations.\n",
    "for collection_name, transformation_pipeline in transformation_pipelines.items():\n",
    "    print(f\"Transforming documents in collection: {collection_name}\")\n",
    "    transformed_documents = []\n",
    "\n",
    "    # Get each document from this collection.\n",
    "    collection = transformer_mongo_client[\"nmdc\"][collection_name]\n",
    "    for original_document in collection.find():\n",
    "        \n",
    "        # Put the document through the transformation pipeline associated with this collection.\n",
    "        print(original_document)\n",
    "        transformed_document = original_document  # initializes the variable\n",
    "        for transformation_function in transformation_pipeline:\n",
    "            transformed_document = transformation_function(transformed_document)\n",
    "\n",
    "        # Store the transformed document.\n",
    "        print(transformed_document)\n",
    "        print(\"\")\n",
    "        transformed_documents.append(transformed_document)\n",
    "\n",
    "    # Replace the original documents with the transformed versions of themselves (in the transformer database).\n",
    "    for transformed_document in transformed_documents:\n",
    "        collection.replace_one({\"id\": {\"$eq\": transformed_document[\"id\"]}}, transformed_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the transformed database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump the transformed database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the database from the transformer MongoDB server.\n",
    "!{mongodump} \\\n",
    "  --config=\"{cfg.transformer_mongo_config_file_path}\" \\\n",
    "  --db=\"nmdc\" \\\n",
    "  --gzip \\\n",
    "  --out=\"{cfg.transformer_dump_folder_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the transformed data into the origin MongoDB server\n",
    "\n",
    "Ensure the command below includes an [`--nsInclude` option](https://www.mongodb.com/docs/database-tools/mongorestore/#std-option-mongorestore.--nsInclude) for each transformed collection.\n",
    "\n",
    "In this step, you'll put the transformed collection(s) into the origin MongoDB server, replacing the original collection(s) that has/have the same name(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date\n",
    "\n",
    "# Replace the same-named collection(s) on the origin server, with the transformed one(s).\n",
    "!{mongorestore} \\\n",
    "  --config=\"{cfg.origin_mongo_config_file_path}\" \\\n",
    "  --gzip \\\n",
    "  --verbose \\\n",
    "  --dir=\"{cfg.transformer_dump_folder_path}\" \\\n",
    "  --nsInclude=\"nmdc.extraction_set\" \\\n",
    "  --nsInclude=\"nmdc.omics_processing_set\" \\\n",
    "  --nsInclude=\"nmdc.biosample_set\" \\\n",
    "  --nsInclude=\"nmdc.study_set\" \\\n",
    "  --drop --preserveUUID\n",
    "\n",
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've restored the database, we'll restore the original user roles (with respect to the `nmdc` database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users_initial:\n",
    "\n",
    "    break  # Abort! TODO: Remove me when I'm ready to run this notebook for real.\n",
    "\n",
    "    if any((role[\"db\"] == \"nmdc\" and role[\"role\"] == \"readWrite\") for role in user[\"roles\"]):\n",
    "        origin_mongo_client[\"admin\"].command(\"grantRolesToUser\", user[\"user\"], roles=[{ \"role\": \"readWrite\", \"db\": \"nmdc\" }])\n",
    "        origin_mongo_client[\"admin\"].command(\"revokeRolesFromUser\", user[\"user\"], roles=[{ \"role\": \"read\", \"db\": \"nmdc\" }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Clean up\n",
    "\n",
    "Delete the temporary files and MongoDB dumps created by this notebook.\n",
    "\n",
    "> Note: You can skip this step, in case you want to delete them manually later (e.g. to examine them before deleting them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_to_files_to_delete = [\n",
    "    users_file.name,\n",
    "]\n",
    "\n",
    "paths_to_folders_to_delete = [\n",
    "    cfg.origin_dump_folder_path,\n",
    "    cfg.transformer_dump_folder_path,\n",
    "]\n",
    "\n",
    "# Delete files.\n",
    "for path in [Path(string) for string in paths_to_files_to_delete]:\n",
    "    try:\n",
    "        path.unlink()\n",
    "        print(f\"Deleted: {path}\")\n",
    "    except:\n",
    "        print(f\"Failed to delete: {path}\")\n",
    "\n",
    "# Delete folders.\n",
    "for path in [Path(string) for string in paths_to_folders_to_delete]:\n",
    "    try:\n",
    "        rmtree(path)\n",
    "        print(f\"Deleted: {path}\")\n",
    "    except:\n",
    "        print(f\"Failed to delete: {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
