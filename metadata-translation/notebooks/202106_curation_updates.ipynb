{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14532162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f239fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env.localhost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import dagster\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=dagster.ExperimentalWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmdc_runtime.site.repository import run_config_frozen__normal_env\n",
    "from nmdc_runtime.site.resources import get_mongo\n",
    "\n",
    "\n",
    "mongo = get_mongo(run_config_frozen__normal_env)\n",
    "mdb = mongo.db\n",
    "#set(db.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf275fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmdc_runtime.site.resources import get_runtime_api_site_client\n",
    "\n",
    "\n",
    "client = get_runtime_api_site_client(run_config_frozen__normal_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmdc_runtime.util import nmdc_jsonschema, nmdc_jsonschema_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_etl_latest = mdb.objects.find_one({\"name\": \"nmdc_database.json.zip\"}, sort=[(\"created_time\", -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_etl_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74994459",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = client.get_object_bytes(gold_etl_latest[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bceb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(BytesIO(rv.content)) as myzip:\n",
    "    # may be e.g. 'metadata-translation/src/bin/output/nmdc_database.json' rather than 'nmdc_database.json'\n",
    "    name = next(n for n in myzip.namelist() if n.endswith(\"nmdc_database.json\"))\n",
    "    with myzip.open(name) as f:\n",
    "        nmdc_database = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdaa2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "nmdc_db_collection_names_to_drop = set(nmdc_jsonschema[\"definitions\"][\"Database\"][\"properties\"])\n",
    "nmdc_db_collection_names_to_drop -= {\n",
    "    # not actually collections\n",
    "    \"activity_set\",\n",
    "    \"nmdc_schema_version\",\n",
    "    \"date_created\",\n",
    "    \"etl_software_version\",\n",
    "    # big collections, loaded elsewhere\n",
    "    \"functional_annotation_set\",\n",
    "    \"genome_feature_set\",\n",
    "    \n",
    "}\n",
    "pprint(nmdc_db_collection_names_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_database(db, coll_names):\n",
    "    for coll_name in coll_names:\n",
    "        print(f\"dropping {coll_name}, creating index\")\n",
    "        db.drop_collection(coll_name)\n",
    "        db[coll_name].create_index(\"id\", unique=True)\n",
    "        \n",
    "init_database(db, nmdc_db_collection_names_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d2214",
   "metadata": {},
   "source": [
    "# fix biosample.part_of and add docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from toolz import assoc_in, dissoc, get_in\n",
    "\n",
    "from nmdc_runtime.api.core.util import pick\n",
    "\n",
    "new_docs = []\n",
    "for doc in nmdc_database[\"biosample_set\"]:\n",
    "    if \"part of\" in doc:\n",
    "        doc = assoc_in(doc, [\"part_of\"], get_in([\"part of\"], doc))\n",
    "        doc = dissoc(doc, \"part of\")\n",
    "    new_docs.append(doc)\n",
    "\n",
    "nmdc_database[\"biosample_set\"] = new_docs\n",
    "len(nmdc_database[\"biosample_set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = nmdc_jsonschema_validate(nmdc_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc0b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = mongo.add_docs(nmdc_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv['biosample_set'].upserted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f168df84",
   "metadata": {},
   "source": [
    "# GOLD IDs to IGSNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from dagster import build_solid_context\n",
    "\n",
    "# from nmdc_runtime.site.ops import local_file_to_api_object as lftao\n",
    "\n",
    "# context = build_solid_context(resources={\"mongo\": mongo, \"runtime_api_site_client\": client})\n",
    "\n",
    "\n",
    "# def local_file_to_api_object(file_info):\n",
    "#     return lftao(context, file_info)\n",
    "\n",
    "# storage_path = os.path.expanduser(\n",
    "#     \"~/Dropbox/repos/nmdc/nmdc-runtime/metadata-translation/src/data/\"\n",
    "#     \"2020-23-12-brodie-Gs0135149-soil-metadata.csv\"\n",
    "# )\n",
    "# obj = next(local_file_to_api_object({\"storage_path\": storage_path, \"mime_type\": 'text/csv'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe7f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "oid__gold_ids_to_igns = \"sys09398\"\n",
    "doc = mdb.objects.find_one({\"id\": oid__gold_ids_to_igns})\n",
    "assert doc[\"name\"] == \"2020-23-12-brodie-Gs0135149-soil-metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "GOLD_ID_IDX = 5\n",
    "IGSN_IDX = 2\n",
    "\n",
    "igsn_golds = defaultdict(list)\n",
    "\n",
    "gold_id_pattern = re.compile(r\"Gb\\d+\")\n",
    "\n",
    "f = StringIO(client.get_object_bytes(oid__gold_ids_to_igns).text)\n",
    "reader = csv.reader(f)\n",
    "for row in reader:\n",
    "    gold_id = row[GOLD_ID_IDX]\n",
    "    igsn = row[IGSN_IDX]\n",
    "    if gold_id_pattern.fullmatch(gold_id):\n",
    "        igsn_golds[igsn].append(gold_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88525c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "igsn_golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import assoc_in, dissoc\n",
    "\n",
    "new_biosample_docs = []\n",
    "\n",
    "for igsn, golds in igsn_golds.items():\n",
    "    igsn_curie = \"igsn:\"+igsn\n",
    "    doc = mdb.biosample_set.find_one({\"id\": igsn_curie})\n",
    "    doc = assoc_in(doc, [\"alternative_identifiers\"], [f\"gold:{g}\" for g in golds])\n",
    "    doc = dissoc(doc, \"_id\")\n",
    "    new_biosample_docs.append(doc)\n",
    "    \n",
    "rv = mongo.add_docs({\"biosample_set\": new_biosample_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31207f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO put [f\"gold:{g}\" for g in golds] in alternative_identifiers field\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from toolz import get_in\n",
    "\n",
    "new_biosample_docs = []\n",
    "\n",
    "for igsn, golds in igsn_golds.items():\n",
    "    igsn_curie = \"igsn:\"+igsn\n",
    "    doc = db.biosample_set.find_one({\"id\": {\"$in\": [f\"gold:{g}\" for g in golds]}})\n",
    "    if doc is None:\n",
    "        print(igsn, golds)\n",
    "        continue\n",
    "    doc = assoc_in(doc, [\"id\"], igsn_curie)\n",
    "    doc = dissoc(doc, \"_id\")\n",
    "    new_biosample_docs.append(doc)\n",
    "\n",
    "assert len(new_biosample_docs) == len(igsn_golds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo.add_docs({\"biosample_set\": new_biosample_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd7dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import DeleteMany\n",
    "from toolz import concat\n",
    "\n",
    "requests = [DeleteMany({\"id\": {\"$in\": [\"gold:\"+g for g in concat(igsn_golds.values())]}})]\n",
    "rv = mongo.db.biosample_set.bulk_write(requests)\n",
    "rv.deleted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce66e9c3",
   "metadata": {},
   "source": [
    "# Update omics_processing_set references to biosample_set ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749de37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldid_igsn = {}\n",
    "for igsn, gids in igsn_golds.items():\n",
    "    for gid in gids:\n",
    "        goldid_igsn[gid] = igsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ebce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = []\n",
    "to_replace = {\"gold:\"+k: \"igsn:\"+v for k, v in goldid_igsn.items()}\n",
    "\n",
    "for doc in db.omics_processing_set.find({\"has_input\": {\"$in\": list(to_replace)}}):\n",
    "    operations = {\"$set\": {\n",
    "        \"has_input\": [to_replace.get(i, i) for i in doc[\"has_input\"]],\n",
    "    }}\n",
    "    requests.append({\"filter\": {\"_id\": doc[\"_id\"]}, \"update\": operations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import UpdateOne\n",
    "\n",
    "rv = db.omics_processing_set.bulk_write([UpdateOne(**r) for r in requests])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv.modified_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [dissoc(doc, \"_id\") for doc in db.omics_processing_set.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = nmdc_jsonschema_validate({\"omics_processing_set\": docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e887f0c",
   "metadata": {},
   "source": [
    "# Update omics_processing_set references from EMSL ids to IGSNs\n",
    "\n",
    "Skip this -- it updates zero documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83debad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMSL_IDS_IDX = 7\n",
    "IGSN_IDX = 2\n",
    "\n",
    "igsn_emsls = {}\n",
    "\n",
    "emsl_ids_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "f = StringIO(client.get_object_bytes(oid__gold_ids_to_igns).text)\n",
    "reader = csv.reader(f)\n",
    "for row in reader:\n",
    "    emsl_ids = row[EMSL_IDS_IDX]\n",
    "    igsn = row[IGSN_IDX]\n",
    "    ids = emsl_ids_pattern.findall(emsl_ids)\n",
    "    # XXX some rows have emsl ids but no IGSN, so igsn.strip() check here\n",
    "    if igsn.strip() and ids:\n",
    "        igsn_emsls[igsn] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "emslid_igsn = {}\n",
    "for igsn, eids in igsn_emsls.items():\n",
    "    for eid in eids:\n",
    "        emslid_igsn[eid] = igsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_with_emsl_id = db.omics_processing_set.count_documents(\n",
    "    {\"id\": {\"$in\": [\"emsl:\"+i for i in emslid_igsn]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e81c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = []\n",
    "to_replace = {\"emsl:\"+k: \"igsn:\"+v for k, v in emslid_igsn.items()}\n",
    "to_replace.update({\"emsl:output_\"+k: \"igsn:\"+v for k, v in emslid_igsn.items()})\n",
    "\n",
    "def omit(blacklist, d):\n",
    "    return keyfilter(lambda k: k not in blacklist, d)\n",
    "\n",
    "def sans_mongo_id(d):\n",
    "    return omit([\"_id\"], d)\n",
    "\n",
    "\n",
    "for doc in db.omics_processing_set.find({\"has_input\": {\"$in\": list(to_replace)}}):\n",
    "    operations = {\"$set\": {\n",
    "        \"has_input\": [to_replace.get(i, i) for i in doc[\"has_input\"]],\n",
    "    }}\n",
    "    requests.append({\"filter\": {\"_id\": doc[\"_id\"]}, \"update\": operations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if requests:\n",
    "    rv = db.omics_processing_set.bulk_write([UpdateOne(**r) for r in requests])\n",
    "    print(rv.modified_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51fe2d",
   "metadata": {},
   "source": [
    "# metaP_stegen.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d8b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fetch = [{\n",
    "    # >100MB\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/stegen_MetaProteomicAnalysis_activity.json\",\n",
    "    \"type\": \"metaproteomics_analysis_activity_set\",\n",
    "}, {\n",
    "    # ~50KB\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/stegen_emsl_analysis_data_objects.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r\"https?://(?P<domain>[^/]+)/(?P<path>.+)\")\n",
    "\n",
    "def url_to_name(url):\n",
    "    m = pattern.match(url)\n",
    "    return f\"{'.'.join(reversed(m.group('domain').split('.')))}__{m.group('path').replace('/', '.')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_them_all(to_fetch):\n",
    "    for i, spec in enumerate(to_fetch):\n",
    "        url = spec[\"url\"]\n",
    "        name = url_to_name(url)\n",
    "        print(f\"{i+1}/{len(to_fetch)}: fetching {url}\")\n",
    "        rv = requests.get(url)\n",
    "        print(f\"saving as {name}\")\n",
    "        with open(f'/Users/dwinston/Downloads/{name}', 'w') as f:\n",
    "            json.dump(rv.json(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_them_all(to_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c37ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_data_object(d):\n",
    "    rv = requests.head(\n",
    "            d[\"url\"], allow_redirects=True, verify=False, timeout=5, headers={\"Accept-Encoding\": \"gzip;q=0\"}\n",
    "        )\n",
    "    if not rv.status_code == 200:\n",
    "        return {\"error\": {\"status_code\": rv.status_code, \"details\": \"not OK\"}, \"id\": d[\"id\"]}\n",
    "    if d[\"file_size_bytes\"] != int(rv.headers[\"Content-Length\"]):\n",
    "        return {\n",
    "            \"error\": {\n",
    "                \"details\": \"file size different than reported\",\n",
    "                \"file_size_actual\": rv.headers[\"Content-Length\"],\n",
    "                \"file_size_reported\": d[\"file_size_bytes\"]\n",
    "            },\n",
    "            \"id\": d[\"id\"],\n",
    "        }\n",
    "    return {\"result\": \"OK\", \"id\": d[\"id\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_downloaded_json(name):\n",
    "    with open(f'/Users/dwinston/Downloads/{name}') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98cfef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = fetch_downloaded_json(url_to_name(to_fetch[1][\"url\"]))\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"{i+1}/{len(docs)}\", check_data_object(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import identity, dissoc, assoc_in\n",
    "\n",
    "metaP_field_map = {\n",
    "    \"PeptideSequence\": (\"peptide_sequence\", identity),\n",
    "    \"sum(MASICAbundance)\": (\"peptide_sum_masic_abundance\", int),\n",
    "    \"sum_MASICAbundance\": (\"peptide_sum_masic_abundance\", int),\n",
    "    \"SpectralCount\": (\"peptide_spectral_count\", int),\n",
    "    \"BestProtein\": (\"best_protein\", identity),\n",
    "    \"min(QValue)\": (\"min_q_value\", float),\n",
    "    \"min_QValue\": (\"min_q_value\", float),\n",
    "    \n",
    "    \"peptide_sequence\": (\"peptide_sequence\", identity),\n",
    "    \"peptide_sum_masic_abundance\": (\"peptide_sum_masic_abundance\", int),\n",
    "    \"peptide_spectral_count\": (\"peptide_spectral_count\", int),\n",
    "    \"best_protein\": (\"best_protein\", identity),\n",
    "    \"min_q_value\": (\"min_q_value\", float),\n",
    "}\n",
    "\n",
    "\n",
    "def map_fields(doc, field_map=None):\n",
    "    for k_old, todo in field_map.items():\n",
    "        if k_old in doc:\n",
    "            k_new, fn = todo\n",
    "            # work around e.g. \"ValueError: invalid literal for int() with base 10: '400840000.0'\"\n",
    "            try:\n",
    "                v_new = fn(doc[k_old])\n",
    "            except ValueError:\n",
    "                v_new = fn(float(doc[k_old]))\n",
    "            doc = dissoc(doc, k_old)\n",
    "            doc = assoc_in(doc, [k_new], v_new)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def correct_metaP_doc(doc):\n",
    "    if not \"has_peptide_quantifications\" in doc:\n",
    "        return doc\n",
    "    new_items = [\n",
    "        map_fields(item, metaP_field_map) for item in doc[\"has_peptide_quantifications\"]\n",
    "    ]\n",
    "    doc = assoc_in(\n",
    "        doc,\n",
    "        [\"has_peptide_quantifications\"],\n",
    "        new_items,\n",
    "    )\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def fetch_metaP_validate_and_add(to_fetch):\n",
    "    to_add = defaultdict(list)\n",
    "    for i, spec in enumerate(to_fetch):\n",
    "        url = spec[\"url\"]\n",
    "        name = Path(url).name\n",
    "        collection_name = spec[\"type\"]\n",
    "        print(f\"{i+1}/{len(to_fetch)}: fetching {name} ({collection_name})\")\n",
    "        docs = fetch_downloaded_json(url_to_name(url))\n",
    "        if not isinstance(docs, list):\n",
    "            docs = [docs]\n",
    "        docs = [correct_metaP_doc(d) for d in docs]\n",
    "        to_add[collection_name].extend(docs)\n",
    "    print(\"validating\")\n",
    "    nmdc_jsonschema_validate(to_add)\n",
    "    print(\"adding\")\n",
    "    mongo.add_docs(to_add, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_metaP_validate_and_add(to_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0254c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fetch = [{\n",
    "    \"url\": \"https://portal.nersc.gov/project/m3408/meta/501128_1781_100340_stegen_MetaProteomicAnalysis_activity.json\",\n",
    "    \"type\": \"metaproteomics_analysis_activity_set\",\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/project/m3408/meta/501128_1781_100340_stegen_emsl_analysis_data_objects.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2874fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_them_all(to_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = fetch_downloaded_json(url_to_name(to_fetch[1][\"url\"]))\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"{i+1}/{len(docs)}\", check_data_object(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_metaP_validate_and_add(to_fetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f519f60",
   "metadata": {},
   "source": [
    "# mongo_etl_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79cab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fetch = [{\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/mt_annotation_objects.json\",\n",
    "    \"type\": \"metagenome_annotation_activity_set\"\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/mt_annotation_data_objects.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/project/m3408/meta/metagenomeAssembly_activity.json\",\n",
    "    \"type\": \"metagenome_assembly_set\",\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/project/m3408/meta/metagenomeAssembly_data_objects.json\",\n",
    "    \"type\": \"data_object_set\",\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/ReadbasedAnalysis_activity.json\",\n",
    "    \"type\": \"read_based_analysis_activity_set\"\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/ReadbasedAnalysis_data_objects.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/MAGs_activity.json\",\n",
    "    \"type\": \"mags_activity_set\",\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/MAGs_data_objects.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/project/m3408/meta/readQC_activity.json\",\n",
    "    \"type\": \"read_QC_analysis_activity_set\"\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/project/m3408/meta/readQC_activity_data_objects.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/img_mg_annotation_objects.json\",\n",
    "    \"type\": \"metagenome_annotation_activity_set\",\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/img_mg_annotation_data_objects.json\",\n",
    "    \"type\": \"data_object_set\",\n",
    "}, {\n",
    "    \"url\": \"https://nmdcdemo.emsl.pnnl.gov/metabolomics/registration/gcms_metabolomics_data_products.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}, {\n",
    "    \"url\": \"https://nmdcdemo.emsl.pnnl.gov/nom/registration/ftms_nom_data_products.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441fab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_them_all(to_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def fetch_validate_and_add(to_fetch):\n",
    "    to_add = defaultdict(list)\n",
    "    for i, spec in enumerate(to_fetch):\n",
    "        url = spec[\"url\"]\n",
    "        name = Path(url).name\n",
    "        collection_name = spec[\"type\"]\n",
    "        print(f\"{i+1}/{len(to_fetch)}: fetching {name} ({collection_name})\")\n",
    "        docs = fetch_downloaded_json(url_to_name(url))\n",
    "        if not isinstance(docs, list):\n",
    "            docs = [docs]\n",
    "        to_add[collection_name].extend(docs)\n",
    "    print(\"validating\")\n",
    "    nmdc_jsonschema_validate(to_add)\n",
    "    print(\"adding\")\n",
    "    mongo.add_docs(to_add, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_validate_and_add(to_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e48d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifests = [{\n",
    "    \"url\": (\n",
    "        \"https://nmdcdemo.emsl.pnnl.gov/metabolomics/registration/\"\n",
    "        \"gcms_metabolomics_metadata_products.json\"\n",
    "    ),\n",
    "    \"type\": \"metabolomics_analysis_activity_set\"\n",
    "}, {\n",
    "    \"url\": (\n",
    "        \"https://nmdcdemo.emsl.pnnl.gov/nom/registration/\"\n",
    "        \"ftms_nom_metadata_products.json\"\n",
    "    ),\n",
    "    \"type\": \"nom_analysis_activity_set\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e41bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "to_fetch = []\n",
    "\n",
    "for m in manifests:\n",
    "    urls = requests.get(m[\"url\"]).json()\n",
    "    for url in urls:\n",
    "        to_fetch.append({\"url\": url, \"type\": m[\"type\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b2b1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def fetch_json(url):\n",
    "    return requests.get(url).json()\n",
    "\n",
    "\n",
    "def download_them_all_parallel(to_fetch):\n",
    "    error_urls = []\n",
    "    pbar = tqdm(total=len(to_fetch))\n",
    "    urls = [spec[\"url\"] for spec in to_fetch]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(fetch_json, url): url\n",
    "            for url in urls\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            pbar.update(1)\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                payload = future.result()\n",
    "            except Exception as e:\n",
    "                error_urls.append((url, str(e)))\n",
    "            else:\n",
    "                name = url_to_name(url)\n",
    "                with open(f'/Users/dwinston/Downloads/{name}', 'w') as f:\n",
    "                    json.dump(payload, f)\n",
    "\n",
    "    pbar.close()\n",
    "    return error_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_them_all_parallel(to_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfcdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_downloaded_json_given_url(url):\n",
    "    name = url_to_name(url)\n",
    "    with open(f'/Users/dwinston/Downloads/{name}') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def validate_and_add_parallel(to_fetch):\n",
    "    nmdc_db = defaultdict(list)\n",
    "    error_urls = []\n",
    "    pbar = tqdm(total=len(to_fetch))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_spec = {\n",
    "            executor.submit(fetch_downloaded_json_given_url, spec[\"url\"]): spec\n",
    "            for spec in to_fetch\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_spec):\n",
    "            spec = future_to_spec[future]\n",
    "            try:\n",
    "                payload = future.result()\n",
    "            except Exception as e:\n",
    "                error_urls.append((spec[\"url\"], url_to_name(spec[\"url\"]), str(e)))\n",
    "            else:\n",
    "                nmdc_db[spec[\"type\"]].append(payload)\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return nmdc_db, error_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmdc_db, error_urls = validate_and_add_parallel(to_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = nmdc_jsonschema_validate(nmdc_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8505d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48007baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo.add_docs(nmdc_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80fab23",
   "metadata": {},
   "source": [
    "MetaG annotations (`/global/project/projectdirs/m3408/www/meta/anno2/*_annotations.json`) are 155 JSON files totalling  ~83GB. To load them into MongoDB, I\n",
    "1. Set up a Globus transfer from NERSC DTN to a Globus Connect Personal endpoint on my laptop. I could e.g.\n",
    "```\n",
    "$ scp dtn01.nersc.gov:/global/project/projectdirs/m3408/www/meta/anno2/*_annotations.json .\n",
    "```\n",
    "but I chose to use Globus, and it works well.\n",
    "2. I have a bash script that uses GNU sed to transform each JSON file to a simple JSON Lines file, as expected by `mongoimport`:\n",
    "\n",
    "```bash\n",
    "# trim.sh\n",
    "\n",
    "task(){\n",
    "    echo $datafile\n",
    "    gsed -e '1,2d' -e '$d' $datafile | gsed -e '$d' | gsed s/\\}\\,/\\}/ > anno2/$(basename $datafile).jsonl\n",
    "}\n",
    "\n",
    "for datafile in ~/globus-nersc/nmdc/m3408/www/meta/anno2/*_annotations.json; do\n",
    "    task $datafile &\n",
    "done\n",
    "```\n",
    "I use `ps aux | grep \"gsed s\" | wc -l` to monitor the progress of the parallel sed tasks. I found that trying to do this head/tail file trimming by `json.load`ing the files in Python and resaving was quite slow because the JSON files are individually quite large.\n",
    "\n",
    "\n",
    "3. I use `jq` to filter for KEGG orthology annotations only, as these are currently the only annotations ingested by the data portal, reducing the number of annotation documents tenfold (from ~500M to ~50M):\n",
    "\n",
    "```bash\n",
    "# jqfilter.sh\n",
    "\n",
    "task(){\n",
    "    echo $datafile\n",
    "    jq 'select(.has_function|test(\"^KEGG.\"))' $datafile > anno2/$(basename $datafile).filtered.jsonl\n",
    "}\n",
    "\n",
    "for datafile in anno2/*_annotations.json.jsonl; do\n",
    "    task $datafile &\n",
    "done\n",
    "```\n",
    "\n",
    "4. I have a bash script that `mongoimport`s each filtered json lines file to the database\n",
    "\n",
    "```bash\n",
    "# mongoimport.sh\n",
    "# Note: be sure to remove any remote collection indexes first, to speed up import. Then, re-create indexes.\n",
    "\n",
    "n=$(ls anno2/*_annotations.*filtered.jsonl | wc -l | xargs) # `| xargs` to trim whitespace\n",
    "i=1\n",
    "for datafile in anno2/*_annotations.*filtered.jsonl; do\n",
    "    echo \"($i of $n): $datafile\"\n",
    "    mongoimport -h HOST \\\n",
    "        -u USER -p PASS --authenticationDatabase admin \\\n",
    "        -d nmdc -c functional_annotation_set \\\n",
    "        --numInsertionWorkers 8 \\\n",
    "        $datafile\n",
    "    i=$((i+1))\n",
    "done\n",
    "```\n",
    "\n",
    "specifying multiple (8 in this case) insertion workers per import."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416bc0c",
   "metadata": {},
   "source": [
    "# ghissue_252_253_linked_samples.ipynb\n",
    "Biosample linking update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca768979",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open(\"../src/data/2021-02-03-stegen_biosample_linking_update.csv\") as f:\n",
    "    next(f) # skip header row\n",
    "    for row in f:\n",
    "        line = row.strip()\n",
    "        tokens = line.split(\",\")\n",
    "        if tokens[-1] == '':\n",
    "            rows.append(tokens[:-1])\n",
    "        else:\n",
    "            rows.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5726c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "with open(\"../src/data/2021-02-04-stegen_biosample_template.json\") as f:\n",
    "    s = f.read()\n",
    "\n",
    "stegen_sample_template = json.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ce37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/data/2021-02-04-stegen_biosample_template.json\", \"w\") as f:\n",
    "    json.dump(stegen_sample_template, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc622f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "gold_pattern = re.compile(r\"Gb\\d+\")\n",
    "\n",
    "def prefix_sample_id(s):\n",
    "    if \":\" in s:\n",
    "        return s\n",
    "    elif re.fullmatch(gold_pattern, s):\n",
    "        return \"gold:\" + s\n",
    "    else:\n",
    "        return \"emsl:\" + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "omics = []\n",
    "for i, row in enumerate(rows):\n",
    "    omics.append({\n",
    "        \"omics_id\": row[0],\n",
    "        \"omics_type\": row[1],\n",
    "        \"sample_name\": row[2],\n",
    "        \"sample_id\": prefix_sample_id(row[3]),\n",
    "        \"new\": len(row) > 4 and row[4] == \"TRUE\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_ids = [\n",
    "    d[\"id\"] for d in\n",
    "    db.biosample_set.find({\"id\": {\"$in\": [o[\"sample_id\"] for o in omics]}}, [\"id\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9299f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import assoc_in, get_in\n",
    "\n",
    "def transform_in(doc, keys, fn):\n",
    "    initial = get_in(keys, doc)\n",
    "    transformed = fn(initial)\n",
    "    return assoc_in(doc, keys, transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae166f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template(template, sample_id, sample_name):\n",
    "    doc = assoc_in(template, [\"id\"], sample_id)\n",
    "    doc = transform_in(\n",
    "        doc, [\"identifier\", \"has_raw_value\"],\n",
    "        lambda s: s.replace(\"$BIOSAMPLE_NAME\", sample_name)\n",
    "    )\n",
    "    doc = transform_in(\n",
    "        doc, [\"name\"],\n",
    "        lambda s: s.replace(\"$BIOSAMPLE_NAME\", sample_name)\n",
    "    )\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ece649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def term_subdocs_to_id_strings(doc):\n",
    "#     keys_with_term_ids = [\n",
    "#         k for k in doc\n",
    "#         if isinstance(doc[k], dict)\n",
    "#         and \"term\" in doc[k]\n",
    "#         and \"id\" in doc[k][\"term\"]\n",
    "#     ]\n",
    "#     for k in keys_with_term_ids:\n",
    "#         doc = assoc_in(doc, [k, \"term\"], doc[k][\"term\"][\"id\"])\n",
    "#     return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = {}\n",
    "for o in omics:\n",
    "    if o[\"new\"]:\n",
    "        new_samples[o[\"sample_id\"]] = o[\"sample_name\"]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for sample_id, sample_name in new_samples.items():\n",
    "    doc = fill_template(stegen_sample_template, sample_id, sample_name)\n",
    "    #doc = term_subdocs_to_id_strings(doc)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0613853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import get_in, assoc_in\n",
    "\n",
    "def un_raw_value(doc, key):\n",
    "    value = get_in([key, \"has_raw_value\"], doc)\n",
    "    if value is not None:\n",
    "        return assoc_in(doc, [key], value)\n",
    "    else:\n",
    "        return doc\n",
    "\n",
    "def re_raw_value(doc, key):\n",
    "    value = get_in([key], doc)\n",
    "    if value is not None and not isinstance(value, dict):\n",
    "        del doc[key]\n",
    "        return assoc_in(doc, [key, \"has_raw_value\"], value)\n",
    "    else:\n",
    "        return doc\n",
    "\n",
    "raws = [\n",
    "    \"ecosystem\",\n",
    "    \"collection_date\",\n",
    "    \"community\",\n",
    "    \"ecosystem_category\",\n",
    "    \"ecosystem_subtype\",\n",
    "    \"ecosystem_type\",\n",
    "    \"geo_loc_name\",\n",
    "    \"habitat\",\n",
    "    \"identifier\",\n",
    "    \"location\",\n",
    "    \"ncbi_taxonomy_name\",\n",
    "    \"sample_collection_site\",\n",
    "    \"specific_ecosystem\",\n",
    "]\n",
    "timestampvalue_fields = [\n",
    "    p for p, spec in nmdc_jsonschema['definitions']['Biosample']['properties'].items()\n",
    "    if '$ref' in spec and spec[\"$ref\"].endswith(\"TimestampValue\")\n",
    "]\n",
    "textvalue_fields = [\n",
    "    p for p, spec in nmdc_jsonschema['definitions']['Biosample']['properties'].items()\n",
    "    if '$ref' in spec and spec[\"$ref\"].endswith(\"TextValue\")\n",
    "]\n",
    "\n",
    "for key in raws:\n",
    "    docs = [un_raw_value(d, key) for d in docs]\n",
    "for key in timestampvalue_fields + textvalue_fields:\n",
    "    docs = [re_raw_value(d, key) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = nmdc_jsonschema_validate({\"biosample_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = mongo.add_docs({\"biosample_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv['biosample_set'].upserted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af29186",
   "metadata": {},
   "source": [
    "Second checklist item of GH Issue 252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdb376",
   "metadata": {},
   "outputs": [],
   "source": [
    "omics = [\n",
    "    transform_in(o, [\"omics_id\"], lambda s: \"emsl:\"+s if \":\" not in s else s)\n",
    "    for o in omics\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "omics_ids = [o[\"omics_id\"] for o in omics]\n",
    "\n",
    "found_omics_ids = [\n",
    "    d[\"id\"] for d in\n",
    "    db.omics_processing_set.find({\"id\": {\"$in\": omics_ids}},[\"id\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122da255",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(omics_ids) == set(found_omics_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03511070",
   "metadata": {},
   "outputs": [],
   "source": [
    "omics_updates = {}\n",
    "for o in omics:\n",
    "    omics_updates[o[\"omics_id\"]] = o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb92c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import dissoc\n",
    "\n",
    "replacing_omics_type = {}\n",
    "\n",
    "docs = []\n",
    "\n",
    "for doc in db.omics_processing_set.find({\"id\": {\"$in\": omics_ids}}):\n",
    "    omics_type = get_in([\"omics_type\"], doc)\n",
    "    updates = omics_updates[doc[\"id\"]]\n",
    "    new_omics_type = {\"has_raw_value\": updates[\"omics_type\"]}\n",
    "    if omics_type != new_omics_type:\n",
    "        replacing_omics_type[doc[\"id\"]] = {\"from\": omics_type, \"to\": new_omics_type}\n",
    "    doc = assoc_in(doc, [\"omics_type\"], new_omics_type)\n",
    "    doc = assoc_in(doc, [\"has_input\"], [updates[\"sample_id\"]])\n",
    "    docs.append(dissoc(doc, \"_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacing_omics_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = nmdc_jsonschema_validate({\"omics_processing_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = mongo.add_docs({\"omics_processing_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6862c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv['omics_processing_set'].modified_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db46ce",
   "metadata": {},
   "source": [
    "GH issue 253 - Brodie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d34817",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open(\"../src/data/2021-02-04-brodie_biosample_linking_update.csv\") as f:\n",
    "    next(f) # skip header row\n",
    "    for row in f:\n",
    "        line = row.strip()\n",
    "        tokens = line.split(\",\")\n",
    "        if tokens[-1] == '':\n",
    "            rows.append(tokens[:-1])\n",
    "        else:\n",
    "            rows.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "omics = []\n",
    "for i, row in enumerate(rows):\n",
    "    omics.append({\n",
    "        \"omics_id\": \"emsl:\" + row[0].strip(),\n",
    "        \"omics_type\": row[1].strip(),\n",
    "        \"sample_name\": row[2].strip(),\n",
    "        \"sample_id\": \"igsn:\" + row[3].strip(),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "omics_ids = [o[\"omics_id\"] for o in omics]\n",
    "\n",
    "found_omics_ids = [\n",
    "    d[\"id\"] for d in\n",
    "    db.omics_processing_set.find({\"id\": {\"$in\": omics_ids}},[\"id\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac85df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(omics_ids) == set(found_omics_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "omics_updates = {}\n",
    "for o in omics:\n",
    "    omics_updates[o[\"omics_id\"]] = o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacing_omics_type = {}\n",
    "\n",
    "docs = []\n",
    "\n",
    "for doc in db.omics_processing_set.find({\"id\": {\"$in\": omics_ids}}):\n",
    "    omics_type = get_in([\"omics_type\"], doc)\n",
    "    updates = omics_updates[doc[\"id\"]]\n",
    "    new_omics_type = {\"has_raw_value\": updates[\"omics_type\"]}\n",
    "    if omics_type != new_omics_type:\n",
    "        replacing_omics_type[doc[\"id\"]] = {\"from\": omics_type, \"to\": new_omics_type}\n",
    "    doc = assoc_in(doc, [\"omics_type\"], new_omics_type)\n",
    "    doc = assoc_in(doc, [\"has_input\"], [updates[\"sample_id\"]])\n",
    "    docs.append(dissoc(doc, \"_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d74af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacing_omics_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8047ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = nmdc_jsonschema_validate({\"omics_processing_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = mongo.add_docs({\"omics_processing_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16095755",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv['omics_processing_set'].modified_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f17b6a",
   "metadata": {},
   "source": [
    "# ghissue_255.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7eee60",
   "metadata": {},
   "source": [
    "Remove the 40 Wrighton EMSL omics_processing and data object documents that relate to isolates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b67561",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfilter = {\n",
    "    \"part_of\": [\"gold:Gs0114675\"],\n",
    "    \"processing_institution\": \"Environmental Molecular Sciences Lab\"\n",
    "}\n",
    "\n",
    "db.omics_processing_set.count_documents(filter=mfilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import DeleteMany\n",
    "from toolz import concat\n",
    "\n",
    "requests = []\n",
    "\n",
    "\n",
    "docs = list(db.omics_processing_set.find(mfilter, [\"id\", \"has_output\"]))\n",
    "omics_processing_ids = [d[\"id\"] for d in docs]\n",
    "data_object_ids = list(concat(d[\"has_output\"] for d in docs))\n",
    "\n",
    "assert len(omics_processing_ids) == db.data_object_set.count_documents({\"id\": {\"$in\": data_object_ids}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2dd214",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv1 = db.omics_processing_set.delete_many({\"id\": {\"$in\": omics_processing_ids}})\n",
    "rv2 = db.data_object_set.delete_many({\"id\": {\"$in\": data_object_ids}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv1.deleted_count, rv2.deleted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311ff82",
   "metadata": {},
   "source": [
    "# ghissue_272.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c202252",
   "metadata": {},
   "source": [
    "add 5 Brodie samples used at EMSL but not JGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "dt_pattern = re.compile(r\"\\d{2}-(?P<month>\\w+)-\\d{2} \\d{2}\\.\\d{2}\\.\\d{2}\\.(?P<ns>\\d+) [A|P]M\")\n",
    "dt_format = \"%d-%b-%y %I.%M.%S.%f %p\"\n",
    "\n",
    "def gold_dtstr_to_iso8601(s):\n",
    "    match = dt_pattern.search(s)\n",
    "    first, month, rest = s.partition(match.group(\"month\"))\n",
    "    s_new = first + month[0] + month[1:].lower() + rest\n",
    "    s_new = s_new.replace(match.group(\"ns\"), match.group(\"ns\")[:-3]) # truncate to microseconds\n",
    "    dt = datetime.strptime(s_new, dt_format)\n",
    "    return dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_brodie_emsl = [\n",
    "   {\n",
    "      \"name\":\"Soil microbial communities from the East River watershed near Crested Butte, Colorado, United States - ER_145\",\n",
    "      \"description\":\"Soil microbial communities from the East River watershed near Crested Butte, Colorado, United States\",\n",
    "      \"lat_lon\":{\n",
    "         \"has_raw_value\":\"38.92045766 -106.9484528\",\n",
    "         \"latitude\":38.92045766,\n",
    "         \"longitude\":-106.9484528\n",
    "      },\n",
    "      \"geo_loc_name\":\"USA: Colorado\",\n",
    "      \"collection_date\":\"2017-05-09\",\n",
    "      \"env_broad_scale\":{\n",
    "         \"has_raw_value\":\"ENVO_00000446\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"env_local_scale\":{\n",
    "         \"has_raw_value\":\"ENVO_00000292\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"env_medium\":{\n",
    "         \"has_raw_value\":\"ENVO_00001998\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"ecosystem\":\"Environmental\",\n",
    "      \"ecosystem_category\":\"Terrestrial\",\n",
    "      \"ecosystem_type\":\"Soil\",\n",
    "      \"ecosystem_subtype\":\"Unclassified\",\n",
    "      \"specific_ecosystem\":\"Unclassified\",\n",
    "      \"depth\": {\"has_numeric_value\": 5},\n",
    "      \"ncbi_taxonomy_name\":\"soil metagenome\",\n",
    "      \"community\":\"microbial communities\",\n",
    "      \"location\":\"The East River watershed near Crested Butte, Colorado, USA\",\n",
    "      \"habitat\":\"soil\",\n",
    "      \"sample_collection_site\":\"soil\",\n",
    "      \"id\":\"igsn:IEWFS000I\",\n",
    "      \"identifier\":\"igsn:IEWFS000I\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"Soil microbial communities from the East River watershed near Crested Butte, Colorado, United States - ER_147\",\n",
    "      \"description\":\"Soil microbial communities from the East River watershed near Crested Butte, Colorado, United States\",\n",
    "      \"lat_lon\":{\n",
    "         \"has_raw_value\":\"38.92045766 -106.9484528\",\n",
    "         \"latitude\":38.92045766,\n",
    "         \"longitude\":-106.9484528\n",
    "      },\n",
    "      \"geo_loc_name\":\"USA: Colorado\",\n",
    "      \"collection_date\":\"2017-05-09\",\n",
    "      \"env_broad_scale\":{\n",
    "         \"has_raw_value\":\"ENVO_00000446\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"env_local_scale\":{\n",
    "         \"has_raw_value\":\"ENVO_00000292\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"env_medium\":{\n",
    "         \"has_raw_value\":\"ENVO_00001998\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"ecosystem\":\"Environmental\",\n",
    "      \"ecosystem_category\":\"Terrestrial\",\n",
    "      \"ecosystem_type\":\"Soil\",\n",
    "      \"ecosystem_subtype\":\"Unclassified\",\n",
    "      \"specific_ecosystem\":\"Unclassified\",\n",
    "      \"depth\":{\"has_numeric_value\": 15},\n",
    "      \"ncbi_taxonomy_name\":\"soil metagenome\",\n",
    "      \"community\":\"microbial communities\",\n",
    "      \"location\":\"The East River watershed near Crested Butte, Colorado, USA\",\n",
    "      \"habitat\":\"soil\",\n",
    "      \"sample_collection_site\":\"soil\",\n",
    "      \"id\":\"igsn:IEWFS000K\",\n",
    "      \"identifier\":\"igsn:IEWFS000K\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"Soil microbial communities from the East River watershed near Crested Butte, Colorado, United States - ER_135\",\n",
    "      \"description\":\"Soil microbial communities from the East River watershed near Crested Butte, Colorado, United States\",\n",
    "      \"lat_lon\":{\n",
    "         \"has_raw_value\":\"38.92028116 -106.9489189\",\n",
    "         \"latitude\":38.92028116,\n",
    "         \"longitude\":-106.94891899\n",
    "      },\n",
    "      \"geo_loc_name\":\"USA: Colorado\",\n",
    "      \"collection_date\":\"2017-05-09\",\n",
    "      \"env_broad_scale\":{\n",
    "         \"has_raw_value\":\"ENVO_00000446\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"env_local_scale\":{\n",
    "         \"has_raw_value\":\"ENVO_00000292\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"env_medium\":{\n",
    "         \"has_raw_value\":\"ENVO_00001998\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"ecosystem\":\"Environmental\",\n",
    "      \"ecosystem_category\":\"Terrestrial\",\n",
    "      \"ecosystem_type\":\"Soil\",\n",
    "      \"ecosystem_subtype\":\"Unclassified\",\n",
    "      \"specific_ecosystem\":\"Unclassified\",\n",
    "      \"depth\":{\"has_numeric_value\": 15},\n",
    "      \"ncbi_taxonomy_name\":\"soil metagenome\",\n",
    "      \"community\":\"microbial communities\",\n",
    "      \"location\":\"The East River watershed near Crested Butte, Colorado, USA\",\n",
    "      \"habitat\":\"soil\",\n",
    "      \"sample_collection_site\":\"soil\",\n",
    "      \"id\":\"igsn:IEWFS000B\",\n",
    "      \"identifier\":\"igsn:IEWFS000B\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"Soil microbial communities from the East River watershed near Crested Butte, Colorado, United States - ER_134\",\n",
    "      \"description\":\"Soil microbial communities from the East River watershed near Crested Butte, Colorado, United States\",\n",
    "      \"lat_lon\":{\n",
    "         \"has_raw_value\":\"38.92028116 -106.9489189\",\n",
    "         \"latitude\":38.92028116,\n",
    "         \"longitude\":-106.9489189\n",
    "      },\n",
    "      \"geo_loc_name\":\"USA: Colorado\",\n",
    "      \"collection_date\":\"2017-05-09\",\n",
    "      \"env_broad_scale\":{\n",
    "         \"has_raw_value\":\"ENVO_00000446\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"env_local_scale\":{\n",
    "         \"has_raw_value\":\"ENVO_00000292\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"env_medium\":{\n",
    "         \"has_raw_value\":\"ENVO_00001998\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"ecosystem\":\"Environmental\",\n",
    "      \"ecosystem_category\":\"Terrestrial\",\n",
    "      \"ecosystem_type\":\"Soil\",\n",
    "      \"ecosystem_subtype\":\"Unclassified\",\n",
    "      \"specific_ecosystem\":\"Unclassified\",\n",
    "      \"depth\":{\"has_numeric_value\": 5},\n",
    "      \"ncbi_taxonomy_name\":\"soil metagenome\",\n",
    "      \"community\":\"microbial communities\",\n",
    "      \"location\":\"The East River watershed near Crested Butte, Colorado, USA\",\n",
    "      \"habitat\":\"soil\",\n",
    "      \"sample_collection_site\":\"soil\",\n",
    "      \"id\":\"igsn:IEWFS000A\",\n",
    "      \"identifier\":\"igsn:IEWFS000A\"\n",
    "   },\n",
    "   {\n",
    "      \"name\":\"Soil microbial communities from the East River watershed near Crested Butte, Colorado, United States - ER_146\",\n",
    "      \"description\":\"Soil microbial communities from the East River watershed near Crested Butte, Colorado, United States\",\n",
    "      \"lat_lon\":{\n",
    "         \"has_raw_value\":\"38.92045766 -106.9484528\",\n",
    "         \"latitude\":38.92045766,\n",
    "         \"longitude\":-106.9484528\n",
    "      },\n",
    "      \"geo_loc_name\":\"USA: Colorado\",\n",
    "      \"collection_date\":\"2017-05-09\",\n",
    "      \"env_broad_scale\":{\n",
    "         \"has_raw_value\":\"ENVO_00000446\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"env_local_scale\":{\n",
    "         \"has_raw_value\":\"ENVO_00000292\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"env_medium\":{\n",
    "         \"has_raw_value\":\"ENVO_00001998\",\n",
    "         \"type\":\"ControlledTermValue\"\n",
    "      },\n",
    "      \"ecosystem\":\"Environmental\",\n",
    "      \"ecosystem_category\":\"Terrestrial\",\n",
    "      \"ecosystem_type\":\"Soil\",\n",
    "      \"ecosystem_subtype\":\"Unclassified\",\n",
    "      \"specific_ecosystem\":\"Unclassified\",\n",
    "      \"depth\":{\"has_numeric_value\": 5},\n",
    "      \"ncbi_taxonomy_name\":\"soil metagenome\",\n",
    "      \"community\":\"microbial communities\",\n",
    "      \"location\":\"The East River watershed near Crested Butte, Colorado, USA\",\n",
    "      \"habitat\":\"soil\",\n",
    "      \"sample_collection_site\":\"soil\",\n",
    "      \"id\":\"igsn:IEWFS000J\",\n",
    "      \"identifier\":\"igsn:IEWFS000J\"\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import get_in, assoc_in\n",
    "\n",
    "def re_raw_value(doc, key):\n",
    "    value = get_in([key], doc)\n",
    "    if value is not None and not isinstance(value, dict):\n",
    "        del doc[key]\n",
    "        return assoc_in(doc, [key, \"has_raw_value\"], value)\n",
    "    else:\n",
    "        return doc\n",
    "\n",
    "timestampvalue_fields = [\n",
    "    p for p, spec in nmdc_jsonschema['definitions']['Biosample']['properties'].items()\n",
    "    if '$ref' in spec and spec[\"$ref\"].endswith(\"TimestampValue\")\n",
    "]\n",
    "textvalue_fields = [\n",
    "    p for p, spec in nmdc_jsonschema['definitions']['Biosample']['properties'].items()\n",
    "    if '$ref' in spec and spec[\"$ref\"].endswith(\"TextValue\")\n",
    "]\n",
    "quantityvalue_fields = [\n",
    "    p for p, spec in nmdc_jsonschema['definitions']['Biosample']['properties'].items()\n",
    "    if '$ref' in spec and spec[\"$ref\"].endswith(\"QuantityValue\")\n",
    "]\n",
    "\n",
    "docs = docs_brodie_emsl\n",
    "\n",
    "for key in timestampvalue_fields + textvalue_fields + quantityvalue_fields:\n",
    "    docs = [re_raw_value(d, key) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in docs:\n",
    "#     d[\"add_date\"] = gold_dtstr_to_iso8601(d[\"add_date\"])\n",
    "#     d[\"mod_date\"] = gold_dtstr_to_iso8601(d[\"mod_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = nmdc_jsonschema_validate({\"biosample_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79459a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = mongo.add_docs({\"biosample_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb862a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert db.biosample_set.count_documents({\"id\": {\"$in\": [d[\"id\"] for d in docs]}}) == len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c15511",
   "metadata": {},
   "source": [
    "# ensure_biosample_set_study_id.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done via biosample_set.part_of field (\"Relates the biosample to the study for which the sample was collected.\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.biosample_set.create_index(\"part_of\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15238579",
   "metadata": {},
   "outputs": [],
   "source": [
    "stegen = \"gold:Gs0114663\"\n",
    "wrighton = \"gold:Gs0114675\"\n",
    "brodie = \"gold:Gs0135149\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab670663",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.biosample_set.count_documents({\"part_of\": brodie})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.biosample_set.count_documents({\"part_of\": stegen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.biosample_set.count_documents({\"part_of\": wrighton})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.biosample_set.count_documents({\"part_of\": {\"$in\": [brodie, stegen, wrighton]}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46a3b7",
   "metadata": {},
   "source": [
    "# Remove omics_processing without has_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85697039",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [d[\"id\"] for d in db.omics_processing_set.find({\"has_input\": \"emsl:TBD\"}, [\"id\"])]\n",
    "print(len(to_delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import DeleteMany\n",
    "\n",
    "rv = db.omics_processing_set.bulk_write([DeleteMany({\"id\": {\"$in\": to_delete}})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b2624",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv.deleted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856a591",
   "metadata": {},
   "source": [
    "# add metaT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc274ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import requests\n",
    "\n",
    "from nmdc_runtime.util import nmdc_jsonschema_validate\n",
    "\n",
    "pattern = re.compile(r\"https?://(?P<domain>[^/]+)/(?P<path>.+)\")\n",
    "\n",
    "to_fetch = [{\n",
    "    \"url\": \"https://portal.nersc.gov/project/m3408/meta/metaT_activity.json\",\n",
    "    \"type\": \"metatranscriptome_activity_set\" # waiting on PR microbiomedata/nmdc-schema#86\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/project/m3408/meta/metaT_data_objects.json\",\n",
    "    \"type\": \"data_object_set\" # already mongoimported, but good to re-do via notebook\n",
    "}]\n",
    "\n",
    "def url_to_name(url):\n",
    "    m = pattern.match(url)\n",
    "    return f\"{'.'.join(reversed(m.group('domain').split('.')))}__{m.group('path').replace('/', '.')}\"\n",
    "\n",
    "def download_them_all(to_fetch):\n",
    "    for i, spec in enumerate(to_fetch):\n",
    "        url = spec[\"url\"]\n",
    "        name = url_to_name(url)\n",
    "        print(f\"{i+1}/{len(to_fetch)}: fetching {url}\")\n",
    "        rv = requests.get(url)\n",
    "        print(f\"saving as {name}\")\n",
    "        with open(f'/Users/dwinston/Downloads/{name}', 'w') as f:\n",
    "            json.dump(rv.json(), f)\n",
    "            \n",
    "def fetch_downloaded_json(name):\n",
    "    with open(f'/Users/dwinston/Downloads/{name}') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def fetch_downloaded_json_given_url(url):\n",
    "    name = url_to_name(url)\n",
    "    with open(f'/Users/dwinston/Downloads/{name}') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def fetch_validate_and_add(to_fetch):\n",
    "    to_add = defaultdict(list)\n",
    "    for i, spec in enumerate(to_fetch):\n",
    "        url = spec[\"url\"]\n",
    "        name = Path(url).name\n",
    "        collection_name = spec[\"type\"]\n",
    "        print(f\"{i+1}/{len(to_fetch)}: fetching {name} ({collection_name})\")\n",
    "        docs = fetch_downloaded_json_given_url(url)\n",
    "        if not isinstance(docs, list):\n",
    "            docs = [docs]\n",
    "        to_add[collection_name].extend(docs)\n",
    "    print(\"validating\")\n",
    "    nmdc_jsonschema_validate(to_add)\n",
    "    print(\"adding\")\n",
    "    mongo.add_docs(to_add, validate=False)\n",
    "            \n",
    "download_them_all(to_fetch)\n",
    "fetch_validate_and_add(to_fetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e041b2",
   "metadata": {},
   "source": [
    "# Create file_type_enum collection\n",
    "May end up being the same as `object_types`, but keep seperate for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56169ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class FileTypeEnumBase(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    filter: str # JSON-encoded data_object_set mongo collection filter document    \n",
    "\n",
    "class FileTypeEnum(FileTypeEnumBase):\n",
    "    id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bb410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "from nmdc_runtime.api.core.idgen import generate_one_id\n",
    "\n",
    "@lru_cache\n",
    "def _fte_id(fte_as_str):\n",
    "    return generate_one_id(mongo.db, \"file_type_enum\")\n",
    "\n",
    "def get_fte_id(fte):\n",
    "    rv = _fte_id(fte.json())\n",
    "    assert isinstance(rv, str)\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de038bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_matches(filter_):\n",
    "    return list(mongo.db.data_object_set.find(filter_).limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca605e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_type_enum = [\n",
    "    FileTypeEnumBase(\n",
    "        name=\"FT ICR-MS analysis results\",\n",
    "        description=\"FT ICR-MS-based metabolite assignment results table\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"nom\\/results\"}, \"description\": {\"$regex\": \"FT ICR-MS\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"GC-MS Metabolomics Results\",\n",
    "        description=\"GC-MS-based metabolite assignment results table\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"metabolomics\\/results\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"Metaproteomics Workflow Statistics\",\n",
    "        description=\"Aggregate workflow statistics file\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"QC_Metrics.tsv\", \"$options\": \"i\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"Protein Report\",\n",
    "        description=\"Filtered protein report file\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"Protein_Report.tsv\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"Peptide Report\",\n",
    "        description=\"Filtered peptide report file\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"Peptide_Report.tsv\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"Unfiltered Metaproteomics Results\",\n",
    "        description=\"MSGFjobs and MASIC output file\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"MSGFjobs_MASIC_resultant.tsv\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"Read Count and RPKM\",\n",
    "        description=\"Annotation read count and RPKM per feature JSON\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"metat_out_json\\/output.json\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"QC non-rRNA R2\",\n",
    "        description=\"QC removed rRNA reads (R2) fastq\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"filtered_R2.fastq\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"QC non-rRNA R1\",\n",
    "        description=\"QC removed rRNA reads (R1) fastq\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"filtered_R1.fastq\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"Metagenome Bins\",\n",
    "        description=\"Metagenome bin contigs fasta\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"(hqmq\\_bin\\.zip)|(bins\\.\\d+\\.fa)\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"CheckM Statistics\",\n",
    "        description=\"CheckM statistics report\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"checkm_qa.out\"}})\n",
    "    ),    \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Krona Plot\",\n",
    "        description=\"[GOTTCHA2] krona plot HTML file\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"gottcha2.*krona.html\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"Krona Plot\",\n",
    "        description=\"[Kraken2] krona plot HTML file\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"kraken2.*krona.html\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"Classification Report\",\n",
    "        description=\"[Kraken2] output report file\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"kraken2.*report.tsv\"}})\n",
    "    ),    \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Taxonomic Classification\",\n",
    "        description=\"[Kraken2] output read classification file\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"kraken2.*classification.tsv\"}})\n",
    "    ),    \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Krona Plot\",\n",
    "        description=\"[Centrifuge] krona plot HTML file\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"centrifuge.*krona.html\"}})\n",
    "    ),    \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Classification Report\",\n",
    "        description=\"[Centrifuge] output report file\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"centrifuge.*report.tsv\"}})\n",
    "    ),    \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Taxonomic Classification\",\n",
    "        description=\"[Centrifuge] output read classification file\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"centrifuge.*classification.tsv\"}})\n",
    "    ),    \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Structural Annotation GFF\",\n",
    "        description=\"GFF3 format file with structural annotations\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"annotation\\/.*structural_annotation\\.gff\"}})\n",
    "    ),    \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Functional Annotation GFF\",\n",
    "        description=\"GFF3 format file with functional annotations\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"annotation\\/.*functional_annotation\\.gff\"}})\n",
    "    ),   \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Annotation Amino Acid FASTA\",\n",
    "        description=\"FASTA amino acid file for annotated proteins\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"annotation.*\\.faa\"}})\n",
    "    ),    \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Annotation Enzyme Commission\",\n",
    "        description=\"Tab delimited file for EC annotation\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"_ec.tsv\"}})\n",
    "    ),    \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Annotation KEGG Orthology\",\n",
    "        description=\"Tab delimited file for KO annotation\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"_ko.tsv\"}})\n",
    "    ),      \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Assembly Coverage BAM\",\n",
    "        description=\"Sorted bam file of reads mapping back to the final assembly\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"pairedMapped_sorted.bam\"}})\n",
    "    ),       \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Assembly AGP\",\n",
    "        description=\"An AGP format file describes the assembly\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"assembly.agp\"}})\n",
    "    ),       \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Assembly Scaffolds\",\n",
    "        description=\"Final assembly scaffolds fasta\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"assembly_scaffolds.fna\"}})\n",
    "    ),      \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Assembly Contigs\",\n",
    "        description=\"Final assembly contigs fasta\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"assembly_contigs.fna\"}})\n",
    "    ),    \n",
    "    FileTypeEnumBase(\n",
    "        name=\"Assembly Coverage Stats\",\n",
    "        description=\"Assembled contigs coverage information\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"mapping_stats.txt\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"Filtered Sequencing Reads\",\n",
    "        description=\"Reads QC result fastq (clean data)\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"filtered.fastq.gz\"}})\n",
    "    ),\n",
    "    FileTypeEnumBase(\n",
    "        name=\"QC Statistics\",\n",
    "        description=\"Reads QC summary statistics\",\n",
    "        filter=json.dumps({\"url\": {\"$regex\": \"filterStats.txt\"}})\n",
    "    ),\n",
    "]\n",
    "\n",
    "# file_type_enum = [\n",
    "#     FileTypeEnum(id=get_fte_id(fte), **fte.dict())\n",
    "#     for fte in file_type_enum\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da62e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdb.drop_collection(\"file_type_enum\")\n",
    "# mdb.file_type_enum.create_index(\"id\", unique=True)\n",
    "# rv = mdb.file_type_enum.insert_many([fte.dict() for fte in file_type_enum])\n",
    "# len(rv.inserted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For changing e.g. `filter` or `description`\n",
    "\n",
    "from toolz import assoc\n",
    "\n",
    "name_to_id = {d[\"name\"]: d[\"id\"] for d in mdb.file_type_enum.find()}\n",
    "\n",
    "for fte in file_type_enum:\n",
    "    id_ = name_to_id[fte.name]\n",
    "    doc = assoc(fte.dict(), \"id\", id_)\n",
    "    mdb.file_type_enum.replace_one({\"id\": id_}, doc, upsert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71457f87",
   "metadata": {},
   "source": [
    "# Add `data_object_type` values to `data_object_set` docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f671b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class FileTypeEnumBase(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    filter: str # JSON-encoded data_object_set mongo collection filter document    \n",
    "\n",
    "class FileTypeEnum(FileTypeEnumBase):\n",
    "    id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d16881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from toolz import dissoc\n",
    "\n",
    "def fte_matches(fte):\n",
    "    return [dissoc(d, \"_id\") for d in mongo.db.data_object_set.find(json.loads(fte.filter))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5330a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmdc_schema.validate_nmdc_json import get_nmdc_schema\n",
    "import fastjsonschema\n",
    "\n",
    "nmdc_jsonschema = get_nmdc_schema()\n",
    "nmdc_jsonschema[\"$defs\"][\"FileTypeEnum\"][\"enum\"] = mdb.file_type_enum.distinct(\"id\")\n",
    "nmdc_jsonschema_validate = fastjsonschema.compile(nmdc_jsonschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in mdb.file_type_enum.find():\n",
    "    fte = FileTypeEnum(**doc)\n",
    "    print(fte.id, fte.name)\n",
    "    docs = fte_matches(fte)\n",
    "    if docs:\n",
    "        docs_to_add = []\n",
    "        for doc in docs:\n",
    "            if \"data_object_type\" not in doc:\n",
    "                doc[\"data_object_type\"] = fte.id\n",
    "                docs_to_add.append(doc)\n",
    "        _ = nmdc_jsonschema_validate({\"data_object_set\": docs_to_add})\n",
    "        if docs_to_add:\n",
    "            mongo.add_docs({\"data_object_set\": docs_to_add}, validate=False)\n",
    "            print(\"added\", len(docs_to_add), \"docs\")\n",
    "    else:\n",
    "        print(f\"no docs matching {fte.dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1a8b4",
   "metadata": {},
   "source": [
    "# Take bins.\\d.fa and change name to {gold ID (end of description)}.bins.\\d.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d1aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from toolz import dissoc\n",
    "\n",
    "fte = FileTypeEnum(**mdb.file_type_enum.find_one({\"name\": \"Metagenome Bins\"}))\n",
    "\n",
    "docs = [dissoc(d, \"_id\") for d in mdb.data_object_set.find(json.loads(fte.filter))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# for d in docs:\n",
    "#     _, _, gold_id = d[\"description\"].rpartition(\" \")\n",
    "#     name = Path(d[\"url\"]).name\n",
    "#     d[\"name\"] = f\"{gold_id}.{name}\"\n",
    "\n",
    "# _ = mongo.add_docs({\"data_object_set\": docs})\n",
    "# docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e039c1",
   "metadata": {},
   "source": [
    "# add profile images\n",
    "https://portal.nersc.gov/project/m3408/profile_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study -> principal_investigator -> profile_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed50bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from toolz import dissoc\n",
    "\n",
    "base_url = \"https://portal.nersc.gov/project/m3408/profile_images/\"\n",
    "\n",
    "docs = [dissoc(d, \"_id\") for d in db.study_set.find()]\n",
    "for doc in docs:\n",
    "    pi = doc[\"principal_investigator\"]\n",
    "    pi_name = pi[\"has_raw_value\"]\n",
    "    image_name = \"_\".join(reversed(pi_name.lower().split(\" \"))) + '.jpg'\n",
    "    url = base_url + image_name\n",
    "    if requests.head(url).status_code == 200:\n",
    "        doc[\"principal_investigator\"][\"profile_image_url\"] = url\n",
    "    else:\n",
    "        print(pi_name, image_name, requests.head(base_url + image_name).status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = nmdc_jsonschema_validate({\"study_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = mongo.add_docs({\"study_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.study_set.count_documents({\"principal_investigator.profile_image_url\": {\"$exists\": True}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc2597",
   "metadata": {},
   "source": [
    "# Correct data_object_set.file_size_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import dissoc\n",
    "\n",
    "docs = [dissoc(d, \"_id\") for d in db.data_object_set.find({\"url\": {\"$exists\": True}})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ff007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_ids = []\n",
    "\n",
    "# with open(\"/Users/dwinston/Downloads/nmdc-data-objects-wrong-size.txt\") as f:\n",
    "#     for line in f:\n",
    "#         do_id, url = line.strip().split(\",\")\n",
    "#         do_ids.append(do_id)\n",
    "        \n",
    "# print(f\"{len(docs)} listed\")\n",
    "\n",
    "# docs = [dissoc(d, \"_id\") for d in db.data_object_set.find({\"id\": {\"$in\": do_ids}})]\n",
    "# print(f\"{len(docs)} found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be29e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "import requests\n",
    "from toolz import assoc\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def get_file_size_bytes(d, header_ok=True):\n",
    "    url = d[\"url\"].replace(\"https://\", \"http://\")\n",
    "    try:\n",
    "        rv = requests.head(\n",
    "            url, allow_redirects=True, verify=False, timeout=5, headers={\"Accept-Encoding\": \"gzip;q=0\"}\n",
    "        )\n",
    "        if not rv.status_code == 200:\n",
    "            return {\"no_ok_response\": [d[\"id\"]]}\n",
    "\n",
    "        if header_ok:\n",
    "            try:\n",
    "                return {\"data\": [assoc(d, 'file_size_bytes', int(rv.headers['Content-Length']))]}\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        #rv = requests.get(url, allow_redirects=True, verify=False, timeout=0.5)\n",
    "        #return {\"data\": [assoc(d, 'file_size_bytes', len(rv.content))]}\n",
    "        return {\"no_header_content_length\": [d[\"id\"]]}\n",
    "    except Exception as e:\n",
    "        return {\"error\": [(d, str(e))]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf18d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = [d[\"error\"][0][0] for d in db[\"_tmp__get_file_size_bytes\"].find({\"error\": {\"$exists\": True}})]\n",
    "# len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa8c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "from toolz import concat, merge_with\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "db.drop_collection(\"_tmp__get_file_size_bytes\")\n",
    "error_docs = []\n",
    "\n",
    "pbar = tqdm(total=len(docs))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    future_to_doc = {\n",
    "        executor.submit(\n",
    "            get_file_size_bytes, doc\n",
    "        ): doc\n",
    "        for doc in docs\n",
    "    }\n",
    "    print(\"created futures...\")\n",
    "    for future in concurrent.futures.as_completed(future_to_doc):\n",
    "        pbar.update(1)\n",
    "        doc = future_to_doc[future]\n",
    "        try:\n",
    "            payload = future.result()\n",
    "        except Exception as e:\n",
    "            error_docs.append(doc, str(e))\n",
    "        else:\n",
    "            db[\"_tmp__get_file_size_bytes\"].insert_one(payload)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [dissoc(d, \"_id\") for d in db[\"_tmp__get_file_size_bytes\"].find()]\n",
    "results = merge_with(lambda v: list(concat(v)), *results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65745e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = mongo.add_docs({\"data_object_set\": results[\"data\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd19eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe62bb",
   "metadata": {},
   "source": [
    "# load genome_feature_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc40e8",
   "metadata": {},
   "source": [
    "MetaG features (`/global/project/projectdirs/m3408/www/meta/anno2/*_features.json`) are 155 JSON files totalling  ~26GB. To load them into MongoDB, I\n",
    "1. Set up a Globus transfer from NERSC DTN to a Globus Connect Personal endpoint on my laptop. I could e.g.\n",
    "```\n",
    "$ scp dtn01.nersc.gov:/global/project/projectdirs/m3408/www/meta/anno2/*_features.json .\n",
    "```\n",
    "but I chose to use Globus, and it works well.\n",
    "2. I have a bash script that uses GNU sed to transform each JSON file to a simple JSON Lines file, as expected by `mongoimport`:\n",
    "\n",
    "```bash\n",
    "# trim.sh\n",
    "\n",
    "task(){\n",
    "    echo $datafile\n",
    "    # Delete n lines from end *in addition to* last line.\n",
    "    # Example: n=1 deletes the last line as well as 1 line prior.\n",
    "    n=1\n",
    "    gsed -e '1,2d' -e \"$(( $(wc -l < $datafile)-n+1 )),$ d\" -e 's/\\}\\,/\\}/' $datafile > $(basename $datafile).jsonl\n",
    "}\n",
    "\n",
    "for datafile in ~/globus-nersc/nmdc/m3408/www/meta/anno2/features/*_features.json; do\n",
    "    task $datafile &\n",
    "done\n",
    "```\n",
    "I use `ps aux | grep \"gsed \" | wc -l` to monitor the progress of the parallel sed tasks. I found that trying to do this head/tail file trimming by `json.load`ing the files in Python and resaving was quite slow because the JSON files are individually quite large.\n",
    "\n",
    "\n",
    "2. I have a bash script that `mongoimport`s each filtered json lines file to the database\n",
    "\n",
    "```bash\n",
    "# mongoimport.sh\n",
    "# Note: be sure to remove any remote collection indexes first, to speed up import. Then, re-create indexes.\n",
    "\n",
    "n=$(ls *features*.jsonl | wc -l | xargs) # `| xargs` to trim whitespace\n",
    "i=1\n",
    "for datafile in *features*.jsonl; do\n",
    "    echo \"($i of $n): $datafile\"\n",
    "    mongoimport -h mongo-ext.nmdc-runtime-dev.polyneme.xyz \\\n",
    "        -u donny -p \"_4tEk6z2YrRYTr@\" --authenticationDatabase admin \\\n",
    "        -d nmdc -c genome_feature_set \\\n",
    "        --numInsertionWorkers 8 \\\n",
    "        $datafile\n",
    "    i=$((i+1))\n",
    "done\n",
    "```\n",
    "\n",
    "specifying multiple (8 in this case) insertion workers per import."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50b659",
   "metadata": {},
   "source": [
    "# individual study doc updates (via spreadsheet spec)\n",
    "\n",
    "\"June Sprint post-ETL updates\" sheet in NMDC Google Drive\n",
    "\n",
    "- many of the study pages are missing information that doesn't yet exist in the mongo database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d86226",
   "metadata": {},
   "outputs": [],
   "source": [
    "stegen_study_id = \"gold:Gs0114663\"\n",
    "wrighton_study_id = \"gold:Gs0114675\"\n",
    "brodie_study_id = \"gold:Gs0135149\"\n",
    "bioscales_study_id = \"gold:Gs0154044\"\n",
    "microbes_persist_sfa_study_id = \"gold:Gs0128850\"\n",
    "plant_microbe_interfaces_sfa_study_id = \"gold:Gs0103573\"\n",
    "spruce_study_id = \"gold:Gs0110138\"\n",
    "watershed_sfa_study_id = \"gold:Gs0149986\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b333c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from nmdc_runtime.api.core.util import pick\n",
    "\n",
    "def study_summary(doc):\n",
    "    return pick([\"id\", \"principal_investigator\", \"name\", \"publications\", \"websites\"], doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = db.study_set.find_one({\"id\": watershed_sfa_study_id})\n",
    "pprint(study_summary(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = [{\n",
    "    # row 6\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": stegen_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$addToSet\": {\n",
    "                \"publications\": \"https://doi.org/10.1371/journal.pone.0228165\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"add to studies/stegen/publications\"    \n",
    "}, {\n",
    "    # row 7\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": stegen_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$set\": {\n",
    "                \"description\": \"\"\"\\\n",
    "This research project, led by James Stegen at PNNL, aimed to understand how molecular-scale processes govern the biogeochemical function of subsurface groundwater-surface water mixing zones (i.e., the hyporheic zone). This project was conducted along the Columbia River in Eastern Washington State, which exhibits variation in microbiome composition, biogeochemical activity, and substrate biogeochemistry, making it an ideal environment for studying biogeochemical hotspots. To capture a range of biogeochemical activities, samples were collected from areas with dense vegetation and virtually no vegetation.\n",
    "\n",
    "This project’s long-term goal is to develop models that can simulate impacts of disturbance on river corridor hydro-biogeochemistry by understanding fundamental molecular processes that lead to emergent function. This project is part of PNNL’s River Corridor Hydrobiogeochemistry Science Focus Area (https://www.pnnl.gov/projects/river-corridor-hydrobiogeochemistry-science-focus-area).\n",
    "\"\"\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"replace studies/stegen/description\"\n",
    "}, {\n",
    "    # row 8\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": wrighton_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$set\": {\n",
    "                \"description\": \"\"\"\\\n",
    "This project aims to improve the understanding of microbial diversity and metabolism in deep shale, with implications for novel enzyme discovery and energy development. This project was conducted along two Appalachian basin shales, the Marcellus and Utica/Point Pleasant formations in Pennsylvania and Ohio, respectively. Samples were collected from input and produced fluids up to a year after hydraulic fracturing at varying depths and locations (4 wells, 2 basin shales).\n",
    "\"\"\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"replace studies/wrighton/description\"\n",
    "}, {\n",
    "    # row 9\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": brodie_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$addToSet\": {\n",
    "                \"publications\": \"https://doi.org/10.21952/WTR/1573029\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"add to studies/brodie/publications\"\n",
    "}, {\n",
    "    # row 10\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": brodie_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$set\": {\n",
    "                \"description\": \"\"\"\\\n",
    "This research project aimed to understand how snow accumulation and snowmelt influences the mobilization of nitrogen through the soil microbiome in a mountainous catchment at the East River Watershed in Colorado. This project sought to identify bacteria, archaea, and fungi that were associated with the microbial biomass bloom that occurs during winter and the biomass crash following snowmelt. This project also sought to understand whether the traits that govern microbial community assembly during and after snowmelt were phylogenetically conserved. Samples were collected during winter, the snowmelt period, and after snowmelt in spring, from an area that transitioned from an upland hillslope to a riparian floodplain.\n",
    "\n",
    "This project is part of the Watershed Function Science Focus Area: https://watershed.lbl.gov/.\n",
    "\"\"\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"replace studies/brodie/description\"\n",
    "}, {\n",
    "    # row 11\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": bioscales_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$set\": {\n",
    "                \"description\": \"\"\"\\\n",
    "The goal of this Bio-Scales Pilot Project study is to understand how plant traits modify the microbiome and in particular how the coupled plant-soil-microbial system influences nitrogen transformation patterns and fluxes.\n",
    "\"\"\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"replace studies/bioscales/description\"\n",
    "}, {\n",
    "    # row 12\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": bioscales_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$addToSet\": {\n",
    "                \"websites\": {\n",
    "                    \"$each\": [\n",
    "                        \"https://www.ornl.gov/staff-profile/mitchel-j-doktycz\",\n",
    "                        \"https://www.ornl.gov/section/bioimaging-and-analytics\",\n",
    "                        \"https://pmiweb.ornl.gov/\",\n",
    "                        \"https://www.ornl.gov/project/bio-scales\",\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"add to studies/bioscales/websites\"\n",
    "}, {\n",
    "    # row 14 (row 13 done elsewhere)\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": bioscales_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$set\": {\n",
    "                \"principal_investigator.has_raw_value\": \"Mitchel J. Doktycz\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"replace studies/bioscales/principal_investigator name\"\n",
    "}, {\n",
    "    # row 15\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": microbes_persist_sfa_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$addToSet\": {\n",
    "                \"websites\": \"https://sc-programs.llnl.gov/biological-and-environmental-research-at-llnl/soil-microbiome\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"add to studies/microbes_persist_sfa/websites\"\n",
    "}, {\n",
    "    # row 16\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": microbes_persist_sfa_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$set\": {\n",
    "                \"description\": \"\"\"\\\n",
    "The Microbes Persist: Systems Biology of the Soil Microbiome SFA seeks to understand how microbial ecophysiology, population dynamics, and microbe–mineral–organic matter interactions regulate the persistence of microbial residues in soil under changing moisture regimes.\n",
    "\"\"\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"replace studies/microbes_persist_sfa/description\"\n",
    "}, {\n",
    "    # row 19 (rows 17 and 18 done elsewhere)\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": plant_microbe_interfaces_sfa_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$addToSet\": {\n",
    "                \"websites\": \"https://pmiweb.ornl.gov/pmi-project-aims/\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"add to studies/plant_microbe_interfaces_sfa/websites\"\n",
    "}, {\n",
    "    # row 20\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": plant_microbe_interfaces_sfa_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$set\": {\n",
    "                \"description\": \"\"\"\\\n",
    "The goal of the Plant-Microbe Interfaces SFA is to gain a deeper understanding of the diversity and functioning of mutually beneficial interactions between plants and microbes in the rhizosphere.\n",
    "\"\"\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"set studies/plant_microbe_interfaces_sfa/description\"\n",
    "}, {\n",
    "    # row 21\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": spruce_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$addToSet\": {\n",
    "                \"websites\": \"https://mnspruce.ornl.gov/project/overview\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"add to studies/spruce/webites\"\n",
    "}, {\n",
    "    # row 22\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": spruce_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$set\": {\n",
    "                \"description\": \"\"\"\\\n",
    "The Spruce and Peatland Responses Under Changing Environments (SPRUCE) experiment is the primary component of the Terrestrial Ecosystem Science Scientific Focus Area of ORNL's Climate Change Program, focused on terrestrial ecosystems and the mechanisms that underlie their responses to climatic change. This project seeks to assess the response of northern peatland ecosystems to increases in temperature and exposures to elevated atmospheric CO2 concentrations.\n",
    "\"\"\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"set studies/spruce/description\"\n",
    "}, {\n",
    "    # row 23\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": watershed_sfa_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$addToSet\": {\n",
    "                \"websites\": \"https://watershed.lbl.gov/about/\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"add to studies/watershed_sfa/webites\"\n",
    "}, {\n",
    "    # row 24\n",
    "    \"update\": \"study_set\",\n",
    "    \"updates\": [{\n",
    "        \"q\": {\n",
    "            \"id\": watershed_sfa_study_id\n",
    "        },\n",
    "        \"u\": {\n",
    "            \"$set\": {\n",
    "                \"description\": \"\"\"\\\n",
    "The Watershed Function Scientific SFA is developing a predictive understanding of how mountainous watersheds retain and release water, nutrients, carbon, and metals. In particular, the SFA is developing understanding and tools to measure and predict how droughts, early snowmelt, and other perturbations impact downstream water availability and biogeochemical cycling at episodic to decadal timescales.\n",
    "\"\"\"\n",
    "            }\n",
    "        },\n",
    "    }],\n",
    "    \"comment\": \"set studies/watershed_sfa/description\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [dissoc(d, \"_id\") for d in db.study_set.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_coll = \"_tmp_study_set\"\n",
    "db.drop_collection(tmp_coll)\n",
    "db[tmp_coll].insert_many(docs)\n",
    "db[tmp_coll].create_index(\"id\", unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965410df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import assoc\n",
    "\n",
    "_tmp_commands = [assoc(c, 'update', tmp_coll) for c in commands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533256d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = []\n",
    "for c in _tmp_commands:\n",
    "    rvs.append(db.command(c))\n",
    "rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee41b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_validate = [dissoc(d, \"_id\") for d in db[tmp_coll].find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf082eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = nmdc_jsonschema_validate({\"study_set\": docs_to_validate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d64fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = mongo.add_docs({\"study_set\": docs_to_validate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7240ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv[\"study_set\"].modified_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bae1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.drop_collection(tmp_coll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7277c",
   "metadata": {},
   "source": [
    "# study metadata updates redux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a887d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "rows = []\n",
    "with open(\"../src/data/2021-07-02-study-changes.csv\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    include = False\n",
    "    for row in reader:\n",
    "        if row['Term'] == 'study title':\n",
    "            include = True\n",
    "        if include:\n",
    "            rows.append({\n",
    "                'name': row[\"Study\"].lower().replace(' ','_').replace('-','_'),\n",
    "                'id': f'gold:{row[\"GOLD Study ID\"]}',\n",
    "                'field': re.findall(r\"\\w+\", row[\"Term\"])[-1].lower(),\n",
    "                'value': row['Value'].strip(),\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{r['field'] for r in rows}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urlify(doi):\n",
    "    if not doi.startswith(\"http\"):\n",
    "        suffix = doi.split(\":\", maxsplit=1)[-1]\n",
    "        return f\"https://doi.org/{suffix}\"\n",
    "    return doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa289c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = []\n",
    "\n",
    "for row in rows:\n",
    "    c = {\n",
    "        \"update\": \"study_set\",\n",
    "        \"updates\": [{\n",
    "            \"q\": {\"id\": row[\"id\"]},\n",
    "            \"u\": {}\n",
    "        }],\n",
    "    }\n",
    "    if row[\"field\"] in {\"title\", \"description\"}:\n",
    "        c[\"updates\"][0][\"u\"] = {\"$set\": {row[\"field\"]: row[\"value\"]}}\n",
    "        c[\"comment\"] = f'set studies/{row[\"name\"]}/{row[\"field\"]}'\n",
    "    elif row[\"field\"] == \"citation\":\n",
    "        c[\"updates\"][0][\"u\"] = {\"$set\": {\"doi\": {\"has_raw_value\": urlify(row[\"value\"])}}}\n",
    "        c[\"comment\"] = f'set studies/{row[\"name\"]}/doi'\n",
    "    elif row[\"field\"] == \"publication\":\n",
    "        c[\"updates\"][0][\"u\"] = {\"$addToSet\": {\"publications\": urlify(row[\"value\"])}}\n",
    "        c[\"comment\"] = f'add studies/{row[\"name\"]}/{row[\"field\"]}'\n",
    "    else:\n",
    "        print(\"Unknown field\", row[\"field\"])\n",
    "    commands.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e227394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import assoc, dissoc\n",
    "\n",
    "docs = [dissoc(d, \"_id\") for d in db.study_set.find()]\n",
    "tmp_coll = \"_tmp_study_set\"\n",
    "db.drop_collection(tmp_coll)\n",
    "db[tmp_coll].insert_many(docs)\n",
    "db[tmp_coll].create_index(\"id\", unique=True)\n",
    "_tmp_commands = [assoc(c, 'update', tmp_coll) for c in commands]\n",
    "rvs = []\n",
    "for c in _tmp_commands:\n",
    "    rvs.append(db.command(c))\n",
    "rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_validate = [dissoc(d, \"_id\") for d in db[tmp_coll].find()]\n",
    "ok = nmdc_jsonschema_validate({\"study_set\": docs_to_validate})\n",
    "rv = mongo.add_docs({\"study_set\": docs_to_validate})\n",
    "print(rv[\"study_set\"].modified_count)\n",
    "db.drop_collection(tmp_coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO get local files to api. Untested\n",
    "\n",
    "from dagster import build_solid_context\n",
    "from nmdc_runtime.solids.core import local_file_to_api_object as lftao\n",
    "\n",
    "context = build_solid_context(\n",
    "    resources={\"mongo\": mongo, \"runtime_api_site_client\": client}\n",
    ")\n",
    "\n",
    "storage_path = \"../src/data/2021-07-02-study-changes.csv\"\n",
    "\n",
    "\n",
    "def local_file_to_api_object(file_info):\n",
    "    return lftao(context, file_info)\n",
    "\n",
    "#obj = local_file_to_api_object({\"storage_path\": storage_path, \"mime_type\": 'text/csv'})\n",
    "\n",
    "#doc = db.objects.find_one({\"id\": obj[\"id\"]})\n",
    "#assert doc[\"name\"] == Path(storage_path).name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030458e1",
   "metadata": {},
   "source": [
    "# ensure all study_set.doi values are HTTPS URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d808238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import assoc_in, dissoc\n",
    "\n",
    "docs = [dissoc(d, \"_id\") for d in db.study_set.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [assoc_in(d, [\"doi\", \"has_raw_value\"], urlify(d[\"doi\"][\"has_raw_value\"])) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35048d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = nmdc_jsonschema_validate({\"study_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5cbaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo.add_docs({\"study_set\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6449a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afdb1c7d",
   "metadata": {},
   "source": [
    "# verify study sample counts\n",
    "From GOLD db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert db.biosample_set.count_documents({\"part_of\": \"gold:Gs0154044\"}) == 217"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd4d3e",
   "metadata": {},
   "source": [
    "# omics_processing_set.part_of as fallback to set biosample_set.part_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ba1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_biosamples__no_part_of = db.biosample_set.distinct(\"id\", {\"part_of\": {\"$exists\": False}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_biosamples__no_part_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.omics_processing_set.count_documents({\"has_input\": {\"$in\": ids_biosamples__no_part_of}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "biosample__part_of = {}\n",
    "\n",
    "for doc in db.omics_processing_set.find(\n",
    "    {\"has_input\": {\"$in\": ids_biosamples__no_part_of}},\n",
    "    [\"has_input\", \"part_of\"],\n",
    "):\n",
    "    assert len(doc[\"part_of\"]) == 1\n",
    "    for biosample in doc[\"has_input\"]:\n",
    "        biosample__part_of[biosample] = doc[\"part_of\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d0199",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(biosample__part_of) == len(ids_biosamples__no_part_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import dissoc\n",
    "\n",
    "docs = [dissoc(d, \"_id\") for d in db.biosample_set.find({\"id\": {\"$in\": list(biosample__part_of.keys())}})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac587a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c96ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import assoc\n",
    "\n",
    "docs_new = [assoc(d, \"part_of\", biosample__part_of[d[\"id\"]]) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = nmdc_jsonschema_validate({\"biosample_set\": docs_new})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c9f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo.add_docs({\"biosample_set\": docs_new})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47265c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(db.biosample_set.distinct(\"id\", {\"part_of\": {\"$exists\": False}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f8b5f",
   "metadata": {},
   "source": [
    "# embargo studies\n",
    "Embargo study `gold:Gs0149986`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_id = \"gold:Gs0149986\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_biosamples = [d[\"id\"]for d in db.biosample_set.find({\"part_of\": study_id})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c20b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_biosamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ccdc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_omics_processings_via_study = [d[\"id\"] for d in db.omics_processing_set.find({\"part_of\": study_id})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_omics_processings_via_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59644160",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_omics_processings_via_biosamples = [\n",
    "    d[\"id\"] for d in db.omics_processing_set.find({\"has_input\": {\"$in\": ids_biosamples}})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decda8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_omics_processings_via_biosamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeeb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_omics_processings = list(set(ids_omics_processings_via_study) | set(ids_omics_processings_via_biosamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541dd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_omics_processings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import concat\n",
    "\n",
    "ids_data_objects_from_omics_processings = list(concat([\n",
    "    d[\"has_output\"] for d in db.omics_processing_set.find({\n",
    "        \"id\": {\n",
    "            \"$in\": ids_omics_processings\n",
    "        }\n",
    "    }, [\"has_output\"])\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_data_objects_from_omics_processings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6cb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "ids_analyses = {}\n",
    "\n",
    "for coll_name in tqdm(db.list_collection_names()):\n",
    "    if coll_name.endswith(\"activity_set\") or coll_name.endswith(\"assembly_set\"):\n",
    "        print(coll_name)\n",
    "        db[coll_name].create_index(\"was_informed_by\")\n",
    "        ids_analyses[coll_name] = [\n",
    "            d[\"id\"] for d in\n",
    "            db[coll_name].find({\"was_informed_by\": {\"$in\": ids_omics_processings}})\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import concat\n",
    "\n",
    "ids_data_objects_from_analyses = []\n",
    "\n",
    "for coll_name, ids_analysis_set in ids_analyses.items():\n",
    "    ids_data_objects_from_analyses.extend(list(concat([\n",
    "        d[\"has_output\"] for d in db[coll_name].find({\n",
    "            \"id\": {\n",
    "                \"$in\": ids_analysis_set\n",
    "            }\n",
    "        }, [\"has_output\"])\n",
    "    ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_data_objects_from_analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662605b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_data_objects = list(set(ids_data_objects_from_omics_processings) | set(ids_data_objects_from_analyses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_data_objects), db.data_object_set.count_documents({\"id\": {\"$in\": ids_data_objects}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733dda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ids_data_objects) == db.data_object_set.count_documents({\"id\": {\"$in\": ids_data_objects}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_object_for_study(db, study_id):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1518b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_embargo = mongo.db.client[\"nmdc_embargo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69337755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "studies = [dissoc(d, \"_id\") for d in db.study_set.find({\"id\": study_id})]\n",
    "biosamples = [dissoc(d, \"_id\") for d in db.biosample_set.find({\"id\": {\"$in\": ids_biosamples}})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3256f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = db_embargo.study_set.insert_many(studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e100b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rv.inserted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = db_embargo.biosample_set.insert_many(biosamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rv.inserted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae755a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = db.study_set.delete_one({\"id\": study_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4010be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv.deleted_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d35ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = db.biosample_set.delete_many({\"id\": {\"$in\": ids_biosamples}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv.deleted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd27816",
   "metadata": {},
   "source": [
    "# Removing omics docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262c6472",
   "metadata": {},
   "source": [
    "From dehays:\n",
    "> need to look at was_informed_by on the metaproteomics_analysis_activity_set docs.\n",
    "And compare with the metaproteomics omics_processing.  There are 4 metaP omics_processing that will not be referred to by any of the metaP analysis activities.  Those 4 omics_processing are the ones do remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mdb.omics_processing_set.find(\n",
    "    {\"id\": {\"$in\": [\"emsl:512156\", \"emsl:512155\", \"emsl:504850\", \"emsl:502966\"]}}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185f546",
   "metadata": {},
   "source": [
    "# removing metaP docs with non prefixed IDs\n",
    "https://github.com/microbiomedata/nmdc-runtime/issues/43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4eddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = mdb.metaproteomics_analysis_activity_set.delete_many(\n",
    "    {\"id\": {\"$not\": {\"$regex\": \"^nmdc:\"}}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc660f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv.deleted_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a452a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nmdc-runtime)",
   "language": "python",
   "name": "nmdc-runtime"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
