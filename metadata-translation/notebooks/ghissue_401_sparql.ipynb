{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ea05af-7579-43ad-aa9c-3bf8b6da8fdb",
   "metadata": {},
   "source": [
    "# Pipeline to transform the set of nmdc-schema-compliant mongodb collections to an RDF dataset amenable to SPARQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675b9ba-c8be-478a-8c72-6edf10f56d8b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before running this notebook, make sure you have done the following:\n",
    "- `make up-dev` has been run and mongo is mapped to `localhost:27018`\n",
    "- a recent dump of the production mongo database has been loaded to `localhost:27018` (see `make mongorestore-nmdc-dev` for an example)\n",
    "- .env has updated `MONGO_HOST` to `mongodb://localhost:27018`\n",
    "- `export $(grep -v '^#' .env | xargs)` has been run in the shell before running `jupyter notebook`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a362b42f-7ae0-40cf-91d4-8f19ca1087cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure code changes in this notebook will be import-able without needing to restart the kernel and lose state\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a456470-920d-4fd4-8040-e0bd3dcabff0",
   "metadata": {},
   "source": [
    "Connect to local dockerized dev environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55932d03-802f-4efe-bceb-e1036cd35567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONGO_HOST=mongodb://localhost:27018\n"
     ]
    }
   ],
   "source": [
    "!env | grep MONGO_HOST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a146763-f03a-4d65-baa0-81ca15cba689",
   "metadata": {},
   "source": [
    "Initialize a db connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb1bb42-005c-49ca-ba59-18c24833f93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "from nmdc_runtime.api.db.mongo import get_mongo_db\n",
    "mdb = get_mongo_db()\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbc9a8-8cac-4798-8d4f-ccbd9c3560e9",
   "metadata": {},
   "source": [
    "Get all populated nmdc-schema collections with entity `id`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a0dd489-74cc-47c4-b3e0-c97dd88f5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmdc_runtime.util import schema_collection_names_with_id_field\n",
    "\n",
    "populated_collections = sorted([\n",
    "    name for name in set(schema_collection_names_with_id_field()) & set(mdb.list_collection_names())\n",
    "    if mdb[name].estimated_document_count() > 0\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a45de7-ba27-4b18-8ff4-9ba44eeb1091",
   "metadata": {},
   "source": [
    "## Get a JSON-LD context for the NMDC Schema, to serialize documents to RDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ed72826-b552-4429-8ab5-9f7126821822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from linkml.generators.jsonldcontextgen import ContextGenerator\n",
    "from nmdc_schema.nmdc_data import get_nmdc_schema_definition\n",
    "\n",
    "context = ContextGenerator(get_nmdc_schema_definition())\n",
    "context = json.loads(context.serialize())[\"@context\"]\n",
    "\n",
    "for k, v in list(context.items()):\n",
    "    if isinstance(v, dict): #and v.get(\"@type\") == \"@id\":\n",
    "        v.pop(\"@id\", None) # use nmdc uri, not e.g. MIXS uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800c5b9-d09e-4be1-899d-62fcf40a2c0e",
   "metadata": {},
   "source": [
    "Ensure `nmdc:type` has a `URIRef` range, i.e. `nmdc:type a owl:ObjectProperty`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62a68c07-0706-4300-a48d-0ab628af87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context['type'] = {'@type': '@id'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe4d54-0a41-4170-9310-45e5f47a6cb5",
   "metadata": {},
   "source": [
    "## Initialize an in-memory graph to store triples, prior to serializing to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "648b4f70-34d6-4c70-8d0a-ef76e7e5d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb8fd0-b847-49fc-a472-a8df2426168a",
   "metadata": {},
   "source": [
    "Define a helper function to speed up triplification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d802017-2a7e-4614-b662-6a0cc027b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chunk(seq, n: int):\n",
    "    \"\"\"\n",
    "    Split sequence into chunks of length n. Do not pad last chunk.\n",
    "    \n",
    "    >>> list(split_chunk(list(range(10)), 3))\n",
    "    [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
    "    \"\"\"\n",
    "    for i in range(0, len(seq), n):\n",
    "        yield seq[i : i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd91d37-b1c7-46ab-b30d-de80132ec091",
   "metadata": {},
   "source": [
    "Define a helper function to ensure each doc has exactly one type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86ff7261-e255-415d-a589-67637292dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmdc_runtime.util import collection_name_to_class_names\n",
    "\n",
    "def ensure_type(doc, collection_name):\n",
    "    if \"type\" in doc:\n",
    "        return doc\n",
    "\n",
    "    class_names = collection_name_to_class_names[collection_name]\n",
    "    \n",
    "    if len(class_names) > 1:\n",
    "        raise Exception(\"cannot unambiguously infer class of document\")\n",
    "        \n",
    "    return assoc(doc, \"type\", class_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eedd442-0f26-4829-a878-cf066b3a3912",
   "metadata": {},
   "source": [
    "## Ingest mongo docs to in-memory graph \n",
    "Uses `rdflib` JSON-LD parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251e0b1-35dc-4f40-91e7-b9bc0d9d79e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99c33f951874aea9a4f325086bde0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading biosample_set collection\n",
      "loading data_object_set collection\n",
      "loading extraction_set collection\n",
      "loading field_research_site_set collection\n",
      "loading library_preparation_set collection\n",
      "loading mags_activity_set collection\n",
      "loading metabolomics_analysis_activity_set collection\n",
      "loading metagenome_annotation_activity_set collection\n",
      "loading metagenome_assembly_set collection\n",
      "loading metagenome_sequencing_activity_set collection\n",
      "loading metaproteomics_analysis_activity_set collection\n"
     ]
    }
   ],
   "source": [
    "from toolz import assoc, dissoc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "chunk_size = 2_000\n",
    "\n",
    "# setup for progress bar\n",
    "total = sum((1 + mdb[name].estimated_document_count() // 2_000) for name in populated_collections)\n",
    "pbar = tqdm(total=total)\n",
    "\n",
    "for collection_name in populated_collections:\n",
    "    print(f\"loading {collection_name} collection\")\n",
    "    # dissociate mongo-generated `_id` field\n",
    "    docs = [dissoc(doc, \"_id\") for doc in mdb[collection_name].find()]\n",
    "    # split collection docs into chunks\n",
    "    chunks = list(split_chunk(docs, chunk_size))\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # ensure each doc in chunk is typed\n",
    "        typed_chunk = [ensure_type(doc, collection_name) for doc in chunk]\n",
    "        # convert each doc to json_ld\n",
    "        doc_jsonld = {\"@context\": context, \"@graph\": chunk}\n",
    "        # add each doc to Graph `g`\n",
    "        g.parse(data=json.dumps(doc_jsonld), format='json-ld')\n",
    "        pbar.update(1)\n",
    "print(f\"{len(g):,} triples loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140ef42-f94c-45c5-a0c1-31b05718aa4f",
   "metadata": {},
   "source": [
    "Correct URIs that end with newlines, which messes up graph serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba832848-2cc9-4d1d-bf5f-966a73e26658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe36373f43ab43fc85fa302d32fc40cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6348584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rdflib import Namespace, RDF, Literal, URIRef\n",
    "\n",
    "NMDC = Namespace(\"https://w3id.org/nmdc/\")\n",
    "\n",
    "for s, p, o in tqdm(g, total=len(g)):\n",
    "    s_str = str(s)\n",
    "    if s_str.endswith(\"\\n\"):\n",
    "        s_str_fixed = str(s_str)[:-2]\n",
    "        g.remove((s,p,o))\n",
    "        g.add((URIRef(s_str_fixed), p,o))\n",
    "    if isinstance(o, URIRef):\n",
    "        o_str = str(o)\n",
    "        if o_str.endswith(\"\\n\"):\n",
    "            o_str_fixed = str(o_str)[:-2]\n",
    "            g.remove((s,p,o))\n",
    "            g.add((s, p, URIRef(o_str_fixed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71893efc-8e19-465e-a33d-3fe6ee475e05",
   "metadata": {},
   "source": [
    "## Connect Schema-Collection Entities\n",
    "Given a schema-collection entity (i.e. one with an `id` and its own mongo document), we want to easily find all other schema-collection entities to which it connects, via any slot.\n",
    "\n",
    "To do this, we first gather all schema classes that are the type of a schema-collection entity, as well as these class' ancestors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "831cbf19-8331-4f2d-814c-89d86d060029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "\n",
    "from nmdc_runtime.util import nmdc_schema_view, nmdc_database_collection_instance_class_names\n",
    "\n",
    "schema_view = nmdc_schema_view()\n",
    "toplevel_classes = set()\n",
    "for name in nmdc_database_collection_instance_class_names():\n",
    "    toplevel_classes |= set(schema_view.class_ancestors(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc7a8c-a104-4ac4-b105-0daeaba598a4",
   "metadata": {},
   "source": [
    "Next, we determine which slots have such a \"top-level\" class as its range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d402b739-4ab8-4d93-b00f-76f677313c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'was_generated_by', 'was_informed_by', 'metagenome_annotation_id', 'has_output', 'part_of', 'collected_from', 'has_input'}\n"
     ]
    }
   ],
   "source": [
    "slots = schema_view.all_slots()\n",
    "\n",
    "toplevel_entity_connectors = set()\n",
    "for k, v in context.items():\n",
    "    if isinstance(v, dict) and \"@type\" in v and v[\"@type\"] == \"@id\":\n",
    "        if slots[k].range in toplevel_classes and slots[k].domain != \"Database\":\n",
    "            toplevel_entity_connectors.add(k)\n",
    "print(toplevel_entity_connectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e58127-013e-40e2-a839-c9317e14c488",
   "metadata": {},
   "source": [
    "Let's construct an entity-relationship diagram to visualize relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c99cdd8d-5fd2-44eb-9090-af6f51770fbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classDiagram\n",
      "\n",
      "NamedThing --> Activity : was_generated_by\n",
      "Activity --> Activity : was_informed_by\n",
      "FunctionalAnnotationAggMember --> WorkflowExecutionActivity : metagenome_annotation_id\n",
      "NamedThing --> NamedThing : has_output\n",
      "NamedThing --> NamedThing : part_of\n",
      "Biosample --> FieldResearchSite : collected_from\n",
      "NamedThing --> NamedThing : has_input\n",
      "\n",
      "MaterialEntity <|-- FieldResearchSite\n",
      "Activity <|-- MetaproteomicsAnalysisActivity\n",
      "NamedThing <|-- Site\n",
      "NamedThing <|-- DataObject\n",
      "NamedThing <|-- FieldResearchSite\n",
      "MaterialEntity <|-- Site\n",
      "Activity <|-- MetatranscriptomeActivity\n",
      "NamedThing <|-- LibraryPreparation\n",
      "WorkflowExecutionActivity <|-- MagsAnalysisActivity\n",
      "NamedThing <|-- PlannedProcess\n",
      "WorkflowExecutionActivity <|-- ReadBasedTaxonomyAnalysisActivity\n",
      "Activity <|-- MetagenomeAssembly\n",
      "WorkflowExecutionActivity <|-- NomAnalysisActivity\n",
      "PlannedProcess <|-- Extraction\n",
      "PlannedProcess <|-- LibraryPreparation\n",
      "PlannedProcess <|-- Pooling\n",
      "MaterialEntity <|-- ProcessedSample\n",
      "BiosampleProcessing <|-- LibraryPreparation\n",
      "NamedThing <|-- Biosample\n",
      "NamedThing <|-- Pooling\n",
      "NamedThing <|-- Extraction\n",
      "Activity <|-- MagsAnalysisActivity\n",
      "NamedThing <|-- MaterialEntity\n",
      "MaterialEntity <|-- Biosample\n",
      "WorkflowExecutionActivity <|-- ReadQcAnalysisActivity\n",
      "NamedThing <|-- ProcessedSample\n",
      "WorkflowExecutionActivity <|-- MetagenomeAnnotationActivity\n",
      "NamedThing <|-- CollectingBiosamplesFromSite\n",
      "NamedThing <|-- BiosampleProcessing\n",
      "Activity <|-- NomAnalysisActivity\n",
      "WorkflowExecutionActivity <|-- MetagenomeSequencingActivity\n",
      "WorkflowExecutionActivity <|-- MetagenomeAssembly\n",
      "WorkflowExecutionActivity <|-- MetatranscriptomeActivity\n",
      "Activity <|-- ReadBasedTaxonomyAnalysisActivity\n",
      "Activity <|-- MetagenomeAnnotationActivity\n",
      "Activity <|-- WorkflowExecutionActivity\n",
      "Site <|-- FieldResearchSite\n",
      "BiosampleProcessing <|-- Pooling\n",
      "PlannedProcess <|-- CollectingBiosamplesFromSite\n",
      "Activity <|-- MetagenomeSequencingActivity\n",
      "PlannedProcess <|-- BiosampleProcessing\n",
      "WorkflowExecutionActivity <|-- MetabolomicsAnalysisActivity\n",
      "WorkflowExecutionActivity <|-- MetaproteomicsAnalysisActivity\n",
      "NamedThing <|-- OmicsProcessing\n",
      "Activity <|-- MetabolomicsAnalysisActivity\n",
      "NamedThing <|-- Study\n",
      "Activity <|-- ReadQcAnalysisActivity\n",
      "PlannedProcess <|-- OmicsProcessing\n"
     ]
    }
   ],
   "source": [
    "print(\"classDiagram\\n\")\n",
    "for slot_name in toplevel_entity_connectors:\n",
    "    slot = slots[slot_name]\n",
    "    domain = slot.domain or \"NamedThing\"\n",
    "    range = slot.range\n",
    "    print(f\"{domain} --> {range} : {slot_name}\")\n",
    "\n",
    "print()\n",
    "\n",
    "inheritance_links = set()\n",
    "for cls in toplevel_classes:\n",
    "    ancestors = schema_view.class_ancestors(cls)\n",
    "    for a in ancestors:\n",
    "        if a != cls:\n",
    "            inheritance_links.add(f\"{a} <|-- {cls}\")\n",
    "\n",
    "for link in inheritance_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb2cc8-ef99-4d5f-9ddf-9eb2949e9c06",
   "metadata": {},
   "source": [
    "### Assert a common `depends_on` relation for all entities connected by `toplevel_entity_connectors`\n",
    "This allows us to traverse the graph of top-level entities without needing to specify any specific slot names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc830d77-5ac2-482e-a4f9-dc2eed3f2ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bb9d2404eb41159d8d03d895fa66ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15851994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16,125,596 triples in total\n"
     ]
    }
   ],
   "source": [
    "from rdflib import PROV\n",
    "\n",
    "for s, p, o in tqdm(g, total=len(g)):\n",
    "    if (connector := p.removeprefix(str(NMDC))) in toplevel_entity_connectors:\n",
    "        if connector == \"has_output\":\n",
    "            g.add((o, NMDC.depends_on, s))\n",
    "        else:\n",
    "            g.add((s, NMDC.depends_on, o))\n",
    "\n",
    "print(f\"{len(g):,} triples in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3dd01c-0f20-40c6-9066-793c9d33b901",
   "metadata": {},
   "source": [
    "### Materialize superclass relations\n",
    "We want each entity to be associated with its own class and all the classes that its class inherits from. For example an entity of type `Biosample` should also be of type `NamedThing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75db4baf-369b-47af-974b-f5298470ad7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d41e5fe31fd423cb17f9f0cca75ab72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16349744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema_view = nmdc_schema_view()\n",
    "toplevel_classes = set()\n",
    "\n",
    "# get top level class names\n",
    "for name in nmdc_database_collection_instance_class_names():\n",
    "    toplevel_classes |= set(getattr(NMDC, a) for a in schema_view.class_ancestors(name))\n",
    "\n",
    "# for each triple (s, p, o) in Graph, add all triples (s, p, o') where o' is a class ancestor of o.\n",
    "for s, p, o in tqdm(g, total=len(g)):\n",
    "    # get the local predicate name (eg mdb slot name) for that triple\n",
    "    p_localname = p.removeprefix(str(NMDC))\n",
    "    # skip if predicate is `type`, as this triple was already loaded \n",
    "    if p_localname != \"type\":\n",
    "        continue\n",
    "    # skip triple if the object is not a top-level class   \n",
    "    if o not in toplevel_classes:\n",
    "        continue\n",
    "    # for each triple where the object is a top-level class,\n",
    "    # for each `class_ancestor` associated with that top-level class,\n",
    "    # add the triple (s, `NMDC.type`, `class_ancestor`) \n",
    "    for a in schema_view.class_ancestors(o.removeprefix(str(NMDC))):\n",
    "        # print(f\"{a=}\")\n",
    "        t = (s, NMDC.type, getattr(NMDC,a))\n",
    "        # pprint(f\"{t=}\")\n",
    "        g.add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092657c9-864c-4978-814b-6f587e92887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sanity check that we have the right number of ActivitySet records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "067a53a9-9220-4ee2-bcce-12d6007dab47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14889"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([t for t in g.subjects(NMDC.type, NMDC.Activity)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91171cf6-f435-4815-970f-a67f51254997",
   "metadata": {},
   "source": [
    "## Serialize and store as gzipped N-Triples file.\n",
    "This can take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "125d2ad4-8433-45d8-86c4-d6a619ea5280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serializing Graph and writing to file...\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "with gzip.open('data/nmdc-db.nt.gz', 'wb') as f:\n",
    "    print(\"Serializing graph and writing to file...\") \n",
    "    f.write(g.serialize(format='nt').encode())\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777151b-ddcb-472f-a71b-48af0224de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data into a dockerized fuseki server\n",
    "\n",
    "1. Add the following to `/nmdc-runtime/docker-compose.yaml`.\n",
    "\n",
    "```yml\n",
    "  fuseki:\n",
    "    container_name: fuseki\n",
    "    build:\n",
    "      dockerfile: nmdc_runtime/fuseki.Dockerfile\n",
    "      context: .\n",
    "    ports:\n",
    "      - \"3030:3030\"\n",
    "    volumes:\n",
    "      - ./nmdc_runtime/site/fuseki/fuseki-config.ttl:/configuration/fuseki-config.ttl\n",
    "      - ./nmdc_runtime/site/fuseki/shiro.ini:/fuseki/run/shiro.ini\n",
    "      - nmdc_runtime_fuseki_data:/fuseki-base\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8c0a2-6f75-4dac-9bb9-ac48838ad2b8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "2. Add the following to `/nmdc-runtime/nmdc-runtime/fuseki.Dockerfile`\n",
    "\n",
    "```Dockerfile\n",
    "# Use an appropriate base image that includes Java and wget\n",
    "FROM openjdk:11-jre-slim\n",
    "\n",
    "# Set environment variables\n",
    "ENV FUSEKI_VERSION 4.9.0\n",
    "ENV FUSEKI_HOME /fuseki\n",
    "\n",
    "# Install wget\n",
    "RUN apt-get update && apt-get install -y wget && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Download and extract Fuseki\n",
    "RUN wget -qO- https://archive.apache.org/dist/jena/binaries/apache-jena-fuseki-$FUSEKI_VERSION.tar.gz | tar xvz -C / && \\\n",
    "    mv /apache-jena-fuseki-$FUSEKI_VERSION $FUSEKI_HOME\n",
    "\n",
    "# Expose the default port\n",
    "EXPOSE 3030\n",
    "\n",
    "# Download and extract Jena Commands\n",
    "RUN wget -qO- https://archive.apache.org/dist/jena/binaries/apache-jena-$FUSEKI_VERSION.tar.gz | tar xvz -C / && \\\n",
    "    mv /apache-jena-$FUSEKI_VERSION $FUSEKI_HOME\n",
    "\n",
    "# Copy the Fuseki configuration file to the container\n",
    "COPY ./nmdc_runtime/site/fuseki/fuseki-config.ttl $FUSEKI_HOME/configuration/\n",
    "COPY ./nmdc_runtime/site/fuseki/shiro.ini $FUSEKI_HOME/run/\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR $FUSEKI_HOME\n",
    "\n",
    "# Command to start Fuseki server with preloaded data\n",
    "CMD [\"./fuseki-server\", \"--config\", \"configuration/fuseki-config.ttl\"]\n",
    "```\n",
    "\n",
    "3. Add the following to `/nmdc-runtime/nmdc-runtime/site/fuseki/shiro.ini`\n",
    "```ini\n",
    "[main]\n",
    "localhost=org.apache.jena.fuseki.authz.LocalhostFilter\n",
    "\n",
    "[urls]\n",
    "## Control functions open to anyone\n",
    "/$/server = anon\n",
    "/$/ping   = anon\n",
    "/$/stats = anon\n",
    "/$/stats/* = anon\n",
    "## and the rest are restricted to localhost\n",
    "/$/** = anon\n",
    "/**=anon\n",
    "```\n",
    "\n",
    "5. Add the following to `/nmdc-runtime/nmdc-runtime/site/fuseki/fuseki-config.ttl`\n",
    "```ttl\n",
    "@prefix afn: <http://jena.apache.org/ARQ/function#> .\n",
    "@prefix fuseki: <http://jena.apache.org/fuseki#> .\n",
    "@prefix ja: <http://jena.hpl.hp.com/2005/11/Assembler#> .\n",
    "@prefix nmdc: <https://w3id.org/nmdc/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix tdb: <http://jena.hpl.hp.com/2008/tdb#> .\n",
    "@prefix xs: <http://www.w3.org/2001/XMLSchema#> .\n",
    "\n",
    "<https://api.microbiomedata.org/fuseki/#baseModel>\n",
    "\ta tdb:GraphTDB ;\n",
    "\ttdb:dataset <https://api.microbiomedata.org/fuseki/#tdbDataset> ;\n",
    "\t.\n",
    "\n",
    "<https://api.microbiomedata.org/fuseki/#dataset>\n",
    "\ta ja:RDFDataset ;\n",
    "\tja:defaultGraph <https://api.microbiomedata.org/fuseki/#inferenceModel> ;\n",
    "\t.\n",
    "\n",
    "<https://api.microbiomedata.org/fuseki/#inferenceModel>\n",
    "\ta ja:InfModel ;\n",
    "\tja:baseModel <https://api.microbiomedata.org/fuseki/#baseModel> ;\n",
    "\tja:reasoner [\n",
    "\t\tja:reasonerURL <http://jena.hpl.hp.com/2003/TransitiveReasoner> ;\n",
    "\t] ;\n",
    "\t.\n",
    "\n",
    "<https://api.microbiomedata.org/fuseki/#nmdc>\n",
    "\ta fuseki:Service ;\n",
    "\tfuseki:dataset <https://api.microbiomedata.org/fuseki/#dataset> ;\n",
    "\tfuseki:name \"nmdc\" ;\n",
    "\tfuseki:serviceQuery\n",
    "\t\t\"query\" ,\n",
    "\t\t\"sparql\"\n",
    "\t\t;\n",
    "\tfuseki:serviceReadWriteGraphStore \"data\" ;\n",
    "\tfuseki:serviceUpdate \"update\" ;\n",
    "\tfuseki:serviceUpload \"upload\" ;\n",
    "\t.\n",
    "\n",
    "<https://api.microbiomedata.org/fuseki/#tdbDataset>\n",
    "\ta tdb:DatasetTDB ;\n",
    "\tja:context [\n",
    "\t\trdfs:comment \"Query timeout on this dataset: 10s.\" ;\n",
    "\t\tja:cxtName \"arq:queryTimeout\" ;\n",
    "\t\tja:cxtValue \"10000\" ;\n",
    "\t] ;\n",
    "\ttdb:location \"/fuseki-base/nmdc-db.tdb\" ;\n",
    "\t.\n",
    "\n",
    "[]\n",
    "\ta fuseki:Server ;\n",
    "\tfuseki:services (\n",
    "\t\t<https://api.microbiomedata.org/fuseki/#nmdc>\n",
    "\t) ;\n",
    "\t.\n",
    "```\n",
    "\n",
    ". Spin up a `fuseki` container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea0bdeee-6b3a-4074-bd73-cc9424569346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 1/0\n",
      " \u001b[32mâœ”\u001b[0m Container fuseki  \u001b[32mRunning\u001b[0m                                               \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker compose up fuseki -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79284de7-ef52-47c6-aeb1-1453bd4b5f59",
   "metadata": {},
   "source": [
    "Wipe any existing persisted data, and copy new RDF data into the `fuseki` container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9037026c-2653-43e3-bb92-2a0eea85b213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: No such container: fuseki\n",
      "no such directory\n"
     ]
    }
   ],
   "source": [
    "!docker exec fuseki rm -rf /fuseki-base/nmdc-db.tdb\n",
    "!docker cp data/nmdc-db.nt.gz fuseki:/fuseki-base/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca86f8-6752-4aba-8d3c-656810f3af3f",
   "metadata": {},
   "source": [
    "Take server down in order to bulk-load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0621c-cf98-4a27-9165-7a0a8711db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose down fuseki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f9843-d5c0-4f8d-bcaf-ad2cf50c0264",
   "metadata": {},
   "source": [
    "Bulk-load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490caff-af8a-4537-8c0b-e4a4752645bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker compose run fuseki ./apache-jena-4.9.0/bin/tdbloader --loc /fuseki-base/nmdc-db.tdb /fuseki-base/nmdc-db.nt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0e50c-102a-4a8e-9bcd-ef23600afd66",
   "metadata": {},
   "source": [
    "Start up server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0bfb4b-e694-40b1-88af-4446e3fcc888",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose up fuseki -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e528d6a-76b1-4629-82a1-58793ad6a481",
   "metadata": {},
   "source": [
    "Now go to <http://localhost:3030/#/dataset/nmdc/query> and SPARQL it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695001d-9722-48a0-98e8-9ac5000551ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-03-14T09:40 : took <4min to run all the above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
