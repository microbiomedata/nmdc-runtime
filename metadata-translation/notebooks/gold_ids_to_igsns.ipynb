{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "import os\n",
    "\n",
    "tic = time()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.expanduser(\"~/.nmdc_mongo.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from toolz import assoc_in, dissoc\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from mongospawn.schema import collschemas_for\n",
    "\n",
    "from nmdc_mongo import (\n",
    "    add_to_db,\n",
    "    correct_metaP_doc,\n",
    "    dbschema,\n",
    "    fetch_and_validate_json,\n",
    "    fetch_conform_and_persist_from_manifest,\n",
    "    fetch_json,\n",
    "    get_db,\n",
    "    reset_database,\n",
    "    snake_case_set_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Adjustments for GSP below\n",
    "###########################\n",
    "\n",
    "defined_object_names = set(dbschema[\"definitions\"])\n",
    "\n",
    "set_for_object_name = {\n",
    "    spec[\"items\"][\"$ref\"].split(\"#/definitions/\")[-1]: set_name\n",
    "    for set_name, spec in dbschema[\"properties\"].items()\n",
    "}\n",
    "\n",
    "existing_set_names = set(dbschema[\"properties\"])\n",
    "\n",
    "for object_without_set in (defined_object_names - set(set_for_object_name.keys())):\n",
    "    proposed_set_name = snake_case_set_name(object_without_set)\n",
    "    if proposed_set_name not in existing_set_names:\n",
    "        dbschema[\"properties\"][proposed_set_name] = {\n",
    "            \"description\": (f\"This property links a database object to the set of\"\n",
    "                            f\" {object_without_set} objects within it.\"),\n",
    "            \"items\": {\"$ref\": f\"#/definitions/{object_without_set}\"},\n",
    "            \"type\": \"array\",\n",
    "        }\n",
    "        \n",
    "dbschema = assoc_in(dbschema, [\"definitions\", \"ControlledTermValue\", \"properties\", \"term\", \"type\"], \"string\")\n",
    "del dbschema[\"definitions\"][\"ControlledTermValue\"][\"properties\"][\"term\"][\"$ref\"]\n",
    "\n",
    "# 'k' not capitalized upstream perhaps. should conform!\n",
    "#dbschema = assoc_in(dbschema, [\"definitions\", \"MetagenomeAssembly\", \"properties\", \"scaf_l_gt50k\", \"type\"], \"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collschemas = collschemas_for(dbschema)\n",
    "\n",
    "# Reconstruct\n",
    "set_for_object_name = {\n",
    "    spec[\"items\"][\"$ref\"].split(\"#/definitions/\")[-1]: set_name\n",
    "    for set_name, spec in dbschema[\"properties\"].items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Re-)load existing NMDC DB from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('../src/data/nmdc_database.json.zip') as myzip:\n",
    "    # may be e.g. 'metadata-translation/src/bin/output/nmdc_database.json' rather than 'nmdc_database.json'\n",
    "    name = next(n for n in myzip.namelist() if n.endswith(\"nmdc_database.json\"))\n",
    "    with myzip.open(name) as f:\n",
    "        nmdc_database = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_db(\"dwinston_dev\")\n",
    "reset_database(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_to_db(nmdc_database[\"study_set\"], get_db(\"dwinston_share\"), \"study_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmdc_mongo import validator_for\n",
    "\n",
    "#validator_for(db.study_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study_set {'Study'}\n",
      "omics_processing_set {'OmicsProcessing'}\n",
      "biosample_set {'Biosample'}\n",
      "data_object_set {'DataObject'}\n",
      "metagenome_assembly_set {'MetagenomeAssembly'}\n",
      "read_QC_analysis_activity_set {'ReadQCAnalysisActivity'}\n"
     ]
    }
   ],
   "source": [
    "target_collection = {\n",
    "    \"Study\": \"study_set\",\n",
    "    \"OmicsProcessing\": \"omics_processing_set\",\n",
    "    \"Biosample\": \"biosample_set\",\n",
    "    \"DataObject\": \"data_object_set\",\n",
    "    \"MetagenomeAssembly\": \"metagenome_assembly_set\",\n",
    "    \"MetaProteomicAnalysis\": \"metaproteomics_analysis_activity_set\",\n",
    "    \"MetagenomeAnnotation\": \"metagenome_annotation_activity_set\",\n",
    "    \"ReadQCAnalysisActivity\": \"read_QC_analysis_activity_set\",\n",
    "}\n",
    "for collection in nmdc_database:\n",
    "    docs = nmdc_database[collection]\n",
    "    object_types = {d.get(\"type\", \"nmdc:\")[5:] for d in docs} - {\"\"}\n",
    "    if any(d for d in docs if \"type\" not in d):\n",
    "        print(\"some\",collection,\"docs have no type\")\n",
    "    print(collection, object_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study_set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae448c445cc544ceb269fadcb7232d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omics_processing_set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574a5f07390e4083a18a82dce1d51468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biosample_set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6ee839cea94714861bbc5a7ebe8e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_object_set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dc310e84f94c75bc1e6503e8888833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metagenome_assembly_set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2402c547ab743d4bbff09d33011d11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_QC_analysis_activity_set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f7d563d0244d82bcc3d9e4241b5592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "for source_collection in nmdc_database:\n",
    "    docs = nmdc_database[source_collection]\n",
    "    docs_per_target = defaultdict(list)\n",
    "    for d in docs:\n",
    "        type_ = d.get(\"type\", \"nmdc:\")[5:]\n",
    "        d_new = dissoc(d, \"type\")\n",
    "        if \"lat_lon\" in d_new:\n",
    "            d_new[\"lat_lon\"].pop(\"type\", None)\n",
    "        for k_float in (\n",
    "            \"asm_score\", \"ctg_logsum\", \"ctg_powsum\", \"gap_pct\", \"gc_avg\", \"gc_std\",\n",
    "            \"scaf_logsum\", \"scaf_powsum\"):\n",
    "            if k_float in d_new:\n",
    "                d_new[k_float] = float(d_new[k_float]) \n",
    "        keys_with_term_ids = [\n",
    "            k for k in d_new\n",
    "            if isinstance(d_new[k], dict)\n",
    "            and \"term\" in d_new[k]\n",
    "            and \"id\" in d_new[k][\"term\"]\n",
    "        ]\n",
    "        for k in keys_with_term_ids:\n",
    "            d_new = assoc_in(d_new, [k, \"term\"], d_new[k][\"term\"][\"id\"])\n",
    "        \n",
    "        key = target_collection[type_] if type_ else source_collection\n",
    "        docs_per_target[key].append(d_new)\n",
    "                \n",
    "    for collection_name, docs in docs_per_target.items():\n",
    "        print(collection_name)\n",
    "        payload = fetch_and_validate_json(docs, collection_name=collection_name)\n",
    "        add_to_db(payload, db, collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "study_set: `Additional properties are not allowed ('principal_investigator_name', 'type' were unexpected)`\n",
    "\n",
    "omics_processing_set: `Additional properties are not allowed ('principal_investigator_name', 'mod_date', 'add_date', 'processing_institution', 'ncbi_project_name' were unexpected)`\n",
    "\n",
    "biosample_set: \n",
    "\n",
    "-`Additional properties are not allowed ('location', 'mod_date', 'identifier', 'habitat', 'ncbi_taxonomy_name', 'add_date', 'community', 'sample_collection_site' were unexpected)`\n",
    "\n",
    "-`Additional properties are not allowed ('type' was unexpected)` for `lat_lon`\n",
    "\n",
    "data_object_set: OK\n",
    "\n",
    "activity_set:\n",
    "- `Additional properties are not allowed ('contigs', 'ctg_N50', 'scaf_max', 'contig_bp', 'scaffolds', 'gc_std', 'gap_pct', 'num_input_reads', 'scaf_N50', 'ctg_max', 'ctg_L90', 'ctg_powsum', 'gc_avg', 'scaf_powsum', 'scaf_L90', 'ctg_L50', 'scaf_l_gt50k', 'scaf_N90', 'ctg_N90', 'scaf_L50', 'asm_score', 'scaf_n_gt50K', 'scaf_pct_gt50K', 'num_aligned_reads', 'scaf_bp', 'ctg_logsum', 'scaf_logsum' were unexpected)` for type 'nmdc:MetagenomeAssembly'\n",
    "\n",
    "- `Additional properties are not allowed ('input_read_count', 'output_read_count', 'input_read_bases', 'output_read_bases' were unexpected)` for type 'nmdc:ReadQCAnalysisActivity'\n",
    "\n",
    "metagenome_assembly_set:\n",
    "`Additional properties are not allowed ('scaf_l_gt50k' was unexpected)`\n",
    "\n",
    "read_QC_analysis_activity_set:\n",
    "- `Additional properties are not allowed ('output_read_bases', 'input_read_bases' were unexpected)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load FICUS Brodie spreadsheet and create gold-id-to-igsn map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "GOLD_ID_IDX = 5\n",
    "IGSN_IDX = 2\n",
    "\n",
    "igsn_golds = defaultdict(list)\n",
    "\n",
    "gold_id_pattern = re.compile(r\"Gb\\d+\")\n",
    "\n",
    "with open('../src/data/FICUS_Soil_Gs0135149_Brodie-12-23-2020_PS.xlsx - Brodie_Gs0135149_Soil_Metadata.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        gold_id = row[GOLD_ID_IDX]\n",
    "        igsn = row[IGSN_IDX]\n",
    "        if gold_id_pattern.fullmatch(gold_id):\n",
    "            igsn_golds[igsn].append(gold_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare helper function to compare timestamps given in e.g. \"15-MAY-20 08.30.01.000000000 am\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt_pattern = re.compile(r\"\\d{2}-(?P<month>\\w+)-\\d{2} \\d{2}\\.\\d{2}\\.\\d{2}\\.(?P<ns>\\d+) [A|P]M\")\n",
    "dt_format = \"%d-%b-%y %I.%M.%S.%f %p\"\n",
    "\n",
    "def order_timestamps(timestamps):\n",
    "    if not all(isinstance(ts, str) for ts in timestamps):\n",
    "        raise Exception(f\"{timestamps} not strings\")\n",
    "    as_datetimes = []\n",
    "    for ts in timestamps:\n",
    "        match = dt_pattern.search(ts)\n",
    "        first, month, rest = ts.partition(match.group(\"month\"))\n",
    "        ts_new = first + month[0] + month[1:].lower() + rest\n",
    "        ts_new = ts_new.replace(match.group(\"ns\"), match.group(\"ns\")[:-3]) # truncate to microseconds\n",
    "        as_datetimes.append(datetime.strptime(ts_new, dt_format))\n",
    "    sorted_dts = sorted(as_datetimes)\n",
    "    return [dt.strftime(dt_format) for dt in sorted_dts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare helper-function pipeline to unify biosample_set documents that should be considered equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from toolz import compose\n",
    "\n",
    "er_xna_pattern = re.compile(r\"ER_[D|R]NA_\\d+$\")\n",
    "\n",
    "def rstrip_name_ER_ID(d):\n",
    "    s = get_in([\"name\"], d)\n",
    "    s_new = er_xna_pattern.split(s)[0] if er_xna_pattern.search(s) else s\n",
    "    return assoc_in(d, [\"name\"], s_new)\n",
    "\n",
    "def capitalize_location(d):\n",
    "    s = get_in([\"location\"], d)\n",
    "    if s is not None:\n",
    "        s_new = (s[0].upper() + s[1:])\n",
    "        return assoc_in(d, [\"location\"], s_new)\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "pipeline = compose(\n",
    "    capitalize_location,\n",
    "    rstrip_name_ER_ID,\n",
    "    lambda d: dissoc(d, \"_id\", \"id\", \"add_date\", \"mod_date\", \"identifier\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce new biosample objects with ISGN ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictdiffer import diff\n",
    "from toolz import get_in\n",
    "\n",
    "merged_biosample_docs = []\n",
    "\n",
    "for igsn, golds in igsn_golds.items():\n",
    "    igsn_curie = \"igsn:\"+igsn\n",
    "    to_change = list(db.biosample_set.find({\"id\": {\"$in\": [f\"gold:{g}\" for g in golds]}}))\n",
    "    \n",
    "    # No merge needed, just change of id.\n",
    "    if len(to_change) == 1:\n",
    "        merged = assoc_in(to_change[0], [\"id\"], igsn_curie)\n",
    "        #merged = assoc_in(merged, [\"identifier\"], igsn_curie)\n",
    "        merged_biosample_docs.append(merged)\n",
    "        continue\n",
    "    elif len(to_change) == 0:\n",
    "        continue\n",
    "\n",
    "    # Ensure that unification pipeline is adequate to resolve differences.\n",
    "    distilled = list(map(pipeline, to_change))\n",
    "    result = list(diff(distilled[0], distilled[1]))\n",
    "    assert result == []\n",
    "    \n",
    "    # Produce a merged document\n",
    "    earlier_ts, _ = order_timestamps([get_in([\"add_date\"], d) for d in to_change])\n",
    "    merged = assoc_in(distilled[0], [\"add_date\"], earlier_ts)\n",
    "    _, later_ts = order_timestamps([get_in([\"mod_date\"], d) for d in to_change])\n",
    "    merged = assoc_in(merged, [\"mod_date\"], later_ts)\n",
    "    merged = assoc_in(merged, [\"id\"], igsn_curie)\n",
    "    merged = assoc_in(merged, [\"identifier\"], igsn_curie)\n",
    "    \n",
    "    merged_biosample_docs.append(merged)\n",
    "    merged = None # defense against accidental reuse during next iteration.\n",
    "\n",
    "assert len(merged_biosample_docs) == len(igsn_golds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete old biosample objects and insert new ones in one bulk-write operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 48)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import DeleteMany, InsertOne\n",
    "from toolz import concat\n",
    "\n",
    "requests = [DeleteMany({\"id\": {\"$in\": [\"gold:\"+g for g in concat(igsn_golds.values())]}})]\n",
    "requests.extend([InsertOne(d) for d in merged_biosample_docs])\n",
    "result = db.biosample_set.bulk_write(requests)\n",
    "result.deleted_count, result.inserted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update omics_processing_set references to biosample_set ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldid_igsn = {}\n",
    "for igsn, gids in igsn_golds.items():\n",
    "    for gid in gids:\n",
    "        goldid_igsn[gid] = igsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = []\n",
    "to_replace = {\"gold:\"+k: \"igsn:\"+v for k, v in goldid_igsn.items()}\n",
    "\n",
    "for doc in db.omics_processing_set.find({\"has_input\": {\"$in\": list(to_replace)}}):\n",
    "    operations = {\"$set\": {\n",
    "        \"has_input\": [to_replace.get(i, i) for i in doc[\"has_input\"]],\n",
    "    }}\n",
    "    requests.append({\"filter\": {\"_id\": doc[\"_id\"]}, \"update\": operations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import UpdateOne\n",
    "\n",
    "rv = db.omics_processing_set.bulk_write([UpdateOne(**r) for r in requests])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv.modified_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update omics_processing_set references from EMSL ids to IGSNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMSL_IDS_IDX = 7\n",
    "IGSN_IDX = 2\n",
    "\n",
    "igsn_emsls = {}\n",
    "\n",
    "emsl_ids_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "with open('../src/data/FICUS_Soil_Gs0135149_Brodie-12-23-2020_PS.xlsx - Brodie_Gs0135149_Soil_Metadata.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        emsl_ids = row[EMSL_IDS_IDX]\n",
    "        igsn = row[IGSN_IDX]\n",
    "        ids = emsl_ids_pattern.findall(emsl_ids)\n",
    "        # XXX some rows have emsl ids but no IGSN, so igsn.strip() check here\n",
    "        if igsn.strip() and ids:\n",
    "            igsn_emsls[igsn] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emslid_igsn = {}\n",
    "for igsn, eids in igsn_emsls.items():\n",
    "    for eid in eids:\n",
    "        emslid_igsn[eid] = igsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_with_emsl_id = db.omics_processing_set.count_documents(\n",
    "    {\"id\": {\"$in\": [\"emsl:\"+i for i in emslid_igsn]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = []\n",
    "to_replace = {\"emsl:\"+k: \"igsn:\"+v for k, v in emslid_igsn.items()}\n",
    "to_replace.update({\"emsl:output_\"+k: \"igsn:\"+v for k, v in emslid_igsn.items()})\n",
    "\n",
    "def omit(blacklist, d):\n",
    "    return keyfilter(lambda k: k not in blacklist, d)\n",
    "\n",
    "def sans_mongo_id(d):\n",
    "    return omit([\"_id\"], d)\n",
    "\n",
    "\n",
    "for doc in db.omics_processing_set.find({\"has_input\": {\"$in\": list(to_replace)}}):\n",
    "    operations = {\"$set\": {\n",
    "        \"has_input\": [to_replace.get(i, i) for i in doc[\"has_input\"]],\n",
    "    }}\n",
    "    requests.append({\"filter\": {\"_id\": doc[\"_id\"]}, \"update\": operations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if requests:\n",
    "    rv = db.omics_processing_set.bulk_write([UpdateOne(**r) for r in requests])\n",
    "    print(rv.modified_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmdc_mongo import validator_for\n",
    "db_share = get_db(\"dwinston_share\")\n",
    "#validator_for(db_share.biosample_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "admin_client = MongoClient(\n",
    "    host=os.getenv(\"NMDC_MONGO_HOST\"),\n",
    "    username=\"nmdc-admin\",\n",
    "    password=os.getenv(\"NMDC_MONGO_ADMIN_PWD\")\n",
    ")\n",
    "admin_dwinston_share = admin_client[\"dwinston_share\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_collection_names = sorted(set(target_collection.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['biosample_set',\n",
       " 'data_object_set',\n",
       " 'metagenome_annotation_activity_set',\n",
       " 'metagenome_assembly_set',\n",
       " 'metaproteomics_analysis_activity_set',\n",
       " 'omics_processing_set',\n",
       " 'read_QC_analysis_activity_set',\n",
       " 'study_set']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_collection_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating biosample_set\n",
      "updating data_object_set\n",
      "updating metagenome_annotation_activity_set\n",
      "updating metagenome_assembly_set\n",
      "updating metaproteomics_analysis_activity_set\n",
      "updating omics_processing_set\n",
      "updating read_QC_analysis_activity_set\n",
      "updating study_set\n"
     ]
    }
   ],
   "source": [
    "from nmdc_mongo.admin import reset_database_schema\n",
    "\n",
    "reset_database_schema(admin_client[\"dwinston_share\"], target_collection_names, collschemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding biosample_set\n",
      "adding data_object_set\n",
      "none to add for metagenome_annotation_activity_set\n",
      "adding metagenome_assembly_set\n",
      "none to add for metaproteomics_analysis_activity_set\n",
      "adding omics_processing_set\n",
      "adding read_QC_analysis_activity_set\n",
      "adding study_set\n"
     ]
    }
   ],
   "source": [
    "from toolz import keyfilter\n",
    "\n",
    "db_share = get_db(\"dwinston_share\")\n",
    "target_collection_names = sorted(set(target_collection.values()))\n",
    "for name in target_collection_names:\n",
    "    docs = [sans_mongo_id(d) for d in db[name].find()]\n",
    "    if docs:\n",
    "        print(\"adding\", name)\n",
    "        add_to_db(docs, db_share, collection_name=name)\n",
    "    else:\n",
    "        print(\"none to add for\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmdc",
   "language": "python",
   "name": "nmdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
