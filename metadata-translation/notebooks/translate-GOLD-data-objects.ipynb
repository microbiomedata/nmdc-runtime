{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate GOLD FAA (amino acid assembly), FNA (nucleotide assembly), and FASTQ data into data objects.\n",
    "The notebooks demostrates how to translate GOLD FAA, FNA, and FASTQ data in the JGI Archive and Metadata Organizer (**JAMO**) into json that conforms with the [NMDC schema](https://github.com/microbiomedata/nmdc-metadata/blob/schema-draft/README.md).  \n",
    "Before doing the translation it is important that you have an up to date `nmdc.py` file in the `lib` directory.  \n",
    "\n",
    "The python modules for running the notebook are in the `requirements.txt` file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('../src/bin/lib/')) # add path to lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pds\n",
    "import jsonasobj\n",
    "import nmdc\n",
    "import data_operations as dop\n",
    "from pandasql import sqldf\n",
    "\n",
    "def pysqldf(q):\n",
    "    return sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GOLD study and project tables from nmdc zip file\n",
    "The NMDC data is currently stored in a zip file. Instead of unzipping the file, simply use the `zipfile` library to load the `study` and `project` tables (stored as tab-delimited files). \n",
    "\n",
    "The code for unzipping and creating the dataframe is found in the `make_dataframe` function. As part of the dataframe creation process, the column names are lower cased and spaces are replaced with underscored. I find it helpful to have some standarization on column names when doing data wrangling. This behavior can be overridden if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = dop.make_dataframe(\"export.sql/STUDY_DATA_TABLE.dsv\", file_archive_name=\"../src/data/nmdc-version2.zip\")\n",
    "project = dop.make_dataframe(\"export.sql/PROJECT_DATA_TABLE.dsv\", file_archive_name=\"../src/data/nmdc-version2.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset GOLD tables to active records that are joined to valid study IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "select \n",
    "    *\n",
    "from\n",
    "    study\n",
    "where\n",
    "    active = 'Yes'\n",
    "\"\"\"\n",
    "study = sqldf(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "select \n",
    "    project.*\n",
    "from\n",
    "    project\n",
    "inner join \n",
    "    study\n",
    "on \n",
    "    study.study_id = project.master_study_id    \n",
    "where\n",
    "    project.active = 'Yes'\n",
    "\"\"\"\n",
    "project = sqldf(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GOLD FAA, FNA, and FASTQ files into data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "faa = dop.make_dataframe(\"../src/data/ficus_project_faa.tsv\")\n",
    "fna = dop.make_dataframe(\"../src/data/ficus_project_fna.tsv\")\n",
    "fastq = dop.make_dataframe(\"../src/data/ficus_project_fastq.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faa.head() # peek at data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine FAA, FNA, and FASTQ dataframes into a single dataframe \n",
    "* Since all the files have the same headers, I can concatenate data into a single dataframe for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_objects = pds.concat([faa, fna, fastq], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_objects.head() # peek at data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build json data files\n",
    "The json data files are build using a general approach:\n",
    "1. Create a pandas dataframe (often using SQL syntax) to be translated.\n",
    "2. Transform the dataframe into a dictionary (these variables end with '_dictdf')\n",
    "3. Define a list of field names whose names and values will be translated into characteristics within an annotation object.\n",
    "4. Pass the dataframe dictionary and characteristices list to the `make_json_string_list` method. This method returns a list of json ojbects each of which has been converted to a string.\n",
    "5. Save the json string to file using `save_json_string_list`.\n",
    "\n",
    "**Note:** Currently, I am using the GOLD IDs as idenifiers. This need to changed to IRIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "select \n",
    "    data_objects.*\n",
    "from\n",
    "    data_objects\n",
    "inner join\n",
    "    project\n",
    "on\n",
    "    data_objects.gold_project_id = project.gold_id\n",
    "\"\"\"\n",
    "data_objects = sqldf(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_objects.head() # peek at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_objects_dictdf = data_objects.to_dict(orient=\"records\") # transorm dataframe to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print out a single record for viewing\n",
    "# for record in data_objects_dictdf:\n",
    "#     print(json.dumps(record, indent=4)); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_json_string_list() got an unexpected keyword argument 'id_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e76bf20ea131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## create list of json string objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_objects_json_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_json_string_list\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mdata_objects_dictdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'file_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"file_type_description\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacteristic_fields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharacteristics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: make_json_string_list() got an unexpected keyword argument 'id_key'"
     ]
    }
   ],
   "source": [
    "## specify characteristics\n",
    "characteristics = ['file_size']\n",
    "\n",
    "## create list of json string objects\n",
    "data_objects_json_list = dop.make_json_string_list\\\n",
    "    (data_objects_dictdf, nmdc.DataObject, id_key='file_id', name_key='file_name', description_key=\"file_type_description\", characteristic_fields=characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_objects_json_list[0]) ## peek at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dop.save_json_string_list(\"output/nmdc-json/data_objects.json\", data_objects_json_list) # save json string list to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update omics processing json to relate omics processing to data objects\n",
    "### Schema pattern: omics processing -- has output --> data object\n",
    "Steps:\n",
    "* load project json data into a dictionary\n",
    "* create dataframe linking a project id to list of files ids associated with it\n",
    "* iterate over dataframe and add \"has_oput\" key to matching project ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load omics processing json into dict list\n",
    "omics_dict_list = dop.load_dict_from_json_file(\"output/nmdc-json/omics_processing.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics_dict_list[0] ## peek at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a dataframe with each project id and the file ids associated with it.\n",
    "q = \"\"\"\n",
    "select\n",
    "    d1.gold_project_id, group_concat(d2.file_id, \" \") as file_ids\n",
    "from\n",
    "    data_objects d1\n",
    "inner join\n",
    "    data_objects d2\n",
    "on\n",
    "    d1.file_id = d2.file_id\n",
    "group by \n",
    "    d1.gold_project_id\n",
    "\"\"\"\n",
    "files_df = sqldf(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate over dataframe and create a has_output key for dictionary items with matching project ids\n",
    "for (ix, gold_project_id, file_ids) in files_df.itertuples():\n",
    "    for omics_dict in omics_dict_list:\n",
    "        if gold_project_id == omics_dict[\"id\"]: # compare project id to id of current dict object\n",
    "            omics_dict[\"has_output\"] = file_ids.split() # create list of file ids associated with project id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omics_dict_list[0] ## peek at data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save updated omics processing data as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_json_list = [] # list to hold individual json objects\n",
    "for omics_dict in omics_dict_list:\n",
    "    project_json_list.append(json.dumps(omics_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dop.save_json_string_list(\"output/nmdc-json/omics_processing.json\", project_json_list) # save json string list to file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
