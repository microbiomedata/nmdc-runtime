{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to the documentation for the NMDC Runtime.</p> <p>The documentation uses the Di\u00e1taxis framework. Content here is partitioned into:</p> <ul> <li>Practical steps:<ul> <li>Learning-oriented tutorials</li> <li>Task-oriented how-to guides</li> </ul> </li> <li>Theoretical knowledge:<ul> <li>Information-oriented reference</li> <li>Understanding-oriented explanation</li> </ul> </li> </ul> <p>Explore these using the site navigation.</p>"},{"location":"#api-resource-diagram","title":"API Resource Diagram","text":"<pre><code>erDiagram\n    user }|--o{ site : administers\n    site ||--o{ job-execution : claims\n    job ||--o{ job-execution : done-via\n    workflow ||--o{ job : configured-as\n    site ||--o{ data-object : stewards\n    nmdc-runtime-site ||--o{ job : creates\n    job-execution ||--o{ data-object : has-output\n    job-execution ||--o{ metadata : has-output\n    nmdc-runtime-site ||--o{ metadata : ingests</code></pre> <p>The above entity-relationship diagram shows various resources of the NMDC Runtime API.</p> <p>A site is a resource, administered by one or more users, that may (1) steward data objects, and (2) claim job executions.</p> <p>The NMDC Runtime site is a special site that creates the jobs that sites can execute. Workflows serve as the templates for these jobs -- specifically, the Runtime creates a job by associating a workflow with a particular data object as its input.</p> <p>Thus, each job execution is an application of a workflow to a particular input. The outputs of job executions are data objects and metadata.</p> <p>In the NMDC Schema, the term data object refers to a metadata record that includes a URL to access a payload, the raw sequence of bytes that the data object identifies. Thus, \"sites steward data objects\" means that sites ensure that data object metadata is accurate and actionable -- this includes either directly hosting URLs for payloads, or else obtaining URLs from delegated services.</p> <p>The NMDC Runtime site ingests metadata output by job executions. This ingestion may trigger the creation of new workflow jobs for sites to claim and execute.</p>"},{"location":"#site-federation","title":"Site Federation","text":"<p>Below is a figure that illustrates the federated nature of site interaction with the runtime.</p> <p> </p> NMDC Runtime Site Federation"},{"location":"admin/","title":"Administration","text":""},{"location":"admin/#uptime-system-status","title":"Uptime / System Status","text":"<p>The up/down status of NMDC Runtime components is available at https://nmdcstatus.polyneme.xyz/, which currently shows the https://updown.io/p/nia64 status page.</p> <p>A GET request is done for each registered HTTP resource every hour, with a satisfactory response time set as 2 seconds. There are three active monitoring locations in North America, and there are four active notification recipients - <code>dehays</code>, <code>dwinston</code>, and <code>scanon</code> via email; and the <code>#updown</code> channel in the NMDC Slack organization.</p> <p>The component containers are hosted on the NERSC Spin system, so NERSC's Live Status page is a place to check if anything is down. Furthermore, note the Planned Outages section of that page, particularly any notices for Spin.</p> <p>If NERSC Spin is up, and some service appears to be down, check out Spin's Rancher 2 web interface at https://rancher2.spin.nersc.gov/. The NMDC Runtime system is currently deployed on the <code>development</code> cluster as part of the NMDC's <code>m3408</code> project, under the <code>nmdc-runtime-dev namespace</code>. There, you can examine the workloads in the namespace and troubleshoot as appropriate -- redeploy, execute a shell in a container to diagnose, view logs, etc. Here is an example screenshot of active workloads:</p> <p></p>"},{"location":"admin/#create-api-users","title":"Create API Users","text":"<p>Users that are admins of the <code>nmdc-runtime-useradmin</code> site may create API users. Currently, these users are <code>scanon</code>, <code>dehays</code>, and <code>dwinston</code>.</p> <p>You can see what sites you administer via <code>GET /users/me</code> when logged in.</p> <p>example <code>GET /users/me</code> result</p> <pre><code>{\n  \"username\": \"dwinston\",\n  \"site_admin\": [\n    \"dwinston-laptop\",\n    \"nmdc-runtime-useradmin\"\n  ]\n}\n</code></pre> <p>Log in via your username and password, and <code>POST /users</code> to create a new user. The only required fields are <code>username</code> and <code>password</code>.</p>"},{"location":"admin/#modifying-api-permissions","title":"Modifying API permissions","text":"<p>For fine-grained control over which users have access to particular API endpoints, you may currently do so at the code level. A suitable template for this is the implementation of the POST /users endpoint. The endpoint code receives the requesting user model via the <code>get_current_active_user</code> dependency, and it uses the <code>check_can_create_user</code> function to verify that the requester can administer the \"nmdc-runtime-useradmin\" site. If not, a <code>403 Forbidden</code> expection is raised. Otherwise, the endpoint logic continues for the authorized user.</p> <p>To add a site ID to an existing user's <code>site_admin</code> list, this must currently be done manually at the MongoDB document level, rather than via an admin-accessible API endpoint.</p>"},{"location":"admin/#mongodb-administration","title":"MongoDB Administration","text":"<p>The MongoDB instance backing the runtime is deployed on NERSC Spin.</p> <p>The root admin password is stored as the <code>mongo-root-password</code> secret in the <code>nmdc-runtime-dev</code> namespace on the Spin k8s development cluster (link).</p> <p><code>scanon</code> and <code>dehays</code> have <code>dbOwner</code> roles on the <code>nmdc</code> database.</p> <p>Tip</p> <p>There is a <code>nersc-ssh-tunnel</code> target in the repository's <code>Makefile</code> that can help you map the remote mongo database to a port on your local machine.</p>"},{"location":"admin/#deployment","title":"Deployment","text":"<p>The release process is administered by the NMDC architecture working group GitHub team. Members of this team have full access to repository administration, including the GitHub Actions.</p> <p>As for the deployed infrastructure, when manual intervention may be necessary, first check the Rancher 2 web interface to the NERSC Spin service's Kubernetes clusters, i.e. https://rancher2.spin.nersc.gov/. The Runtime system is currently deployed on the development cluster as part of the NMDC's <code>m3408</code> project, under the <code>nmdc-runtime-dev</code> namespace.</p> <p>The go-to people to troubleshoot deployment issues within NERSC Spin at this time are <code>shreyas</code>, <code>dwinston</code>, <code>eecavanna</code>, and <code>scanon</code>.</p>"},{"location":"admin/#databases","title":"Databases","text":"<p>Data is stored in the <code>nmdc</code> database.</p> <p>If you need to delete objects copy it to <code>nmdc_deleted</code> database under the corresponding collection</p>"},{"location":"admin/#nmdc-schema-update","title":"nmdc-schema update","text":"<ol> <li>Set the desired version of <code>nmdc-schema</code> in <code>requirements/main.in</code>.</li> <li><code>make update-deps</code>.</li> <li>commit and push, start PR and seek approval + merge to <code>main</code>, which will trigger GH actions to deploy.</li> </ol>"},{"location":"contributing-docs/","title":"Contributing to the Docs","text":"<p>Reference for contributing to this documentation.</p>"},{"location":"contributing-docs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"contributing-docs/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"draft/","title":"[Draft] API Simplification","text":"<p>The following is a draft proposal to simplify the design of the Runtime API.</p>"},{"location":"draft/#job-run-1n-metadata-submission","title":"Job run \u2014(1:N)\u2014 metadata submission","text":"<p>Every metadata submission is associated with a job run. There is a special <code>custom</code> job in case you cannot associate your submission with a known job-to-be-done for a known workflow. You can mark a job for which you are submitting metadata as already done, so that you need only send one API request. See the sequence diagrams below for more elaboration on this.</p>"},{"location":"draft/#branch-and-merge","title":"Branch and merge","text":"<p>All metadata submitted as part of a job run is staged separately from the main, validated metadata. You can consider each job run as having a \"branch\" that stores only how it differs from the \"main\" metadata.</p> <p>When a job run is marked <code>done:true</code>, that triggers a \"pull request\" to merge the job-run branch into the main branch. It is at this point that arbitrary integrity checks can be performed against the proposed new whole of the metadata, and that any JSON Schema validation failure will cause a merge error. Before a job run is marked done, metadata submissions for the run that fail schema validation will return warnings, but the submission will be accepted.</p>"},{"location":"draft/#example-registering-a-run-of-a-workflow-job","title":"Example: Registering a run of a workflow job","text":"<p>There is no need to \"claim\" a job. If you see a job you want to run, or that you have already run, then POST a new (<code>done:false</code>, the default) run for the job <code>job_id</code> so that your intent is registered. You will get back a run ID <code>run_id</code> to (a) inform the Runtime about status updates for the run, and (b) as a token to submit metadata for the job run.</p> <pre><code>sequenceDiagram\n    participant Agent\n    participant API  \n    Agent-&gt;&gt;API: POST /jobs/{job_id}/runs &lt;br/&gt;\u2026\n    API-&gt;&gt;Agent: {done:false, status:waiting, id:&lt;run_id&gt;}&lt;br/&gt;\u2026</code></pre>"},{"location":"draft/#example-submitting-metadata-associated-with-a-job-run-and-marking-it-done","title":"Example: Submitting metadata associated with a job run, and marking it done","text":"<p>When you have metadata to submit for a job run, submit it by POSTing it to the appropriate metadata collection endpoint. Include the run ID <code>run_id</code> that represents the job run. If the run is done and there is no more metadata to submit, mark the run as <code>done:true</code>.</p> <pre><code>sequenceDiagram\n    participant Agent\n    participant API  \n    Agent-&gt;&gt;API: POST /biosamples {run: {id:&lt;run_id&gt;, done:true}, in:[{...}]}&lt;br/&gt;\u2026\n    API-&gt;&gt;Agent: {run: {id:&lt;run_id&gt;, done:true, status:success}, result: {...}}&lt;br/&gt;\u2026</code></pre>"},{"location":"draft/#example-one-off-custom-metadata-submission","title":"Example: One-off custom metadata submission","text":"<p>There is an evergreen job, with a <code>job_id</code> of <code>custom</code>, for custom metadata submission not associated with a specific job for a known workflow. For example, if you want to submit metadata collected from a spreadsheet that represent one-off corrections or additions, you may accomplish this with a single POST.</p> <pre><code>sequenceDiagram\n    participant Agent\n    participant API\n    Agent-&gt;&gt;API: POST /biosamples {run: {done:true, job_id: custom}, in: [{...}]&lt;br/&gt;\u2026\n    API-&gt;&gt;Agent: {run: {id:&lt;run_id&gt;, done:true, status:success}, result: {...}}&lt;br/&gt;\u2026</code></pre>"},{"location":"draft/#example-multi-step-custom-metadata-submission","title":"Example: Multi-step custom metadata submission","text":"<p>Just like with any job, you may spread metadata submissions for a <code>custom</code> job over multiple sessions, as long as you retain the run ID <code>run_id</code>. The benefit of the multi-step flow is that your metadata need only be schema-compliant when you mark the run as <code>done:true</code>, that is, when you wish to merge your submission as a whole into the main database.</p> <pre><code>sequenceDiagram\n    participant Agent\n    participant API  \n    Agent-&gt;&gt;API: POST /biosamples {run: {job_id: custom}}, in:[{...}]}&lt;br/&gt;\u2026\n    API-&gt;&gt;Agent: {run: {id:&lt;run_id&gt;, done:false, status:waiting}, result: {...}}&lt;br/&gt;\u2026\n    Agent-&gt;&gt;API: POST /biosamples {run: {id:&lt;run_id&gt;, done:true}, in: [{...}]}&lt;br/&gt;\u2026\n    API-&gt;&gt;Agent: {run: {id:&lt;run_id&gt;, done:true, status:success}, result: {...}}&lt;br/&gt;\u2026</code></pre>"},{"location":"explanation/domain-vision-statement/","title":"Domain Vision Statement","text":"<p>The model will represent the status and nature of studies involving biosamples  in such a way that automated routing for computational analysis can be supported. </p> <p>Biosamples are acquired from an environment; they are prepared for analysis using protocols; they are subject to analysis by instruments that output raw data; and they can be associated thematically with one or more studies.</p> <p>A study, conducted by a research team to answer research questions, collects (a) a set of biosamples, associated instrument-obtained (\"raw\") data, and/or associated computation-obtained (\"derived\") data; and (b) performs additional analysis activities in order to answer research questions.</p> <p>The model represents classes of operational activities for (1) environmental sampling, (2) biosample preparation for instrument analysis, (3) instrument-analysis runs, and (4) computational-analysis workflow executions.</p> <p>The model represents metadata for biosamples, studies, and their associated operational activities in such a way that researcher teams can leverage existing data in order to more easily answer research questions with minimal duplication of effort, by either obtaining answers from the data of existing studies, or by designing new studies that maximally leverage existing data.</p>"},{"location":"explanation/domain-vision-statement/#about","title":"About","text":"<p>A Domain Vision Statement<sup>1</sup> is a short description (about one page) of the core domain and the value it will bring, the \"value proposition.\" Ignore those aspects that do not distinguish this domain model from others. Show how the domain model serves and balances diverse interests. Keep it narrow. Write this statement early and revise it as you gain new insight.</p> <p>It should be usable directly by the management and technical staff during all phases of development to guide resource allocation, to guide modeling choices, and to educate team members.</p>"},{"location":"explanation/domain-vision-statement/#references","title":"References","text":"<ol> <li> <p>Domain Vision Statement, pp415-6, in Evans, Eric. Domain-Driven Design: Tackling Complexity in the Heart of Software. Boston: Addison-Wesley, 2004.\u00a0\u21a9</p> </li> </ol>"},{"location":"explanation/identifiers/","title":"Identifier Minting and Administration","text":"<p>How does identifier minting and administration work in NMDC? </p>"},{"location":"explanation/identifiers/#minting","title":"Minting","text":"<p>A minting request has these attributes:</p> <ul> <li><code>service</code>: a reference to a minter service, e.g. the central minter service</li> <li><code>requester</code>: a reference to a requesting agent, e.g. a person</li> <li><code>schema_class</code>: a reference to a NMDC Schema class, e.g. nmdc:Biosample.</li> <li><code>how_many</code>: how many new <code>schema_class</code> identifiers the <code>requester</code> wants <code>service</code> to mint</li> </ul> <p>Assuming the <code>requester</code> is authenticated and is authorized by the <code>service</code> to mint identifiers, one or more draft identifiers are minted. Each minted identifier has these attributes:</p> <ul> <li><code>name</code>: a literal string value that adheres to the scheme of nmdc:id, e.g. \"nmdc:bsm-11-abc123\".</li> <li><code>typecode</code>: a reference to the ID's typecode component, e.g. with <code>name</code> \"bsm\" and <code>schema_class</code> nmdc:Biosample.</li> <li><code>shoulder</code>: a reference to the ID's shoulder component, e.g. with <code>name</code> \"11\" and <code>assigned_to</code> the <code>service</code> that minted the ID.</li> <li><code>status</code>:  a reference to the ID's status, e.g. \"draft\".</li> </ul> <pre><code>flowchart LR\n    %% o1[/\"(input/output) object\"/]\n    %% s1[\"(process) step\"]\n    %% t1([\"terminator (process start/end)\"])\n    %% d1{decision}\n    %% b1[(\"(data)base\")]\n    %% m1[\\manual step/]\n\n    %% p1[[predefined step]]\n    classDef literal stroke-dasharray:1 4;\n    classDef user stroke-dasharray:8 8;\n\n\n    %% request-handler user-story model-diagram\n\n    o_typecodename[/\"#quot;bsm#quot;\"/]:::literal\n    o_idname[/\"#quot;nmdc:bsm-11-abc123#quot;\"/]:::literal\n    o_shouldername[/\"#quot;11#quot;\"/]:::literal\n    o_howmany[/\"1\"/]:::literal\n    o_request[/\"minting request\"/]:::user\n    o_shoulder[/\"11\"/]\n    o_typecode[/\"bsm\"/]\n    o_schemaclass[/\"nmdc:Biosample\"/]\n    o_service[/\"Central Minting Service\"/]\n    o_requester[/Alicia/]\n    o_id_draft1[/draft 1 of ID/]\n    o_draft[/draft/]\n\n    s_minting[minting]\n\n    o_typecode-. name .-&gt;o_typecodename\n    o_typecode-. schema_class .-&gt;o_schemaclass\n\n    click o_schemaclass href \"https://microbiomedata.github.io/nmdc-schema/Biosample\"\n\n    o_request-. service .-&gt;o_service\n    o_request-. requester .-&gt;o_requester\n    o_request-. schema_class .-&gt;o_schemaclass\n    o_request-. how_many .-&gt;o_howmany\n\n    o_request--&gt;|starts|s_minting[minting]\n\n    o_shoulder-. assigned_to .-&gt; o_service\n    o_shoulder.-&gt;|name|o_shouldername\n\n    s_minting-- draft_identifier --&gt;o_id_draft1\n    o_id_draft1-. name .-&gt;o_idname\n    o_id_draft1-. shoulder .-&gt;o_shoulder\n    o_id_draft1-. typecode .-&gt; o_typecode\n    o_id_draft1-. status .-&gt; o_draft</code></pre> <p>All typecodes and shoulders are sourced from the database.</p> <p>All minted identifiers are persisted to the database.</p>"},{"location":"explanation/journeys/","title":"User Journeys","text":"<p>Re: \"do workflows need to know NMDC specifics\", I've drafted a set of user journey diagrams with \"Workflow\" and \"NMDC\" as actors.</p> <p>The first, \"Keep NMDC in the know\", is the current intended Workflow executioner journey, and is implemented by the NMDC Runtime:</p> <pre><code>journey\n    title Keep NMDC in the know\n    section Ensure and claim job\n      Register metadata for inputs: 2: Workflow, NMDC\n      Spawn job to be done: 3: NMDC\n      Claim advertised job: 3: Workflow, NMDC\n    section Job executiom\n      Execute workflow: 5: Workflow\n      Register execution status: 3: Workflow, NMDC\n    section Output metadata registration\n      Register data objects: 2: Workflow, NMDC\n      Register other metadata: 2: Workflow, NMDC</code></pre> <p>The second journey, \"Tell NMDC after-the-fact\", is also currently supported by the implementation, and is similar to the first journey, but there is no real-time provenance / status update reporting:</p> <pre><code>journey\n    title Tell NMDC after-the-fact\n    section Ensure and claim job\n      Register metadata for inputs: 2: Workflow, NMDC\n      Spawn job to be done: 3: NMDC\n      Claim advertised job: 3: Workflow, NMDC\n    section Output metadata registration\n      Register completed execution: 3: Workflow, NMDC\n      Register data objects: 2: Workflow, NMDC\n      Register other metadata: 2: Workflow, NMDC</code></pre> <p>The third journey, \"Discover via NMDC, and give back\", is aspirational. In current practice, Workflow executioners already know what they want to do, already know the relevant input data objects / metadata, and are simply informing the Runtime of what they did / are doing in the language of the NMDC schema. This last journey reflects some feedback we've gotten. For example, folks would like to register transformation functions with NMDC so that NMDC can e.g. generate schema-complaint JSON from a workflow's native GFF output files:</p> <pre><code>journey\n    title Discover via NMDC, and give back\n    section Create job to be done\n      Sense relevant metadata updates: 3: NMDC\n      Spawn and advertise job to be done: 3: NMDC\n    section Job execution\n      Register job-started status: 5: Workflow, NMDC\n      Execute workflow, register status updates: 5: Workflow, NMDC\n    section Output metadata registration\n      Register data objects: 2: Workflow, NMDC\n      Register some other metadata: 2: Workflow, NMDC\n    section Additional metadata generation\n      Sense relevant new data objects: 3: NMDC\n      Generate schema-compliant JSON: 3: NMDC</code></pre>"},{"location":"howto-guides/author-changesheets/","title":"Authoring Changesheets","text":""},{"location":"howto-guides/author-changesheets/#introduction","title":"Introduction","text":"<p>Changesheets is a mechanism to update records (JSON documents) that have already been ingested into MongoDB. Meaning, you can update an already ingested document - you can insert new keys on a document, delete/remove certain keys on a document, or even update the value corresponding to a key in a document. But it is important to note that you cannot create/delete whole new JSON documents via changesheets.</p> <p>If you look at the changesheet workflow diagram below, you'll see that there are two main steps in the workflow - validation of the changesheet, and upon successful validation, submission of the changesheet.</p> <p>There are two endpoints in the NMDC runtime API infrastructure that you can use:</p> <ul> <li>validation: <code>/metadata/changesheets:validate</code></li> <li>submission: <code>/metadata/changesheets:submit</code></li> </ul> <p>The Swagger UI, which is one of the ways in which you can interact with the suite of endpoints that the runtime API provides and which also serves as the primary API documentation reference, is accessible here: https://api.microbiomedata.org/docs.</p> <p>Changesheets are typically specified as TSV files with pre-defined column names. There are four columns that need to be defined - id, action, attribute and value.</p>"},{"location":"howto-guides/author-changesheets/#changesheet-workflow","title":"Changesheet Workflow","text":"<p>The various steps that are involved in a typical changesheet workflow are as summarized in the below flowchart.</p> <pre><code>flowchart LR;\n    A[Create changesheet template] --&gt; B[Fill changesheet];\n    B --&gt; C[Validate changesheet];\n    C --&gt; D[Submit changesheet];</code></pre> <ol> <li> <p>The first step is to create a TSV file that follows the standard changesheet template. A changesheet has the following columns:</p> <ul> <li><code>id</code>: The id value corresponding to the id of the JSON document in the database. Specifying this will tell the changesheet what record in the database needs to be modified. There are no restrictions on the ids that can be modified. For example, it can be a Biosample id (with typecode bsm), or a Study id (with typecode sty), or another class of id.</li> <li><code>action</code>: The action to be performed on the database. It may be one of the following: <ul> <li><code>insert</code> / <code>insert item</code> / <code>insert items</code>: Add new values to a multivalued field, i.e., a field/key on a document which captures a list of values instead of single values.</li> <li><code>remove</code>: Drop a key/value pair for a single-value slot. Leave <code>value</code> field empty on changesheet.</li> <li><code>remove item</code> / <code>remove items</code>: Remove item(s) from a list/set of values for a multivalued slot. </li> <li><code>update</code> / <code>set</code> / <code>replace</code> / <code>replace items</code>: Update the value of a particular field/key on a document and replace it with a new value.</li> </ul> </li> <li><code>attribute</code>: the name of the field/key in the NMDC JSON document that is to be modified.</li> <li><code>value</code>: New value, which may be added (if it wasn't present already) to a multi-valued field for an <code>insert</code> action. For an <code>update</code> action, it will overwrite any current value.</li> </ul> </li> <li> <p>The second step is to fill out the rows of your changesheet appropriately as described above.</p> <p>Note</p> <p>A couple of notes to keep in mind when you are modifying a key in your document, the value of which has some substructure to it:</p> <ul> <li>If you are modifying/replacing the value of a key in a document which has a dictionary structure, and you want to modify individual keys/attributes in that dictionary, then changesheets employs a dot syntax to indicate those. For example, if you wanted to modify a field called depth within the records in the <code>biosample_set</code> collection, and specifically the <code>has_numeric_value</code> portion that can be asserted on depth fields, then the <code>attribute</code> column would have a value like this: <code>depth.has_numeric_value</code>. Another way in which you can rewrite the same syntax as above is by using symbolic names. Consider another field called doi which is a key on documents in the <code>study_set</code> collection. You can specify a symbolic name in the <code>value</code> column of the changesheet, say something like <code>v1</code>. Then you can specify what attribute of the substructure you want to modify on another row for <code>v1</code> in the changesheet. Let's say you want to modify the <code>has_raw_value</code> component of the <code>doi</code>, then you would simply indicate <code>has_raw_value</code> in the attribute column of the changesheet (on the row corresponding to <code>v1</code>). You can find an example of a changesheet using symbolic names here.</li> <li>If you are modifying/replacing the value of a key in a document which has a list structure, then specifying a single value in the value column of the changesheet will be interpreted as a single element list. If you wanted to replace it with a multi-element list, then you can specify the individual elements of that list value in a pipe (<code>|</code>) delimited fashion in the value column of the changesheet</li> </ul> </li> <li> <p>The third step is to use the validation endpoint from the runtime API which you can find here. Click on Try it out, and upload your TSV file. You should see a <code>200</code> successful response for proper validation.</p> </li> <li> <p>The fourth and final step in the protocol is to actually submit the changesheet using the submission endpoint from the nmdc-runtime API which you can find here. For this, you must be logged in using your username/password (click on any lock icon) and authorized to submit changesheets. Click on Try it out, and upload your TSV file. Similar to the validation endpoint, you should see a <code>200</code> successful response on execution the request. For an example submission changesheet, see here.</p> </li> </ol> <p>Note</p> <p>The submission endpoint runs the validation endpoint prior to actually submitting the data.</p>"},{"location":"howto-guides/claim-and-run-jobs/","title":"Claim and Run Jobs","text":"<p>The Runtime advertises jobs to be done, where a Job is a Workflow paired with a chosen input Object. See Guide - Create Workflow Triggers To Spawn Jobs to learn how to arrange for jobs of interest to be automatically available when relevant new workflow inputs are available.</p> <p>You can list open jobs via <code>GET /jobs</code>. To claim a job, <code>POST</code> to <code>/jobs/{job_id}:claim</code>. The response will be an Operation to track job execution and send updates to the system.</p> <p><code>PATCH /operations/{op_id}</code> is how to report state transitions for the job execution. If a job execution has been completed successfully, has failed, or has been aborted, the <code>done</code> field for the operation must be set to <code>true</code>.</p> <p>Example sensors that claim and run jobs are available in the codebase in <code>nmdc_runtime.site.repository</code>:</p> <ul> <li> <p><code>claim_and_run_metadata_in_jobs</code> senses new jobs to ingest NMDC-Schema-compliant metadata. Users   submit metadata as Objects and associate them with the <code>metadata-in</code> Object Type. This in turn   triggers the creation of Jobs (for the <code>metadata-in-1.0.0</code> Workflow) to validate and ingest the   metadata. Because these jobs are not too intensive wrt data or compute, the Runtime itself hosts a   Site that claims and runs these jobs. *</p> </li> <li> <p><code>claim_and_run_apply_changesheet_jobs</code> senses new jobs to apply changesheets, that is, to apply   surgical updates to existing metadata. Users submit changesheets via <code>POST   /metadata/changesheets:submit</code>,   which creates an Object on behalf of the user, annotates it with the <code>metadata-changesheet</code> Object   Type. A registered trigger then creates a corresponding <code>apply-changesheet-1.0.0</code> job, which this   sensor senses, allowing the Runtime Site to claim and run the job.</p> </li> <li> <p><code>claim_and_run_gold_translation_curation</code> is another example. The jobs that this sensor claims and   runs are created not by a conventionally registered Trigger, but instead by another sensor,   <code>ensure_gold_translation_job</code>. This pattern may be appropriate if the logic to trigger job creation   is more nuanced and would benefit from being expressed using Python code rather than as a simple   data structure as in   <code>nmdc_runtime.api.boot.triggers</code>.</p> </li> </ul> <p>If your workflow is neither data- nor resource-intensive, you may opt to implement it as a Dagster Graph of Ops (Operations) within a Runtime Site Repository, e.g. to include it in the <code>nmdc_runtime.site.repository</code> module with the above examples. Otherwise, NMDC workflow jobs are generally run at Sites that have access to more expensive and specialized resources, with job execution state and outputs communicated and delivered to the Runtime via the API.</p>"},{"location":"howto-guides/create-triggers/","title":"Create Workflow Triggers To Spawn Jobs","text":"<p>Jobs represent work to be done. Specifically, a Job is a request to run a Workflow given a certain Object as its input. The Runtime system will spawn a new Job when:</p> <ol> <li>An Object,</li> <li>is assigned an Object Type,</li> <li>which is associated with a Workflow,</li> <li>through a Trigger.</li> </ol> <p> </p> Job Triggering <p>Thus, to empower the Runtime to recognize and spawn new jobs to be done, a new Trigger must be registered with the system. Add a trigger to the codebase in <code>nmdc_runtime.api.boot.triggers</code>. You will need an Object Type ID and a Workflow ID</p> <p>Workflows are also registered with the system via a code module, <code>nmdc_runtime.api.boot.workflows</code>. Find your target workflow ID there, or add it there if need be.</p> <p>Finally, Object Types are registered in a similar manner, via <code>nmdc_runtime.api.boot.object_types</code>. Find your target object type ID there, or add it there if need be.</p> <p>Once a valid trigger is registered with the system, the <code>process_workflow_job_triggers</code> Sensor in <code>nmdc_runtime.site.repository</code> can sense when there are object IDs tagged with object types corresponding to a registered trigger. If a job hasn't yet been created for the object-type-triggered workflow with the given object as input, the Runtime will create this job so that a site can claim and run the job.</p>"},{"location":"howto-guides/improving-search-api/","title":"Improving the Search API","text":"<p>A standard operating procedure (SOP) for search API maintainers/contributors.</p> <p>The \"Search API\" may be thought of that part of the Runtime API that is both read-only and is particular to the needs of (meta)data consumers to retrieve NMDC data products that conform to the NMDC schema.</p> <p>The \"Data Management API\", on the other hand, is all other aspects of the runtime API -- that is, the read-write parts that also serve the needs of data producers and data stewards.</p>"},{"location":"howto-guides/improving-search-api/#endpoints","title":"Endpoints","text":"<p>Endpoints for the Search API are defined in the <code>nmdc_runtime.api.endpoints.find</code> module.</p> <p>To add an endpoint, you will likely use the <code>FindRequest</code> and <code>FindResponse</code> models in <code>nmdc_runtime.api.models.util</code>, as they are the shared request/response models used by existing Search API endpoints.</p> <p>You will also likely use the <code>find_resources</code> helper function from <code>nmdc_runtime.api.endpoints.util</code>. Improvements to this helper function will improve all existing Search API endpoints.</p> <p>Adding a dedicated endpoint for a particular resource collection may be as simple as copying the code for a representative pair of endpoints, such as <code>GET /studies</code> and <code>GET /studies/{study_id}</code>, and changing names accordingly.</p>"},{"location":"howto-guides/improving-search-api/#index-backed-filter-attributes","title":"Index-backed Filter Attributes","text":"<p>In order to ensure an index for a particular attribute/slot of a collection entity, add it to the <code>entity_attributes_to_index</code> dictionary in the <code>nmdc_runtime.api.models.util</code> module. Each key of that dictionary is the collection name for the entity, e.g. <code>biosample_set</code>, and each value corresponding to a key is the set of attributes, e.g. <code>ecosystem</code> and <code>collection_date.has_raw_value</code>, for which an index will be ensured.</p> <p>Currently, due to the limitations of the database technology (MongoDB) that backs the API, a single collection can have no more than 64 indexes.</p> <p>When the API server code is re-deployed, the <code>ensure_indexes</code> startup hook in the <code>nmdc_runtime.api.main</code> module is run, which fetches <code>entity_attributes_to_index</code> and ensures the corresponding indexes exist.</p>"},{"location":"howto-guides/release-process/","title":"Release Process","text":""},{"location":"howto-guides/release-process/#nmdc-runtime-releases","title":"NMDC Runtime Releases","text":"<p>How do new versions of the API and NMDC Runtime site (Dagster daemon and Dagit frontend) get released? Here's how.</p> <ol> <li> <p>Ensure the tests pass (i.e., a \"smoke test\").</p> <ul> <li>Either run tests locally via   <pre><code>make up-test\nmake test\n</code></pre></li> <li>or confirm the test pass via   our python-app.yml GitHub  action,  which is triggered automatically when a change to any Python file in the repository is pushed to the  <code>main</code> branch, or to a Pull Request. You can monitor the status of GitHub Actions  here.</li> </ul> </li> <li> <p>Create a new GitHub Release. When creating the new release:</p> </li> <li>Use the \"Choose a tag\" dropdown to create a new tag by typing in a tag name which does not exist yet.</li> <li>The new tag name should start with <code>v</code> and be followed by a semantic version number, for example <code>v3.0.2</code>.</li> <li>If the last published version is <code>vX.Y.Z</code>, the next release number should be <code>vX.Y.{Z+1}</code> for a patch release (bux fixes and refactoring), <code>vX.{Y+1}.0</code> for a minor release (new functionality that is backwards-compatible), or <code>v{X+1}.0.0</code> for a major release (new functionality that may be backwards-incompatible).</li> <li> <p>You may leave the \"Release title\" input blank. If it is blank the release title will be populated with the tag name.</p> <p>Once the GitHub Release has been created, two GitHub Actions will be triggered which:</p> </li> <li> <p>build Docker images and deploy them to Spin for the API server and for the NMDC Runtime site's Dagster daemon and Dagit dashboard.</p> </li> <li> <p>build a Python package and publish it to PyPI.</p> </li> </ol>"},{"location":"howto-guides/release-process/#data-releases","title":"Data Releases","text":"<p>In order to make sure the schema, database, and NMDC Runtime API are in sync we need to coordinate data updates that require schema changes. </p> <p>Here is a summary of the process:</p> <ol> <li>NMDC Schema repo releases new version. All releases must include a migration script (even if it is null / empty) to run against MongoDB. See ADR 007</li> <li>Submit/Merge a PR with updated schema version and any related code changes.</li> <li>Build a new NMDC-runtime image so that it is ready to be deployed (See above). </li> <li>Database (Mongo) is switched to read-only mode to prevent inconsistencies.<ul> <li>TODO: decide on process for read-only mode</li> </ul> </li> <li>Run <code>mongodump</code> to dump database on local machine<ul> <li>TODO: document <code>mongodump</code> command</li> <li>FUTURE: improved process for doing inline DB migrations</li> </ul> </li> <li>Run migration script runs against database on local machine (to migrate data)<ul> <li>TODO: Finalize location and instructions for migration script</li> </ul> </li> <li>Run validation to make sure database on local machine adheres to updated schema version<ul> <li>TODO: Steps for validation</li> </ul> </li> <li>If validation succeeds, run <code>mongorestore</code> to update database<ul> <li>TODO: Steps for <code>mongorestore</code></li> </ul> </li> <li>Database (Mongo) is switched from read-only mode back to original mode.</li> <li>Upgrade NMDC-runtime repo to latest version in Spin</li> </ol>"},{"location":"howto-guides/update-sensors-ops/","title":"Update Sensors, Jobs, Graphs, and Ops","text":"<p>This guide will walk through the operation of the underlying Dagster orchestrator used by the Runtime, using a particular Sensor as the entry point.</p> <p>A Dagster Repository is a collection of code that defines how orchestration is to be done. The <code>nmdc_runtime.site.repository</code> module exposes three such repositories via the <code>@repository</code> decorator. The creatively named <code>repo</code> repository is the main one. The <code>translation</code> and <code>test_translation</code> repositories are used for GOLD database translation jobs.</p> <p>Why multiple repositories? A given repository may require resources that a given Dagster deployment may not provide -- it is nice to opt-in to serve a given repository of functionality.</p> <p>A Dagster Workspace loads one or more repositories for a given deployment.</p> <p>A Dagster Sensor defines a process that will be run over and over again at a regular, relatively tight interval such as every 30 seconds. The <code>claim_and_run_apply_changesheet_jobs</code> sensor, defined in <code>nmdc_runtime.site.repository</code> via the <code>@sensor</code> decorator, is the example we'll explore here.</p> <p>A sensor decides, via the code in its body, to yield one or more <code>RunRequest</code>s, requests to run a particular job with a particular configuration. A sensor may also yield a <code>SkipReason</code> to log why no job requests were yielded for a particular run of the sensor.</p> <p>What is a Dagster \"job\"? A Dagster Job is an executable (directed acyclic) graph of operations. Breaking this down, a Dagster Graph will define abstract dependencies such as \"I need a resource called 'mongo' for my operations\". A Dagster Op, or Operation, is a Python function that performs work and that depends on resources, e.g. \"mongo\", that are made accessible to it at runtime. Finally, a Job is a configuration of a Graph that makes resources more concrete, e.g. \"by 'mongo', I mean instantiate this Python class, passing it these parameters with values fetched from these environment variables.\"</p> <p>In the case of the <code>claim_and_run_apply_changesheet_jobs</code> sensor, the kind of job that it yields <code>RunRequest</code>s for is given as an argument to the <code>@sensor</code> decorator, i.e.</p> <pre><code>@sensor(job=apply_changesheet.to_job(**preset_normal))\ndef claim_and_run_apply_changesheet_jobs(_context):\n    ...\n</code></pre> <p>where <code>apply_changesheet</code> is a Graph definition and <code>preset_normal</code> is a Python dictionary that supplies resource definitions (a mapping of resource names to Python functions decorated with <code>@resource</code>) as well as configuration for the resources (incl. specifying environment variables to source).</p> <p>The <code>apply_changesheet</code> Graph is defined in <code>nmdc_runtime.site.graphs</code>  as follows:</p> <pre><code>@graph\ndef apply_changesheet():\n    sheet_in = get_changesheet_in()\n    perform_changesheet_updates(sheet_in)\n</code></pre> <p>which is rendered in Dagster's Dagit UI as a graph:</p> <p> </p> Dagit UI rendering of `apply_changesheet` job <p>Thus, Dagster inspects the Python code's abstract syntax tree (AST) in order to create a graph of operation nodes and their input/output dependencies. You can explore the Dagit rendering of this job at <code>/workspace/repo@nmdc_runtime.site.repository:repo/jobs/apply_changesheet/</code> on your local instance (at <code>http://localhost:3000</code> by default) or e.g. the read-only deployed version.</p> <p>The operations comprising <code>apply_changesheet</code> are <code>@op</code>-decorated functions in <code>nmdc_runtime.site.ops</code>, i.e.</p> <pre><code>@op(required_resource_keys={\"mongo\"})\ndef get_changesheet_in(context) -&gt; ChangesheetIn:\n    mdb: MongoDatabase = context.resources.mongo.db\n    object_id = context.solid_config.get(\"object_id\")\n    ...\n</code></pre> <p>and </p> <pre><code>@op(required_resource_keys={\"mongo\"})\ndef perform_changesheet_updates(context, sheet_in: ChangesheetIn):\n    ....\n</code></pre> <p>Here you can see that Dagster operations (Ops) communicate what resources the need when they are run. Resources are injected as part of the <code>context</code> argument supplied to the function call. You can also see that other particular values that configure the job run, and thus are usable as input, can be passed to the specific solid being run (solid is Dagster's deprecated term for op, and <code>context</code> objects now have equivalent <code>op_config</code> attributes).</p> <p>This extra level of indirection allows different schemes for the execution of job steps, i.e. ops. For example, the default Dagster Executor executes each step in its own spawned system process on the host machine, i.e. multiprocessing. One can also decide to e.g. execute each step within its own Docker container or even within an ephemeral kubernetes pod. There is similar system flexibility at the Run Launcher level for each Job (configured graph of Ops) run, where one can launch jobs runs in new processes (the default), new Docker containers, etc.</p> <p>You can read more about options for Dagster Deployment and see how the NMDC Runtime deployment is configured via its dagster.yaml file.</p> <p>In summary, there are many \"touch points\" for a given job, such as how it is scheduled (e.g. via a Sensor or via a Schedule that is more akin to Cron), how it is configured to source particular Resources from a Graph template, and how the work of the Graph is split into a collection of Ops that declare their dependencies so that everything can be run in a suitable order. All of these touch points are code-based, and so any modifications made to a given job should be expressed as a git branch for a Pull Request.</p>"},{"location":"howto-guides/jobs/gold-translation-etl/","title":"GOLD Translation ETL","text":"<p>This job takes a <code>nmdc_merged_data.tsv.zip</code> file as a data object input.</p> <p>First, start your local runtime stack:</p> <pre><code>make up-dev\n</code></pre> <p>Load http://localhost:3000/workspace and ensure the <code>translation</code> repository is loaded:</p> <p></p> <p>Next, navigate to the Jobs page for the <code>translation</code> repository:</p> <p></p> <p>And finally, select the <code>gold</code> job from the table:</p> <p></p> <p>This will bring you to an overview of the job. Specifically, you will see the graph of operations that the job uses, as well as information about the resources needed for the operations. Click on the <code>Lauchpad</code> tab to examine the job's configuration:</p> <p></p> <p>In the launchpad, we see the configuration for the job, which includes a reference to a <code>nmdc_merged_data.tsv.zip</code> input file that was present in the nmdc-runtime git repository, accessible via the <code>/opt/dagster/lib/</code> path. We also see a button to launch a run of the job, i.e. a run of the graph of operations configured with the YAML shown:</p> <p></p> <p>This job is being run manually. Other jobs in the Runtime system may be launched via Dagster sensors that <code>yield RunRequest(run_key=..., run_config=...)</code> in the body of the sensor logic. Sensors can periodically (e.g. every ~30 seconds) poll the system for any activation conditions, such as the presence of new data objects of certain types. A sensor may then yield a <code>RunRequest</code> for an appropriate job with e.g. the data object ID in the job configuration.</p> <p>Once this translation ETL job finishes successfully, there will be new NMDC-schema-compliant metadata in your local MongoDB instance (which was also started by <code>make up-dev</code>). One option now is to export this metadata using the <code>mongoexport</code> command-line tool. Here's an example of exporting the <code>gold.biosample_set</code> collection and conforming it to the <code>nmdc:Database</code> JSON schema using the <code>jq</code> tool:</p> <p><pre><code>mongoexport --host=\"localhost:27018\" \\\n    -u admin -p root --authenticationDatabase admin \\\n    -d nmdc_etl_staging -c gold.biosample_set \\\n    --jsonArray -o gold.biosample_set.json\n</code></pre> <pre><code>jq '{biosample_set:.}' gold.biosample_set.json \\\n    &gt; gold.biosample_set.as_nmdcdb.2021-11-18.json\n</code></pre></p> <p>Now, you may submit this metadata as JSON through the API.</p> <p>In the tutorial linked to above, GitHub's Gist service is used to host and obtain a URL for the JSON. Here's let's walk through using the NMDC's <code>www</code> directory on NERSC to host the file and obtain the URL, as well as obtain the sha256 hash and file size in bytes we need in order to create a DRS object in the API:</p> <p><pre><code># Use https://docs.nersc.gov/connect/mfa/#sshproxy\nscp  gold.biosample_set.as_nmdcdb.2021-11-18.json \\\n    dtn01.nersc.gov:/global/cfs/cdirs/m3408/www/meta/\n</code></pre> <pre><code>ssh dtn01.nersc.gov\n\ncd /global/cfs/cdirs/m3408/www/meta/\n\n# ensure world-readable\nchmod 444 gold.biosample_set.as_nmdcdb.2021-11-18.json\n\n# get sha256 checksum\nopenssl dgst -sha256 gold.biosample_set.as_nmdcdb.2021-11-18.json\n\n# get bytes\ndu --bytes gold.biosample_set.as_nmdcdb.2021-11-18.json\n</code></pre></p> <p>Now, you can use the normal procedure to register the object, ensure the \"metadata-in\" tag, and monitor the progress of the validation and ingest.</p>"},{"location":"nb/bulk_validation_referential_integrity_check/","title":"Referential integrity checker (prototype)","text":"In\u00a0[1]: Copied! <pre>!echo $MONGO_HOST\n</pre> !echo $MONGO_HOST <pre>localhost:27018\n</pre> In\u00a0[2]: Copied! <pre># Ensure code changes in this notebook will be import-able  \n# without needing to restart the kernel and lose state\n%load_ext autoreload\n%autoreload 2\n</pre> # Ensure code changes in this notebook will be import-able   # without needing to restart the kernel and lose state %load_ext autoreload %autoreload 2 <p>Be sure you're using the version of <code>nmdc-schema</code> you think you are!</p> In\u00a0[3]: Copied! <pre>from importlib.metadata import version\n\nversion(\"nmdc-schema\")\n</pre> from importlib.metadata import version  version(\"nmdc-schema\") Out[3]: <pre>'11.1.0'</pre> In\u00a0[4]: Copied! <pre>from collections import defaultdict\nimport concurrent.futures\nfrom itertools import chain\nimport os\nimport re\n\nfrom linkml_runtime.utils.schemaview import SchemaView\nfrom pymongo import InsertOne\nfrom toolz import dissoc, assoc\nfrom tqdm.notebook import tqdm\n\nfrom nmdc_runtime.api.core.util import pick\nfrom nmdc_runtime.api.db.mongo import get_mongo_db, get_nonempty_nmdc_schema_collection_names, get_collection_names_from_schema\nfrom nmdc_runtime.util import collection_name_to_class_names, populated_schema_collection_names_with_id_field, nmdc_schema_view, nmdc_database_collection_instance_class_names, get_nmdc_jsonschema_dict\nfrom nmdc_schema.nmdc import Database as NMDCDatabase \nfrom nmdc_schema.get_nmdc_view import ViewGetter\n\nmdb = get_mongo_db()\nschema_view = nmdc_schema_view()\n</pre> from collections import defaultdict import concurrent.futures from itertools import chain import os import re  from linkml_runtime.utils.schemaview import SchemaView from pymongo import InsertOne from toolz import dissoc, assoc from tqdm.notebook import tqdm  from nmdc_runtime.api.core.util import pick from nmdc_runtime.api.db.mongo import get_mongo_db, get_nonempty_nmdc_schema_collection_names, get_collection_names_from_schema from nmdc_runtime.util import collection_name_to_class_names, populated_schema_collection_names_with_id_field, nmdc_schema_view, nmdc_database_collection_instance_class_names, get_nmdc_jsonschema_dict from nmdc_schema.nmdc import Database as NMDCDatabase  from nmdc_schema.get_nmdc_view import ViewGetter  mdb = get_mongo_db() schema_view = nmdc_schema_view() In\u00a0[5]: Copied! <pre>collection_names = populated_schema_collection_names_with_id_field(mdb) # `get_nonempty_nmdc_schema_collection_names` to include \"functional_annotation_agg\"\n</pre> collection_names = populated_schema_collection_names_with_id_field(mdb) # `get_nonempty_nmdc_schema_collection_names` to include \"functional_annotation_agg\" <p>Collect all possible classes of documents across all schema collections. <code>collection_name_to_class_names</code> is a mapping from collection name to a list of class names allowable for that collection's documents.</p> In\u00a0[6]: Copied! <pre>document_class_names = set(chain.from_iterable(collection_name_to_class_names.values()))\n</pre> document_class_names = set(chain.from_iterable(collection_name_to_class_names.values())) <p>Map each document-class name to a map of slot name to slot definition. Class slots here are (to quote the LinkML SchemaView documentation) \"all slots that are asserted or inferred for [the] class, with their inferred semantics.\"</p> In\u00a0[7]: Copied! <pre>cls_slot_map = {\n    cls_name : {slot.name: slot\n                for slot in schema_view.class_induced_slots(cls_name)\n               }\n    for cls_name in document_class_names\n}\n</pre> cls_slot_map = {     cls_name : {slot.name: slot                 for slot in schema_view.class_induced_slots(cls_name)                }     for cls_name in document_class_names } <p>The <code>alldocs</code> collection associates each database document's <code>id</code> with not only its class (via that document's <code>type</code> field) but also with all ancestors of the docuement's class.</p> <p>The set-of-classes association is done by setting the <code>type</code> field in an <code>alldocs</code> document to be a list, which facilitates filtering by type using the same strutured query forms as for upstream schema collections. The first element of the <code>type</code> list must correspond to the source document's asserted class; this is so that validation code can determine the expected range of document slots, as slot ranges may be specialized by a class (via linkml \"slot_usage\").</p> <p>To keep the <code>alldocs</code> collection focused on supporting referential-integrity checking, only document-reference-ranged slots from source documents are copied to an entity's corresponding <code>alldocs</code> materialization.</p> In\u00a0[8]: Copied! <pre># From https://github.com/microbiomedata/refscan/blob/af092b0e068b671849fe0f323fac2ed54b81d574/refscan/lib/helpers.py#L141-L176\n\nfrom typing import List\nfrom linkml_runtime import linkml_model\n\ndef get_names_of_classes_in_effective_range_of_slot(\n    schema_view: SchemaView, slot_definition: linkml_model.SlotDefinition\n) -&gt; List[str]:\n    r\"\"\"\n    Determine the slot's \"effective\" range, by taking into account its `any_of` constraints (if defined).\n\n    Note: The `any_of` constraints constrain the slot's \"effective\" range beyond that described by the\n          induced slot definition's `range` attribute. `SchemaView` does not seem to provide the result\n          of applying those additional constraints, so we do it manually here (if any are defined).\n          Reference: https://github.com/orgs/linkml/discussions/2101#discussion-6625646\n\n    Reference: https://linkml.io/linkml-model/latest/docs/any_of/\n    \"\"\"\n\n    # Initialize the list to be empty.\n    names_of_eligible_target_classes = []\n\n    # If the `any_of` constraint is defined on this slot, use that instead of the `range`.\n    if \"any_of\" in slot_definition and len(slot_definition.any_of) &gt; 0:\n        for slot_expression in slot_definition.any_of:\n            # Use the slot expression's `range` to get the specified eligible class name\n            # and the names of all classes that inherit from that eligible class.\n            if slot_expression.range in schema_view.all_classes():\n                own_and_descendant_class_names = schema_view.class_descendants(slot_expression.range)\n                names_of_eligible_target_classes.extend(own_and_descendant_class_names)\n    else:\n        # Use the slot's `range` to get the specified eligible class name\n        # and the names of all classes that inherit from that eligible class.\n        if slot_definition.range in schema_view.all_classes():\n            own_and_descendant_class_names = schema_view.class_descendants(slot_definition.range)\n            names_of_eligible_target_classes.extend(own_and_descendant_class_names)\n\n    # Remove duplicate class names.\n    names_of_eligible_target_classes = list(set(names_of_eligible_target_classes))\n\n    return names_of_eligible_target_classes\n</pre> # From https://github.com/microbiomedata/refscan/blob/af092b0e068b671849fe0f323fac2ed54b81d574/refscan/lib/helpers.py#L141-L176  from typing import List from linkml_runtime import linkml_model  def get_names_of_classes_in_effective_range_of_slot(     schema_view: SchemaView, slot_definition: linkml_model.SlotDefinition ) -&gt; List[str]:     r\"\"\"     Determine the slot's \"effective\" range, by taking into account its `any_of` constraints (if defined).      Note: The `any_of` constraints constrain the slot's \"effective\" range beyond that described by the           induced slot definition's `range` attribute. `SchemaView` does not seem to provide the result           of applying those additional constraints, so we do it manually here (if any are defined).           Reference: https://github.com/orgs/linkml/discussions/2101#discussion-6625646      Reference: https://linkml.io/linkml-model/latest/docs/any_of/     \"\"\"      # Initialize the list to be empty.     names_of_eligible_target_classes = []      # If the `any_of` constraint is defined on this slot, use that instead of the `range`.     if \"any_of\" in slot_definition and len(slot_definition.any_of) &gt; 0:         for slot_expression in slot_definition.any_of:             # Use the slot expression's `range` to get the specified eligible class name             # and the names of all classes that inherit from that eligible class.             if slot_expression.range in schema_view.all_classes():                 own_and_descendant_class_names = schema_view.class_descendants(slot_expression.range)                 names_of_eligible_target_classes.extend(own_and_descendant_class_names)     else:         # Use the slot's `range` to get the specified eligible class name         # and the names of all classes that inherit from that eligible class.         if slot_definition.range in schema_view.all_classes():             own_and_descendant_class_names = schema_view.class_descendants(slot_definition.range)             names_of_eligible_target_classes.extend(own_and_descendant_class_names)      # Remove duplicate class names.     names_of_eligible_target_classes = list(set(names_of_eligible_target_classes))      return names_of_eligible_target_classes In\u00a0[9]: Copied! <pre># Any ancestor of a document class is a document-referenceable range, i.e., a valid range of a document-reference-ranged slot.\ndocument_referenceable_ranges = set(chain.from_iterable(schema_view.class_ancestors(cls_name) for cls_name in document_class_names))\n\ndocument_reference_ranged_slots = defaultdict(list)\nfor cls_name, slot_map in cls_slot_map.items():\n    for slot_name, slot in slot_map.items():\n        if set(get_names_of_classes_in_effective_range_of_slot(schema_view, slot)) &amp; document_referenceable_ranges:\n            document_reference_ranged_slots[cls_name].append(slot_name)\n</pre> # Any ancestor of a document class is a document-referenceable range, i.e., a valid range of a document-reference-ranged slot. document_referenceable_ranges = set(chain.from_iterable(schema_view.class_ancestors(cls_name) for cls_name in document_class_names))  document_reference_ranged_slots = defaultdict(list) for cls_name, slot_map in cls_slot_map.items():     for slot_name, slot in slot_map.items():         if set(get_names_of_classes_in_effective_range_of_slot(schema_view, slot)) &amp; document_referenceable_ranges:             document_reference_ranged_slots[cls_name].append(slot_name) In\u00a0[10]: Copied! <pre># Drop any existing `alldocs` collection (e.g. from previous use of this notebook).\nmdb.alldocs.drop()\n\nBULK_WRITE_BATCH_SIZE = 2_000 # ensure bulk-write batches aren't too huge\n\n# Set up progress bar\nn_docs_total = sum(mdb[name].estimated_document_count() for name in collection_names)\npbar = tqdm(total=n_docs_total)\n\nfor coll_name in collection_names:\n    pbar.set_description(f\"processing {coll_name}...\")\n    requests = []\n    for doc in mdb[coll_name].find():\n        doc_type = doc['type'][5:] # lop off \"nmdc:\" prefix\n        slots_to_include = [\"id\", \"type\"] + document_reference_ranged_slots[doc_type]\n        new_doc = pick(slots_to_include, doc)\n        new_doc[\"_type_and_ancestors\"] = schema_view.class_ancestors(doc_type)\n        requests.append(InsertOne(new_doc))\n        if len(requests) == BULK_WRITE_BATCH_SIZE: \n            result = mdb.alldocs.bulk_write(requests, ordered=False)\n            pbar.update(result.inserted_count)\n            requests.clear()\n    if len(requests) &gt; 0:\n        result = mdb.alldocs.bulk_write(requests, ordered=False)\n        pbar.update(result.inserted_count)\npbar.close()\n\n# Prior to re-ID-ing, some IDs are not unique across Mongo collections (eg nmdc:0078a0f981ad3f92693c2bc3b6470791)\n\n# Ensure unique id index for `alldocs` collection.\n# The index is sparse because e.g. nmdc:FunctionalAnnotationAggMember documents don't have an \"id\".\nmdb.alldocs.create_index(\"id\", unique=True, sparse=True)\n\nprint(\"refreshed `alldocs` collection\")\n</pre> # Drop any existing `alldocs` collection (e.g. from previous use of this notebook). mdb.alldocs.drop()  BULK_WRITE_BATCH_SIZE = 2_000 # ensure bulk-write batches aren't too huge  # Set up progress bar n_docs_total = sum(mdb[name].estimated_document_count() for name in collection_names) pbar = tqdm(total=n_docs_total)  for coll_name in collection_names:     pbar.set_description(f\"processing {coll_name}...\")     requests = []     for doc in mdb[coll_name].find():         doc_type = doc['type'][5:] # lop off \"nmdc:\" prefix         slots_to_include = [\"id\", \"type\"] + document_reference_ranged_slots[doc_type]         new_doc = pick(slots_to_include, doc)         new_doc[\"_type_and_ancestors\"] = schema_view.class_ancestors(doc_type)         requests.append(InsertOne(new_doc))         if len(requests) == BULK_WRITE_BATCH_SIZE:              result = mdb.alldocs.bulk_write(requests, ordered=False)             pbar.update(result.inserted_count)             requests.clear()     if len(requests) &gt; 0:         result = mdb.alldocs.bulk_write(requests, ordered=False)         pbar.update(result.inserted_count) pbar.close()  # Prior to re-ID-ing, some IDs are not unique across Mongo collections (eg nmdc:0078a0f981ad3f92693c2bc3b6470791)  # Ensure unique id index for `alldocs` collection. # The index is sparse because e.g. nmdc:FunctionalAnnotationAggMember documents don't have an \"id\". mdb.alldocs.create_index(\"id\", unique=True, sparse=True)  print(\"refreshed `alldocs` collection\") <pre>  0%|          | 0/163175 [00:00&lt;?, ?it/s]</pre> <pre>refreshed `alldocs` collection\n</pre> <p>The resulting <code>alldocs</code> collection contains a copy of every document from every Mongo collection identified earlier. The copy has a subset of the key-value pairs as the original document, except that its <code>type</code> field contains a list of the names of its own class and all of its ancestor classes (whereas the original document's <code>type</code> field either is unset or contains its own class only).</p> In\u00a0[11]: Copied! <pre>def doc_assertions(limit=0, batch_size=2_000):\n    \"\"\"Yields batches of assertions to greatly speed up processing.\"\"\"\n    # Initialize progress bar.\n    pbar = tqdm(total=(mdb.alldocs.estimated_document_count() if limit == 0 else limit))\n    rv = []\n    for doc in mdb.alldocs.find(limit=limit):\n        # Iterate over each key/value pair in the dictionary (document).\n        for field, value in doc.items():\n            if field.startswith(\"_\") or field in (\"id\", \"type\"):\n                continue\n            acceptable_slot_classes = get_names_of_classes_in_effective_range_of_slot(\n                schema_view,\n                cls_slot_map[doc[\"type\"][5:]][field],\n            )\n            if not isinstance(value, list):\n                value = [value]\n            for v in value:\n                rv.append({\n                    \"id\": doc.get(\"id\", doc[\"_id\"]),\n                    \"id_is_nmdc_id\": \"id\" in doc,\n                    \"field\": field,\n                    \"value\": v,\n                    \"acceptable_slot_classes\": acceptable_slot_classes,\n                })\n                if len(rv) == batch_size:\n                    yield rv\n                    rv.clear()\n        pbar.update(1)\n    yield rv\n    pbar.close()\n</pre> def doc_assertions(limit=0, batch_size=2_000):     \"\"\"Yields batches of assertions to greatly speed up processing.\"\"\"     # Initialize progress bar.     pbar = tqdm(total=(mdb.alldocs.estimated_document_count() if limit == 0 else limit))     rv = []     for doc in mdb.alldocs.find(limit=limit):         # Iterate over each key/value pair in the dictionary (document).         for field, value in doc.items():             if field.startswith(\"_\") or field in (\"id\", \"type\"):                 continue             acceptable_slot_classes = get_names_of_classes_in_effective_range_of_slot(                 schema_view,                 cls_slot_map[doc[\"type\"][5:]][field],             )             if not isinstance(value, list):                 value = [value]             for v in value:                 rv.append({                     \"id\": doc.get(\"id\", doc[\"_id\"]),                     \"id_is_nmdc_id\": \"id\" in doc,                     \"field\": field,                     \"value\": v,                     \"acceptable_slot_classes\": acceptable_slot_classes,                 })                 if len(rv) == batch_size:                     yield rv                     rv.clear()         pbar.update(1)     yield rv     pbar.close() In\u00a0[12]: Copied! <pre>from pprint import pprint\n\nalldocs_ids = set(mdb.alldocs.distinct(\"id\"))\n\ndef doc_field_value_errors(assertions):\n    errors = {\"not_found\": [], \"invalid_type\": []}\n    # group assertions by referenced \"id\" value.\n    assertions_by_referenced_id_value = defaultdict(list)\n    for a in assertions:\n        assertions_by_referenced_id_value[a[\"value\"]].append(a)\n    # associate each referenced document id with its type.\n    doc_id_types = {}\n    for d in list(mdb.alldocs.find({\"id\": {\"$in\": list(assertions_by_referenced_id_value.keys())}}, {\"_id\": 0, \"id\": 1, \"type\": 1})):\n        doc_id_types[d[\"id\"]] = d[\"type\"]\n\n    for id_value, id_value_assertions in assertions_by_referenced_id_value.items():\n        if id_value not in alldocs_ids:\n            errors[\"not_found\"].extend(id_value_assertions)\n        else:\n            for a in id_value_assertions:\n                # check that the document-reported type for this id reference is kosher as per the referring slot's schema definition.\n                if doc_id_types[a[\"value\"]][5:] not in a[\"acceptable_slot_classes\"]:\n                    errors[\"invalid_type\"].append(a)\n\n    return errors\n\n\n# Initialize \"global\" error lists.\nerrors = {\"not_found\": [], \"invalid_type\": []}\n\nfor das in doc_assertions(batch_size=2_000):\n    rv = doc_field_value_errors(das)\n    errors[\"not_found\"].extend(rv[\"not_found\"])\n    errors[\"invalid_type\"].extend(rv[\"invalid_type\"])\n</pre> from pprint import pprint  alldocs_ids = set(mdb.alldocs.distinct(\"id\"))  def doc_field_value_errors(assertions):     errors = {\"not_found\": [], \"invalid_type\": []}     # group assertions by referenced \"id\" value.     assertions_by_referenced_id_value = defaultdict(list)     for a in assertions:         assertions_by_referenced_id_value[a[\"value\"]].append(a)     # associate each referenced document id with its type.     doc_id_types = {}     for d in list(mdb.alldocs.find({\"id\": {\"$in\": list(assertions_by_referenced_id_value.keys())}}, {\"_id\": 0, \"id\": 1, \"type\": 1})):         doc_id_types[d[\"id\"]] = d[\"type\"]      for id_value, id_value_assertions in assertions_by_referenced_id_value.items():         if id_value not in alldocs_ids:             errors[\"not_found\"].extend(id_value_assertions)         else:             for a in id_value_assertions:                 # check that the document-reported type for this id reference is kosher as per the referring slot's schema definition.                 if doc_id_types[a[\"value\"]][5:] not in a[\"acceptable_slot_classes\"]:                     errors[\"invalid_type\"].append(a)      return errors   # Initialize \"global\" error lists. errors = {\"not_found\": [], \"invalid_type\": []}  for das in doc_assertions(batch_size=2_000):     rv = doc_field_value_errors(das)     errors[\"not_found\"].extend(rv[\"not_found\"])     errors[\"invalid_type\"].extend(rv[\"invalid_type\"]) <pre>  0%|          | 0/163175 [00:00&lt;?, ?it/s]</pre> In\u00a0[13]: Copied! <pre>len(errors[\"not_found\"]), len(errors[\"invalid_type\"])\n# results with v11.1.0 on `/global/cfs/projectdirs/m3408/nmdc-mongodumps/dump_nmdc-prod_2024-11-25_20-12-02/nmdc`: (33, 0)\n</pre> len(errors[\"not_found\"]), len(errors[\"invalid_type\"]) # results with v11.1.0 on `/global/cfs/projectdirs/m3408/nmdc-mongodumps/dump_nmdc-prod_2024-11-25_20-12-02/nmdc`: (33, 0) Out[13]: <pre>(33, 0)</pre>"},{"location":"nb/bulk_validation_referential_integrity_check/#referential-integrity-checker-prototype","title":"Referential integrity checker (prototype)\u00b6","text":""},{"location":"nb/bulk_validation_referential_integrity_check/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before running this notebook, make sure you have done the following:</p> <ol> <li>Run <code>$ make up-dev</code></li> <li>Map <code>localhost:27018</code> to the Mongo server you want to use</li> <li>Load a recent dump of the production Mongo database into that Mongo server (see <code>$ make mongorestore-nmdc-db</code> for an example)</li> <li>In the <code>.env</code> file, set <code>MONGO_HOST</code> to <code>mongodb://localhost:27018</code></li> <li>Run <code>$ export $(grep -v '^#' .env | xargs)</code> to load the environment variables defined in <code>.env</code> into your shell environment</li> <li>Run <code>make init</code> to ensure a consistent python kernel for this notebook.</li> </ol> <p>Once you've done all of those things, you can run this notebook (e.g. via <code>$ jupyter notebook</code>)</p>"},{"location":"nb/bulk_validation_referential_integrity_check/#enable-automatic-reloading-of-modules","title":"Enable automatic reloading of modules\u00b6","text":"<p>Reference: https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#autoreload</p>"},{"location":"nb/bulk_validation_referential_integrity_check/#import-python-modules","title":"Import Python modules\u00b6","text":""},{"location":"nb/bulk_validation_referential_integrity_check/#create-slot-mappings","title":"Create slot mappings\u00b6","text":""},{"location":"nb/bulk_validation_referential_integrity_check/#materialize-single-collection-view-of-database","title":"Materialize single-collection view of database\u00b6","text":""},{"location":"nb/bulk_validation_referential_integrity_check/#validate","title":"Validate\u00b6","text":""},{"location":"nb/bulk_validation_referential_integrity_check/#check-referential-integrity","title":"Check referential integrity\u00b6","text":"<p>In this cell, we populate two lists:</p> <ul> <li><code>errors.not_found</code>: a list of \"naive\" errors</li> <li><code>errors.invalid_type</code>: a list of (hierarchy-aware) type errors (document was found, but is of an invalid type)</li> </ul> <p>Reference: https://linkml.io/linkml/developers/schemaview.html#linkml_runtime.utils.schemaview.SchemaView.class_induced_slots</p>"},{"location":"nb/bulk_validation_referential_integrity_check/#results","title":"Results\u00b6","text":"<p>Display the number errors in each list.</p>"},{"location":"nb/get_data/","title":"Get NMDC Metadata and Data Objects","text":"<p>This notebook describes and provides example code to</p> <ol> <li>Filter NMDC metadata to obtain IDs and fetch attributes, using API endpoints.</li> <li>Download collected metadata to files, and data objects to files.</li> <li>Fetch and load collected metadata, or data object bytes, to in-memory Python objects.</li> </ol> <p>The following modules, constants, and helper functions are used by one or more use case cells below, so be sure to run this cell first:</p> In\u00a0[1]: Copied! <pre>from io import BytesIO\nimport json\nfrom operator import itemgetter\nfrom pathlib import Path\nfrom pprint import pprint\nimport shutil\nimport subprocess\nfrom tqdm.notebook import tqdm\nfrom urllib.parse import parse_qsl, urlencode\n\nimport requests\nfrom toolz import keyfilter, merge, concat\n\nHOST = \"https://api.microbiomedata.org\"\n\ndef get_json(path, host=HOST, **kwargs):\n    r = requests.get(host + path, **kwargs)\n    r.raise_for_status()\n    return r.json()\n\ndef pick(allowlist, d):\n    return keyfilter(lambda k: k in allowlist, d)\n\nmeta = itemgetter(\"meta\")\nresults = itemgetter(\"results\")\n</pre> from io import BytesIO import json from operator import itemgetter from pathlib import Path from pprint import pprint import shutil import subprocess from tqdm.notebook import tqdm from urllib.parse import parse_qsl, urlencode  import requests from toolz import keyfilter, merge, concat  HOST = \"https://api.microbiomedata.org\"  def get_json(path, host=HOST, **kwargs):     r = requests.get(host + path, **kwargs)     r.raise_for_status()     return r.json()  def pick(allowlist, d):     return keyfilter(lambda k: k in allowlist, d)  meta = itemgetter(\"meta\") results = itemgetter(\"results\") In\u00a0[2]: Copied! <pre>get_json(\"/nmdcschema/ids/nmdc:f2a40483485c45baaf30160d0ca2ac40\")\n</pre> get_json(\"/nmdcschema/ids/nmdc:f2a40483485c45baaf30160d0ca2ac40\") Out[2]: <pre>{'has_output': ['nmdc:bb7a9edac41c31f6d36c34f6bfa7491a'],\n 'started_at_time': '2021-01-21T23:31:33Z',\n 'execution_resource': 'EMSL-RZR',\n 'has_input': ['emsl:output_747989'],\n 'was_informed_by': 'emsl:747989',\n 'git_url': 'https://github.com/microbiomedata/enviroMS',\n 'id': 'nmdc:f2a40483485c45baaf30160d0ca2ac40',\n 'used': '12T_FTICR_B',\n 'type': 'nmdc:NomAnalysisActivity',\n 'ended_at_time': '2021-01-21T23:31:33Z'}</pre> In\u00a0[3]: Copied! <pre>get_json(\"/nmdcschema/biosample_set/gold:Gb0115217\")\n</pre> get_json(\"/nmdcschema/biosample_set/gold:Gb0115217\") Out[3]: <pre>{'env_local_scale': {'has_raw_value': 'ENVO:01000621'},\n 'collection_date': {'has_raw_value': '2014-09-23'},\n 'add_date': '2015-05-28',\n 'geo_loc_name': {'has_raw_value': 'USA: Columbia River, Washington'},\n 'location': 'groundwater-surface water interaction zone in Washington, USA',\n 'mod_date': '2021-06-17',\n 'description': 'Sterilized sand packs were incubated back in the ground and collected at time point T2.',\n 'depth': {'has_raw_value': '0.5',\n  'has_numeric_value': 0.5,\n  'has_unit': 'meter'},\n 'part_of': ['gold:Gs0114663'],\n 'ncbi_taxonomy_name': 'sediment metagenome',\n 'GOLD_sample_identifiers': ['gold:Gb0115217'],\n 'ecosystem_category': 'Artificial ecosystem',\n 'ecosystem_type': 'Sand microcosm',\n 'env_broad_scale': {'has_raw_value': 'ENVO:01000253'},\n 'sample_collection_site': 'sand microcosm',\n 'id': 'gold:Gb0115217',\n 'identifier': 'GW-RW T2_23-Sept-14',\n 'ecosystem_subtype': 'Unclassified',\n 'depth2': {'has_raw_value': '1.0',\n  'has_numeric_value': 1,\n  'has_unit': 'meter'},\n 'specific_ecosystem': 'Unclassified',\n 'INSDC_biosample_identifiers': ['biosample:SAMN06343863'],\n 'community': 'microbial communities',\n 'name': 'Sand microcosm microbial communities from a hyporheic zone in Columbia River, Washington, USA - GW-RW T2_23-Sept-14',\n 'alternative_identifiers': ['img.taxon:3300042741'],\n 'lat_lon': {'has_raw_value': '46.37228379 -119.2717467',\n  'latitude': 46.37228379,\n  'longitude': -119.2717467},\n 'habitat': 'sand microcosm',\n 'ecosystem': 'Engineered',\n 'env_medium': {'has_raw_value': 'ENVO:01000017'},\n 'type': 'nmdc:Biosample'}</pre> In\u00a0[4]: Copied! <pre>def get_json_mql(path, filter_):\n    return get_json(path, params={\"filter\": json.dumps(filter_)})\n\ndef resources_count(json_response):\n    return len(json_response[\"resources\"])\n\nresources_count(get_json_mql(\n    \"/nmdcschema/biosample_set\",\n    {\"ecosystem\": \"Engineered\"}\n))\n</pre> def get_json_mql(path, filter_):     return get_json(path, params={\"filter\": json.dumps(filter_)})  def resources_count(json_response):     return len(json_response[\"resources\"])  resources_count(get_json_mql(     \"/nmdcschema/biosample_set\",     {\"ecosystem\": \"Engineered\"} )) Out[4]: <pre>19</pre> In\u00a0[5]: Copied! <pre>def id_and_ecosystem_fields(doc):\n    return pick(\n        [\"id\"] + [f for f in doc if f.startswith(\"ecosystem\")],\n        doc)\n\nprint(\"\\nStudies filter:\\n\")\njson_response = get_json(\"/studies?filter=ecosystem_type:Soil\")\npprint(meta(json_response))\npprint([id_and_ecosystem_fields(r) for r in results(json_response)])\n\nprint(\"\\nData Objects filter and sort:\\n\")\n\njson_response = get_json(\n    \"/data_objects?\"\n    \"filter=description.search:GFF\"\n    \"&amp;\"\n    \"sort=file_size_bytes:desc\"\n)\npprint(meta(json_response))\npprint([pick(\n    [\"description\", \"file_size_bytes\", \"id\", \"url\"]\n    , r\n) for r in results(json_response)][:5])\n\nprint(\"\\nActivities filter and sort:\\n\")\n\njson_response = get_json(\n    \"/activities?\"\n    \"filter=started_at_time:&gt;2022-01-01\"\n    \",\"\n    \"execution_resource.search:NERSC\"\n    \"&amp;\"\n    \"sort=ended_at_time:desc\"\n)\npprint(meta(json_response))\npprint([\n    pick([\n        \"id\",\n        \"started_at_time\",\n        \"ended_at_time\",\n        \"execution_resource\",\n        \"type\"],\n        r\n    ) for r in results(json_response)][:5]\n)\n</pre> def id_and_ecosystem_fields(doc):     return pick(         [\"id\"] + [f for f in doc if f.startswith(\"ecosystem\")],         doc)  print(\"\\nStudies filter:\\n\") json_response = get_json(\"/studies?filter=ecosystem_type:Soil\") pprint(meta(json_response)) pprint([id_and_ecosystem_fields(r) for r in results(json_response)])  print(\"\\nData Objects filter and sort:\\n\")  json_response = get_json(     \"/data_objects?\"     \"filter=description.search:GFF\"     \"&amp;\"     \"sort=file_size_bytes:desc\" ) pprint(meta(json_response)) pprint([pick(     [\"description\", \"file_size_bytes\", \"id\", \"url\"]     , r ) for r in results(json_response)][:5])  print(\"\\nActivities filter and sort:\\n\")  json_response = get_json(     \"/activities?\"     \"filter=started_at_time:&gt;2022-01-01\"     \",\"     \"execution_resource.search:NERSC\"     \"&amp;\"     \"sort=ended_at_time:desc\" ) pprint(meta(json_response)) pprint([     pick([         \"id\",         \"started_at_time\",         \"ended_at_time\",         \"execution_resource\",         \"type\"],         r     ) for r in results(json_response)][:5] ) <pre>\nStudies filter:\n\n{'count': 3,\n 'db_response_time_ms': 12,\n 'mongo_filter_dict': {'ecosystem_type': 'Soil'},\n 'mongo_sort_list': None,\n 'page': 1,\n 'per_page': 25}\n[{'ecosystem': 'Environmental',\n  'ecosystem_category': 'Terrestrial',\n  'ecosystem_subtype': 'Unclassified',\n  'ecosystem_type': 'Soil',\n  'id': 'gold:Gs0128850'},\n {'ecosystem': 'Environmental',\n  'ecosystem_category': 'Terrestrial',\n  'ecosystem_subtype': 'Meadow',\n  'ecosystem_type': 'Soil',\n  'id': 'gold:Gs0135149'},\n {'ecosystem': 'Environmental',\n  'ecosystem_category': 'Terrestrial',\n  'ecosystem_subtype': 'Unclassified',\n  'ecosystem_type': 'Soil',\n  'id': 'gold:Gs0154044'}]\n\nData Objects filter and sort:\n\n{'count': 11556,\n 'db_response_time_ms': 362,\n 'mongo_filter_dict': {'description': {'$regex': 'GFF'}},\n 'mongo_sort_list': [['file_size_bytes', -1]],\n 'page': 1,\n 'per_page': 25}\n[{'description': 'Prodigal GFF file for gold:Gp0208583',\n  'file_size_bytes': 3013162589,\n  'id': 'nmdc:ef4512eba3c1bc0a3c99d1ee7a78270b',\n  'url': 'https://data.microbiomedata.org/data/nmdc:mga06f4615/annotation/nmdc_mga06f4615_prodigal.gff'},\n {'description': 'Prodigal GFF file for gold:Gp0116338',\n  'file_size_bytes': 2898136903,\n  'id': 'nmdc:14917611a1d1fe3b4f5dc97ae9ffcb86',\n  'url': 'https://data.microbiomedata.org/data/nmdc:mga0m894/annotation/nmdc_mga0m894_prodigal.gff'},\n {'description': 'Prodigal GFF file for gold:Gp0208578',\n  'file_size_bytes': 2837716826,\n  'id': 'nmdc:a206b25e328af837ea708d6a7acb4e9f',\n  'url': 'https://data.microbiomedata.org/data/nmdc:mga0x8zm48/annotation/nmdc_mga0x8zm48_prodigal.gff'},\n {'description': 'Prodigal GFF file for gold:Gp0116337',\n  'file_size_bytes': 2789133841,\n  'id': 'nmdc:0fba5e13ecb1c51f9298e89d1bfca222',\n  'url': 'https://data.microbiomedata.org/data/nmdc:mga0kt39/annotation/nmdc_mga0kt39_prodigal.gff'},\n {'description': 'Prodigal GFF file for gold:Gp0208581',\n  'file_size_bytes': 2728019630,\n  'id': 'nmdc:274b9d4e3f71858c94e129a93f5a1232',\n  'url': 'https://data.microbiomedata.org/data/nmdc:mga0qfj577/annotation/nmdc_mga0qfj577_prodigal.gff'}]\n\nActivities filter and sort:\n\n{'count': 2054,\n 'db_response_time_ms': 2591,\n 'mongo_filter_dict': {'execution_resource': {'$regex': 'NERSC'},\n                       'started_at_time': {'$gt': '2022-01-01'}},\n 'page': 1,\n 'per_page': 25}\n[{'ended_at_time': '2022-05-31T12:31:18-07:00',\n  'execution_resource': 'NERSC-Cori',\n  'id': 'nmdc:683c4a7adaae08cf5456f7b80bb6f4d3',\n  'started_at_time': '2022-05-31T12:31:18-07:00',\n  'type': 'nmdc:MetatranscriptomeAssembly'},\n {'ended_at_time': '2022-05-31T12:29:49-07:00',\n  'execution_resource': 'NERSC-Cori',\n  'id': 'nmdc:6305a511f040e8bef679b8a2e439329e',\n  'started_at_time': '2022-05-31T12:29:49-07:00',\n  'type': 'nmdc:MetatranscriptomeAssembly'},\n {'ended_at_time': '2022-05-31T12:29:02-07:00',\n  'execution_resource': 'NERSC-Cori',\n  'id': 'nmdc:17b505f7781a3f0e932e8f39f4190068',\n  'started_at_time': '2022-05-31T12:29:02-07:00',\n  'type': 'nmdc:MetatranscriptomeAssembly'},\n {'ended_at_time': '2022-05-31T12:28:04-07:00',\n  'execution_resource': 'NERSC-Cori',\n  'id': 'nmdc:b502fd974951d11591564592ecff731c',\n  'started_at_time': '2022-05-31T12:28:04-07:00',\n  'type': 'nmdc:MetatranscriptomeAssembly'},\n {'ended_at_time': '2022-05-31T12:26:24-07:00',\n  'execution_resource': 'NERSC-Cori',\n  'id': 'nmdc:839560f9650622f232c262d8cf7a9db9',\n  'started_at_time': '2022-05-31T12:26:24-07:00',\n  'type': 'nmdc:MetatranscriptomeAssembly'}]\n</pre> In\u00a0[6]: Copied! <pre>def write_jsonlines_file(path, all_results):\n    with open(path, \"w\") as f:\n        f.writelines([json.dumps(doc)+\"\\n\" for doc in all_results])\n\ncursor = \"*\"\nall_results = []\nwhile cursor is not None:\n    json_response = get_json(\n        f\"/biosamples?filter=part_of:gold:Gs0110119&amp;cursor={cursor}\"\n    )\n    m, rs = meta(json_response), results(json_response)\n    cursor = m['next_cursor']\n    print(\"fetched\", len(rs), f\"results out of {m['count']} total\")\n    all_results.extend(rs)\n\npath = \"~/biosamples_part_of_gold:Gs0110119.jsonl\"\n\nwrite_jsonlines_file(\n    Path(path).expanduser(),\n    all_results\n)\n\nsubprocess.check_output(\n    f\"head -1 {path}\",\n    shell=True,\n)\n</pre> def write_jsonlines_file(path, all_results):     with open(path, \"w\") as f:         f.writelines([json.dumps(doc)+\"\\n\" for doc in all_results])  cursor = \"*\" all_results = [] while cursor is not None:     json_response = get_json(         f\"/biosamples?filter=part_of:gold:Gs0110119&amp;cursor={cursor}\"     )     m, rs = meta(json_response), results(json_response)     cursor = m['next_cursor']     print(\"fetched\", len(rs), f\"results out of {m['count']} total\")     all_results.extend(rs)  path = \"~/biosamples_part_of_gold:Gs0110119.jsonl\"  write_jsonlines_file(     Path(path).expanduser(),     all_results )  subprocess.check_output(     f\"head -1 {path}\",     shell=True, ) <pre>fetched 25 results out of 60 total\nfetched 25 results out of 60 total\nfetched 10 results out of 60 total\n</pre> Out[6]: <pre>b'{\"GOLD_sample_identifiers\": [\"gold:Gb0110680\"], \"INSDC_biosample_identifiers\": [\"BIOSAMPLE:SAMN08902828\"], \"add_date\": \"2015-02-26\", \"collection_date\": {\"has_raw_value\": \"2014-09-03\"}, \"depth\": {\"has_maximum_numeric_value\": 0.2, \"has_minimum_numeric_value\": 0.1, \"has_numeric_value\": 0.1, \"has_raw_value\": \"0.1 to 0.2 meters\", \"has_unit\": \"metre\"}, \"depth2\": {\"has_numeric_value\": 0.2, \"has_raw_value\": \"0.2 meters\", \"has_unit\": \"metre\"}, \"description\": \"Grasslands soil microbial communities from the Angelo Coastal Reserve, plot 2. There is a duplicate submission for this entry in NCBI. The NCBI identifiers for a duplicate are PRJNA449266 and SAMN08902828\", \"ecosystem\": \"Environmental\", \"ecosystem_category\": \"Terrestrial\", \"ecosystem_subtype\": \"Grasslands\", \"ecosystem_type\": \"Soil\", \"elev\": {\"has_numeric_value\": 432, \"has_raw_value\": \"432 meters\", \"has_unit\": \"metre\"}, \"env_broad_scale\": {\"has_raw_value\": \"grassland biome [ENVO:01000177]\", \"term\": {\"id\": \"ENVO:01000177\", \"name\": \"grassland biome\"}}, \"env_local_scale\": {\"has_raw_value\": \"biosphere reserve [ENVO:00000376]\", \"term\": {\"id\": \"ENVO:00000376\", \"name\": \"biosphere reserve\"}}, \"env_medium\": {\"has_raw_value\": \"grassland soil [ENVO:00005750]\", \"term\": {\"id\": \"ENVO:00005750\", \"name\": \"grassland soil\"}}, \"geo_loc_name\": {\"has_raw_value\": \"USA: California: Angelo Coastal Reserve\"}, \"habitat\": \"Grasslands soil\", \"id\": \"gold:Gb0110680\", \"identifier\": \"14_0903_02_20cm\", \"lat_lon\": {\"has_raw_value\": \"39.7392 -123.6308\", \"latitude\": 39.7392, \"longitude\": -123.6308}, \"location\": \"USA: California: Angelo Coastal Reserve\", \"mod_date\": \"2022-08-02\", \"name\": \"Grasslands soil microbial communities from the Angelo Coastal Reserve, California, USA - 14_0903_02_20cm\", \"ncbi_taxonomy_name\": \"soil metagenome\", \"part_of\": [\"gold:Gs0110119\"], \"sample_collection_site\": \"grassland soil\", \"specific_ecosystem\": \"Unclassified\"}\\n'</pre> In\u00a0[7]: Copied! <pre>def download_file(url, directory=\"~/\"):\n    local_filename = url.split('/')[-1]\n    with requests.get(url, stream=True) as r:\n        with open(Path(directory + local_filename).expanduser(), 'wb') as f:\n            shutil.copyfileobj(r.raw, f)\n\n    return local_filename\n\nid_biosample = \"igsn:IEWFS000A\"\nrs_ompro = results(get_json(f\"/activities?filter=type:nmdc:OmicsProcessing,has_input:{id_biosample}\"))\nfor id_ompro in tqdm([d[\"id\"] for d in rs_ompro]):\n    rs_act = results(get_json(f\"/activities?filter=was_informed_by:{id_ompro}\"))\n    for data_object_ids, activity_type in [(d[\"has_output\"], d[\"type\"]) for d in rs_act]:\n        for data_object_id in data_object_ids:\n            do = results(get_json(f\"/data_objects?filter=id:{data_object_id}\"))[0]\n            print(f'downloading biosample {id_biosample} &gt; omics processing activity {id_ompro} '\n                  f'&gt; {activity_type} activity &gt; data object {data_object_id} from {do[\"url\"]}...')\n            download_file(do[\"url\"])\n</pre> def download_file(url, directory=\"~/\"):     local_filename = url.split('/')[-1]     with requests.get(url, stream=True) as r:         with open(Path(directory + local_filename).expanduser(), 'wb') as f:             shutil.copyfileobj(r.raw, f)      return local_filename  id_biosample = \"igsn:IEWFS000A\" rs_ompro = results(get_json(f\"/activities?filter=type:nmdc:OmicsProcessing,has_input:{id_biosample}\")) for id_ompro in tqdm([d[\"id\"] for d in rs_ompro]):     rs_act = results(get_json(f\"/activities?filter=was_informed_by:{id_ompro}\"))     for data_object_ids, activity_type in [(d[\"has_output\"], d[\"type\"]) for d in rs_act]:         for data_object_id in data_object_ids:             do = results(get_json(f\"/data_objects?filter=id:{data_object_id}\"))[0]             print(f'downloading biosample {id_biosample} &gt; omics processing activity {id_ompro} '                   f'&gt; {activity_type} activity &gt; data object {data_object_id} from {do[\"url\"]}...')             download_file(do[\"url\"]) <pre>  0%|          | 0/22 [00:00&lt;?, ?it/s]</pre> <pre>downloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:705701 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:2a779b0132303d5999c6f7c99915fd34 from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134A_CHCl3_15Oct18_IAT_p1_1_01_35922.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:705702 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:9f0d52cc46d247b8d2ba12d5842b9fb6 from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134A_H2O_15Oct18_IAT_p1_1_01_35893.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:705703 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:e3449444d03be27addeaca224ce9a3a3 from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134A_MeOH_15Oct18_IAT_p1_1_01_35910.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:705704 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:ed4d444af3672b33bf43a1d5b6dd1ca9 from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134B_CHCl3_15Oct18_IAT_p1_1_01_35927.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:705705 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:4296e45e09241a4ac76202d4f1f40458 from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134B_H2O_15Oct18_IAT_p1_1_01_35898.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:705706 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:031d764e78bd55a9247ee11fcf407587 from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134B_H2O_SPE_15Oct18_IAT_p05_1_01_35903.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:705707 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:f2f58fc563ac5836762826b0e6db6f48 from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134B_MeOH_15Oct18_IAT_p1_1_01_35915.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:713600 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:d7bda0fca4304e2699eed4be527c81de from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134_r3_H2O_30Nov18_IATp1_1_01_37902.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:715134 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:f905c863ed0369cc3f83a5cbc46561d4 from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134_H2O_3_IATp1_07Dec18_1_01_38232.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:717358 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:2d795aad78f1ddba9e3f7add02fc259f from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134_H2O_1_12Dec18_IATp1_1_01_38495.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:717359 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:ce403bfa29fdce24d6047570f0336c48 from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134_H2O_2_12Dec18_IATp1_1_01_38496.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:717360 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:119d7c652e0c29e32c5066cc987b17ff from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134_H2O_3_12Dec18_IATp1_1_01_38497.csv...\ndownloading biosample igsn:IEWFS000A &gt; omics processing activity emsl:717365 &gt; nmdc:NomAnalysisActivity activity &gt; data object nmdc:4b649d353b2c2385ab042682ba516d14 from https://nmdcdemo.emsl.pnnl.gov/nom/results/Brodie_134_H2O_1_13Dec18_IATp15_1_01_38565.csv...\n</pre> In\u00a0[8]: Copied! <pre>cursor = \"*\"\nall_results = []\nwhile cursor is not None:\n    json_response = get_json(\n        f\"/biosamples?filter=part_of:gold:Gs0110119&amp;cursor={cursor}\"\n    )\n    m, rs = meta(json_response), results(json_response)\n    cursor = m['next_cursor']\n    print(\"fetched\", len(rs), f\"results out of {m['count']} total\")\n    all_results.extend(rs)\n\npprint([pick([\"id\",\"lat_lon\"], r) for r in all_results][:5])\n</pre> cursor = \"*\" all_results = [] while cursor is not None:     json_response = get_json(         f\"/biosamples?filter=part_of:gold:Gs0110119&amp;cursor={cursor}\"     )     m, rs = meta(json_response), results(json_response)     cursor = m['next_cursor']     print(\"fetched\", len(rs), f\"results out of {m['count']} total\")     all_results.extend(rs)  pprint([pick([\"id\",\"lat_lon\"], r) for r in all_results][:5]) <pre>fetched 25 results out of 60 total\nfetched 25 results out of 60 total\nfetched 10 results out of 60 total\n[{'id': 'gold:Gb0110680',\n  'lat_lon': {'has_raw_value': '39.7392 -123.6308',\n              'latitude': 39.7392,\n              'longitude': -123.6308}},\n {'id': 'gold:Gb0110681',\n  'lat_lon': {'has_raw_value': '39.7392 -123.6308',\n              'latitude': 39.7392,\n              'longitude': -123.6308}},\n {'id': 'gold:Gb0110682',\n  'lat_lon': {'has_raw_value': '39.7392 -123.6308',\n              'latitude': 39.7392,\n              'longitude': -123.6308}},\n {'id': 'gold:Gb0110683',\n  'lat_lon': {'has_raw_value': '39.7392 -123.6308',\n              'latitude': 39.7392,\n              'longitude': -123.6308}},\n {'id': 'gold:Gb0110684',\n  'lat_lon': {'has_raw_value': '39.7392 -123.6308',\n              'latitude': 39.7392,\n              'longitude': -123.6308}}]\n</pre> In\u00a0[9]: Copied! <pre>def load_bytes(url):\n    with requests.get(url, stream=True) as r:\n        b = BytesIO()\n        shutil.copyfileobj(r.raw, b)\n\n    return b.getvalue()\n\nb = load_bytes(get_json(\"/nmdcschema/data_object_set/nmdc:4b649d353b2c2385ab042682ba516d14\")[\"url\"])\n\nfor line in b.decode('utf-8').split(\"\\n\"):\n    print(line)\n</pre> def load_bytes(url):     with requests.get(url, stream=True) as r:         b = BytesIO()         shutil.copyfileobj(r.raw, b)      return b.getvalue()  b = load_bytes(get_json(\"/nmdcschema/data_object_set/nmdc:4b649d353b2c2385ab042682ba516d14\")[\"url\"])  for line in b.decode('utf-8').split(\"\\n\"):     print(line) <pre>Index,m/z,Calibrated m/z,Calculated m/z,Peak Height,Resolving Power,S/N,Ion Charge,m/z Error (ppm),m/z Error Score,Isotopologue Similarity,Confidence Score,DBE,H/C,O/C,Heteroatom Class,Ion Type,Is Isotopologue,Mono Isotopic Index,Molecular Formula,C,H,O,S,N\n3,888.4887543119316,888.4883146532127,888.4878482725782,3085144.7556615113,115985.22008880168,8.926135177919196,-1,-0.5249150401183273,0.21637236558431258,0.0,0.12982341935058755,21.0,1.290909090909091,0.12727272727272726,N1 S1 O7,de-protonated,0,,C55 H71 O7 S1 N1,55,71,7,1,1\n4,885.3918562712611,885.3914165190189,885.3914239992979,3271768.2235989384,116390.47009957765,9.466085953040722,-1,0.008448555979629386,0.999603533623901,0.0,0.5997621201743406,16.0,1.3478260869565217,0.3695652173913043,O17,de-protonated,0,,C46 H62 O17,46,62,17,,\n6,877.03548957106,877.0350496100808,877.035237742867,3413803.0920212218,93999.77607505002,9.877030182866147,-1,0.2145099513797522,0.7744236389320375,0.0,0.4646541833592225,32.0,0.5238095238095238,0.47619047619047616,S1 O20,de-protonated,0,,C42 H22 O20 S1,42,22,20,1,\n7,871.5868128663766,871.5863728035725,871.5857118408668,2525810.108505442,118234.47009250808,7.307832937466206,-1,-0.758345044867053,0.04096943704460999,0.0,0.024581662226765992,23.0,1.2903225806451613,0.016129032258064516,S1 O1,de-protonated,0,,C62 H80 O1 S1,62,80,1,1,\n8,869.7066517807076,869.7062116890468,869.7062208994556,2533183.851969944,118489.97010116033,7.329167116619603,-1,0.010590252923018975,0.999377119309806,0.0,0.5996262715858836,7.0,1.7818181818181817,0.09090909090909091,S1 O5,de-protonated,0,,C55 H98 O5 S1,55,98,5,1,\n11,862.1881537094312,862.18771353458,862.18698735984,2500237.533314158,95618.37607241525,7.233844751794849,-1,-0.8422473901817746,0.019428704336239354,0.0,0.011657222601743612,19.0,1.0789473684210527,0.5263157894736842,N1 S1 O20,de-protonated,0,,C38 H41 O20 S1 N1,38,41,20,1,1\n12,841.990360022908,841.9899198796957,841.9907299023058,2370457.287293944,97912.17607420433,6.858356367570994,-1,0.9620326939315207,0.005847710725539058,0.0,0.003508626435323435,35.0,0.325,0.525,N1 O21,de-protonated,0,,C40 H13 O21 N1,40,13,21,,1\n13,838.2477426554628,838.2473025589466,838.2480119249798,2449834.394824783,122936.47008928134,7.088015216009881,-1,0.846248393230588,0.018713029385901064,0.0,0.011227817631540639,33.0,0.7884615384615384,0.15384615384615385,N1 S1 O8,de-protonated,0,,C52 H41 O8 S1 N1,52,41,8,1,1\n14,830.2707204394443,830.2702804850616,830.2699291394654,2775378.1338916654,82745.31339509624,8.029898871842809,-1,-0.423170325523773,0.3697786516541105,0.0,0.2218671909924663,17.0,1.225,0.4,N1 S1 O16,de-protonated,0,,C40 H49 O16 S1 N1,40,49,16,1,1\n15,820.3453445049055,820.3449048083498,820.3455960048715,2743261.455279875,100495.57607049527,7.936976873862471,-1,0.8425674801075833,0.019370581727126596,0.0,0.011622349036275957,5.0,1.7878787878787878,0.6666666666666666,N1 O22,de-protonated,0,,C33 H59 O22 N1,33,59,22,,1\n17,799.3685325405778,799.3680936843464,799.367398784561,3118598.9451273754,103132.77607188243,9.022926946568637,-1,-0.8693121415769531,0.015020289920773781,0.0,0.009012173952464269,23.0,1.12,0.14,S1 O7,de-protonated,0,,C50 H56 O7 S1,50,56,7,1,\n18,780.4733701715519,780.4729324155969,780.4725921312933,2463579.1177946297,105629.57606904797,7.127782314453726,-1,-0.43599776224277864,0.3478172613374125,0.0,0.2086903568024475,8.0,1.6904761904761905,0.23809523809523808,N1 S1 O10,de-protonated,0,,C42 H71 O10 S1 N1,42,71,10,1,1\n22,653.5153865174419,653.5149645932129,653.5150488223496,2773339.3701903233,157687.97009893385,8.024000192255532,-1,0.12888630016707295,0.9118433198656568,0.0,0.547105991919394,8.0,1.6666666666666667,0.11904761904761904,O5,de-protonated,0,,C42 H70 O5,42,70,5,,\n35,450.26572507976397,450.2653590871265,450.2649821433228,21202835.25855894,228867.72008897978,61.345378794865574,-1,-0.8371599361487543,0.010186573708662465,0.766236582456902,0.31260657720795826,11.0,1.3214285714285714,0.14285714285714285,N1 O4,de-protonated,0,,C28 H37 O4 N1,28,37,4,,1\n37,283.2645638441444,283.2642719869493,283.2642538675084,4273299.506825067,291039.1760681066,12.36377936032282,-1,-0.06396656369622783,0.9775246355422157,0.0,0.5865147813253294,1.0,2.0,0.1111111111111111,O2,de-protonated,0,,C18 H36 O2,18,36,2,,\n38,273.0618794463733,273.06159294231406,273.06159087568057,2432007.4278552043,377392.22020885994,7.036437112035749,-1,-0.007568378593162039,0.9996818264361175,0.0,0.5998090958616704,5.0,1.2727272727272727,0.7272727272727273,O8,de-protonated,0,,C11 H14 O8,11,14,8,,\n39,255.23321441074,255.2329374887667,255.23295373855242,6820572.259571557,323002.9760412817,19.73370937230056,-1,0.06366648772467076,0.9777326510838139,0.0,0.5866395906502884,1.0,2.0,0.125,O2,de-protonated,0,,C16 H32 O2,16,32,2,,\n0,893.9807192795649,893.9802798081946,,2925279.2994384016,92217.97607563925,8.463602368102553,-1,,,,,,,,unassigned,,,,,,,,,\n1,891.2401722007025,891.239732632407,,3414246.805595051,92501.5760653175,9.87831396293287,-1,,,,,,,,unassigned,,,,,,,,,\n2,889.1174652085989,889.1170255699338,,2955400.261042048,77268.8133940652,8.550750231900409,-1,,,,,,,,unassigned,,,,,,,,,\n5,884.5134572608324,884.5130174836548,,5254152.640152923,116506.4700934881,15.201645441550752,-1,,,,,,,,unassigned,,,,,,,,,\n9,864.2012204090391,864.2007802514113,,3030984.065495402,119244.47008147289,8.769434056889162,-1,,,,,,,,unassigned,,,,,,,,,\n10,862.8953391929115,862.89489902369,,3055128.0467927232,119424.97008838368,8.83928894470198,-1,,,,,,,,unassigned,,,,,,,,,\n16,803.9459525978525,803.9455135240407,,5601357.72374313,170909.29346210908,16.20620106407045,-1,,,,,,,,unassigned,,,,,,,,,\n19,752.2242722248513,752.223836720794,,3273358.7560336525,136995.72010069533,9.470687781687843,-1,,,,,,,,unassigned,,,,,,,,,\n20,750.7742477178517,750.7738123490101,,2904412.8971422175,137259.97009147579,8.40323037835044,-1,,,,,,,,unassigned,,,,,,,,,\n21,653.5198929566078,653.519471031556,,3485404.0251306985,210248.96017618917,10.084190513553223,-1,,,,,,,,unassigned,,,,,,,,,\n23,619.4881452383327,619.4877300537637,,6028785.5745970225,95057.26862897968,17.442862251046506,-1,,,,,,,,unassigned,,,,,,,,,\n24,619.4845026976315,619.4840875138403,,6626103.056320465,221800.29347879274,19.171058821471128,-1,,,,,,,,unassigned,,,,,,,,,\n25,618.4823634993785,618.4819485300792,,67607054.28433818,166619.97011002325,195.60498884712098,-1,,,,,,,,unassigned,,,,,,,,,\n26,605.4715778756023,605.4711657741412,,7625159.61822322,226933.62680355,22.06159220910761,-1,,,,,,,,unassigned,,,,,,,,,\n27,605.4681309396186,605.4677188389377,,6698551.287281573,97257.26863065724,19.380670607684152,-1,,,,,,,,unassigned,,,,,,,,,\n28,604.4669880341766,604.4665761605637,,90288493.75827102,170483.22009523786,261.2283585131924,-1,,,,,,,,unassigned,,,,,,,,,\n29,591.4558841306729,591.4554752911807,,3570435.8944568695,232311.29346066722,10.330210075080675,-1,,,,,,,,unassigned,,,,,,,,,\n30,591.4525206690176,591.4521118303297,,4196033.12055477,232312.62681350167,12.140227383614969,-1,,,,,,,,unassigned,,,,,,,,,\n31,591.4362071597824,591.4357983249955,,4325198.308595796,99565.84004350418,12.513936243343444,-1,,,,,,,,unassigned,,,,,,,,,\n32,590.4511390833583,590.450730484585,,33051017.328779608,174529.72008540362,95.62528562170476,-1,,,,,,,,unassigned,,,,,,,,,\n33,574.4557295997837,574.4553249571502,,57270445.86539885,179389.47009937343,165.6984621406013,-1,,,,,,,,unassigned,,,,,,,,,\n34,509.29372586837684,509.2933397523243,,3016102.9919072534,115624.41149005273,8.726379197243881,-1,,,,,,,,unassigned,,,,,,,,,\n36,311.1007284631918,311.10042248349583,,2602658.391103224,220831.98006834995,7.530175230287296,-1,,,,,,,,unassigned,,,,,,,,,\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"nb/get_data/#get-nmdc-metadata-and-data-objects","title":"Get NMDC Metadata and Data Objects\u00b6","text":""},{"location":"nb/get_data/#dependencies","title":"Dependencies\u00b6","text":""},{"location":"nb/get_data/#filterfetch-metadata","title":"Filter/fetch metadata\u00b6","text":""},{"location":"nb/get_data/#use-case-fetch-metadata-directly-associated-with-an-id-with-unknown-type","title":"Use Case: fetch metadata directly associated with an ID with unknown type\u00b6","text":""},{"location":"nb/get_data/#use-case-fetch-metadata-for-an-id-from-a-known-nmdcdatabase-collection","title":"Use case: fetch metadata for an ID from a known nmdc:Database collection.\u00b6","text":""},{"location":"nb/get_data/#use-case-filter-metadata-from-a-known-nmdcdatabase-collection-using-the-mongodb-query-language","title":"Use Case: filter metadata from a known nmdc:Database collection using the MongoDB Query Language.\u00b6","text":""},{"location":"nb/get_data/#use-case-filter-metadata-from-studies-biosamples-data_objects-or-any-activities-collection-using-a-readable-url-with-a-solr-like-query-language","title":"Use Case: filter metadata from studies, biosamples, data_objects, or any activities collection using a readable URL with a Solr-like query language.\u00b6","text":""},{"location":"nb/get_data/#download-metadata","title":"Download (meta)data\u00b6","text":""},{"location":"nb/get_data/#use-case-download-metadata-of-all-biosamples-for-study","title":"Use case: download metadata of all biosamples for study.\u00b6","text":""},{"location":"nb/get_data/#use-case-download-all-data-objects-for-biosample","title":"Use case: download all data objects for biosample\u00b6","text":""},{"location":"nb/get_data/#load-metadata-objects-to-in-memory-python-objects","title":"Load (meta)data objects to in-memory Python objects\u00b6","text":""},{"location":"nb/get_data/#use-case-load-metadata-of-all-biosamples-for-study","title":"Use case: load metadata of all biosamples for study.\u00b6","text":""},{"location":"nb/get_data/#use-case-load-data-object","title":"Use case: load data object\u00b6","text":""},{"location":"nb/queue_and_trigger_data_jobs/","title":"Queue and Trigger Data and Metadata Jobs","text":"<p>This notebook describes and provides examples to</p> <ul> <li>understand and manage requests to the central queueing system for NMDC job orchestration</li> <li>understand and trigger jobs that can move and analyze data, and update metadata, based on precursor activities</li> </ul> <p>The following modules, constants, and helper functions are used by one or more use case cells below, so be sure to run this cell first (ensuring that your relative path to an environment-variables file is set):</p> In\u00a0[1]: Copied! <pre>from datetime import datetime, timezone\nimport json\nimport os\nfrom pprint import pprint\nimport secrets\nimport time\n\nfrom dotenv import load_dotenv\nimport requests\n\n# relative path to file with format\n# ```\n# NMDC_RUNTIME_HOST=fixme\n# NMDC_RUNTIME_USER=fixme\n# NMDC_RUNTIME_PASS=fixme\n# NMDC_RUNTIME_SITE_ID=fixme  # Okay if you don't have yet\n# NMDC_RUNTIME_SITE_CLIENT_ID=fixme  # Okay if you don't have yet\n# NMDC_RUNTIME_SITE_CLIENT_SECRET=fixme  # Okay if you don't have yet\n# ```\nenvfile_path = \"../../.env.client\"\n\nload_dotenv(envfile_path)\n\nENV = {\n    k: v for k, v in os.environ.items()\n    if k.startswith(\"NMDC_RUNTIME_\")\n}\n\nassert (\n    ENV[\"NMDC_RUNTIME_HOST\"] == \n    \"https://api.microbiomedata.org\"\n)\n\nHOST = ENV[\"NMDC_RUNTIME_HOST\"]\n\ndef request_and_return_json(method, path, host=HOST, **kwargs):\n    r = requests.request(method, host + path, **kwargs)\n    r.raise_for_status()\n    return r.json()\n\ndef get_json(path, host=HOST, **kwargs):\n    return request_and_return_json(\"GET\", path, host=host, **kwargs)\n\ndef post_and_return_json(path, host=HOST, **kwargs):\n    return request_and_return_json(\"POST\",  path, host=host, **kwargs)\n\ndef patch_and_return_json(path, host=HOST, **kwargs):\n    return request_and_return_json(\"PATCH\",  path, host=host, **kwargs)\n\ndef put_and_return_json(path, host=HOST, **kwargs):\n    return request_and_return_json(\"PUT\",  path, host=host, **kwargs)\n\ndef auth_header(bearer_token):\n    return {\"Authorization\": f\"Bearer {bearer_token}\"}\n\ndef get_token_for_user():\n    response = post_and_return_json(\n        \"/token\",\n        data={\n            \"grant_type\": \"password\",\n            \"username\": ENV[\"NMDC_RUNTIME_USER\"],\n            \"password\": ENV[\"NMDC_RUNTIME_PASS\"]\n        }\n    )\n    expires_minutes = response['expires']['minutes']\n    print(f\"Bearer token expires in {expires_minutes} minutes\")\n    return response[\"access_token\"]\n\ndef get_token_for_site_client():\n    response = post_and_return_json(\n        \"/token\",\n        data={\n            \"grant_type\": \"client_credentials\",\n            \"client_id\": ENV[\"NMDC_RUNTIME_SITE_CLIENT_ID\"],\n            \"client_secret\": ENV[\"NMDC_RUNTIME_SITE_CLIENT_SECRET\"]\n        }\n    )\n    expires_minutes = response['expires']['minutes']\n    print(f\"Bearer token expires in {expires_minutes} minutes\")\n    return response[\"access_token\"]\n\ndef now(as_str=False):\n    dt = datetime.now(timezone.utc)\n    return dt.isoformat() if as_str else dt\n\nTOKEN_U = get_token_for_user()\n</pre> from datetime import datetime, timezone import json import os from pprint import pprint import secrets import time  from dotenv import load_dotenv import requests  # relative path to file with format # ``` # NMDC_RUNTIME_HOST=fixme # NMDC_RUNTIME_USER=fixme # NMDC_RUNTIME_PASS=fixme # NMDC_RUNTIME_SITE_ID=fixme  # Okay if you don't have yet # NMDC_RUNTIME_SITE_CLIENT_ID=fixme  # Okay if you don't have yet # NMDC_RUNTIME_SITE_CLIENT_SECRET=fixme  # Okay if you don't have yet # ``` envfile_path = \"../../.env.client\"  load_dotenv(envfile_path)  ENV = {     k: v for k, v in os.environ.items()     if k.startswith(\"NMDC_RUNTIME_\") }  assert (     ENV[\"NMDC_RUNTIME_HOST\"] ==      \"https://api.microbiomedata.org\" )  HOST = ENV[\"NMDC_RUNTIME_HOST\"]  def request_and_return_json(method, path, host=HOST, **kwargs):     r = requests.request(method, host + path, **kwargs)     r.raise_for_status()     return r.json()  def get_json(path, host=HOST, **kwargs):     return request_and_return_json(\"GET\", path, host=host, **kwargs)  def post_and_return_json(path, host=HOST, **kwargs):     return request_and_return_json(\"POST\",  path, host=host, **kwargs)  def patch_and_return_json(path, host=HOST, **kwargs):     return request_and_return_json(\"PATCH\",  path, host=host, **kwargs)  def put_and_return_json(path, host=HOST, **kwargs):     return request_and_return_json(\"PUT\",  path, host=host, **kwargs)  def auth_header(bearer_token):     return {\"Authorization\": f\"Bearer {bearer_token}\"}  def get_token_for_user():     response = post_and_return_json(         \"/token\",         data={             \"grant_type\": \"password\",             \"username\": ENV[\"NMDC_RUNTIME_USER\"],             \"password\": ENV[\"NMDC_RUNTIME_PASS\"]         }     )     expires_minutes = response['expires']['minutes']     print(f\"Bearer token expires in {expires_minutes} minutes\")     return response[\"access_token\"]  def get_token_for_site_client():     response = post_and_return_json(         \"/token\",         data={             \"grant_type\": \"client_credentials\",             \"client_id\": ENV[\"NMDC_RUNTIME_SITE_CLIENT_ID\"],             \"client_secret\": ENV[\"NMDC_RUNTIME_SITE_CLIENT_SECRET\"]         }     )     expires_minutes = response['expires']['minutes']     print(f\"Bearer token expires in {expires_minutes} minutes\")     return response[\"access_token\"]  def now(as_str=False):     dt = datetime.now(timezone.utc)     return dt.isoformat() if as_str else dt  TOKEN_U = get_token_for_user() <pre>Bearer token expires in 30 minutes\n</pre> In\u00a0[2]: Copied! <pre>user_info = get_json(\"/users/me/\", headers=auth_header(TOKEN_U))\n\nid_newsite = f'{ENV[\"NMDC_RUNTIME_USER\"]}-{secrets.token_urlsafe()}'\n\npost_and_return_json(\n    \"/sites\",\n    json={\"id\": id_newsite},\n    headers=auth_header(TOKEN_U)\n)\nENV[\"NMDC_RUNTIME_SITE_ID\"] = id_newsite\nprint(ENV[\"NMDC_RUNTIME_SITE_ID\"])\n</pre> user_info = get_json(\"/users/me/\", headers=auth_header(TOKEN_U))  id_newsite = f'{ENV[\"NMDC_RUNTIME_USER\"]}-{secrets.token_urlsafe()}'  post_and_return_json(     \"/sites\",     json={\"id\": id_newsite},     headers=auth_header(TOKEN_U) ) ENV[\"NMDC_RUNTIME_SITE_ID\"] = id_newsite print(ENV[\"NMDC_RUNTIME_SITE_ID\"]) <pre>dwinston-_SIO50Lu0Whd45uyX9xKm3t7VmVV1KVxk6HrTm27brE\n</pre> In\u00a0[3]: Copied! <pre>site_id = ENV[\"NMDC_RUNTIME_SITE_ID\"]\n\nprint(f\"New client ID for site {site_id}:\")\nresponse = post_and_return_json(\n    f\"/sites/{site_id}:generateCredentials\",\n    headers=auth_header(TOKEN_U),\n)\nresponse\nENV[\"NMDC_RUNTIME_SITE_CLIENT_ID\"] = response[\"client_id\"]\nENV[\"NMDC_RUNTIME_SITE_CLIENT_SECRET\"] = response[\"client_secret\"]\n\nprint(ENV[\"NMDC_RUNTIME_SITE_CLIENT_ID\"])\n</pre> site_id = ENV[\"NMDC_RUNTIME_SITE_ID\"]  print(f\"New client ID for site {site_id}:\") response = post_and_return_json(     f\"/sites/{site_id}:generateCredentials\",     headers=auth_header(TOKEN_U), ) response ENV[\"NMDC_RUNTIME_SITE_CLIENT_ID\"] = response[\"client_id\"] ENV[\"NMDC_RUNTIME_SITE_CLIENT_SECRET\"] = response[\"client_secret\"]  print(ENV[\"NMDC_RUNTIME_SITE_CLIENT_ID\"]) <pre>New client ID for site dwinston-_SIO50Lu0Whd45uyX9xKm3t7VmVV1KVxk6HrTm27brE:\nsys0rgj0z957\n</pre> In\u00a0[4]: Copied! <pre>TOKEN_S = get_token_for_site_client()\n\ndef filter_jobs(filter_):\n    return get_json(\n        f\"/jobs/\",\n        headers=auth_header(TOKEN_U),\n        params={\"filter\": json.dumps(filter_)})\n\nresponse = filter_jobs({\"workflow.id\": \"test\"})\n\npprint(response)\n\njob_id = response['resources'][0]['id']\nprint(job_id)\n</pre> TOKEN_S = get_token_for_site_client()  def filter_jobs(filter_):     return get_json(         f\"/jobs/\",         headers=auth_header(TOKEN_U),         params={\"filter\": json.dumps(filter_)})  response = filter_jobs({\"workflow.id\": \"test\"})  pprint(response)  job_id = response['resources'][0]['id'] print(job_id) <pre>Bearer token expires in 30 minutes\n{'resources': [{'claims': [{'op_id': 'nmdc:sys08wb3p548',\n                            'site_id': 'dwinston-J4TsenGGwEf0WXGNE5GKDOvZ15tpxPU2DXSsrytEZl8'}],\n                'config': {},\n                'id': 'nmdc:fk0jb83',\n                'workflow': {'id': 'test'}},\n               {'config': {'object_id': '1bte-2c60-26'},\n                'created_at': '2021-09-15T21:21:33.565000',\n                'id': 'nmdc:sys0d9st65',\n                'workflow': {'id': 'test'}},\n               {'config': {'object_id': 'px81-r1xd-77'},\n                'created_at': '2021-09-27T21:17:03.606000',\n                'id': 'nmdc:sys09zw052',\n                'workflow': {'id': 'test'}},\n               {'config': {'object_id': 'sys04b34c032'},\n                'created_at': '2022-08-16T20:13:43.339047+00:00',\n                'id': 'nmdc:sys0m8808k69',\n                'workflow': {'id': 'test'}},\n               {'config': {'object_id': 'sys0gprg5t78'},\n                'created_at': '2022-08-16T20:18:19.335866+00:00',\n                'id': 'nmdc:sys088x72f03',\n                'workflow': {'id': 'test'}},\n               {'config': {'object_id': 'sys091bcr845'},\n                'created_at': '2022-08-16T20:22:01.353465+00:00',\n                'id': 'nmdc:sys0grg8vd94',\n                'workflow': {'id': 'test'}}]}\nnmdc:fk0jb83\n</pre> In\u00a0[5]: Copied! <pre>TOKEN_S = get_token_for_site_client()\n\nresponse = post_and_return_json(\n    f\"/jobs/{job_id}:claim\",\n    headers=auth_header(TOKEN_S),\n)\npprint(response)\n\noperation_id = response[\"id\"]\nprint(\"Operation ID is \", operation_id)\n</pre> TOKEN_S = get_token_for_site_client()  response = post_and_return_json(     f\"/jobs/{job_id}:claim\",     headers=auth_header(TOKEN_S), ) pprint(response)  operation_id = response[\"id\"] print(\"Operation ID is \", operation_id) <pre>Bearer token expires in 30 minutes\n{'done': False,\n 'expire_time': '2022-09-15T20:22:52.487625+00:00',\n 'id': 'nmdc:sys05me5jk63',\n 'metadata': {'job': {'config': {},\n                      'id': 'nmdc:fk0jb83',\n                      'workflow': {'id': 'test'}},\n              'model': 'nmdc_runtime.api.models.job.JobOperationMetadata',\n              'site_id': 'dwinston-_SIO50Lu0Whd45uyX9xKm3t7VmVV1KVxk6HrTm27brE'},\n 'result': None}\nOperation ID is  nmdc:sys05me5jk63\n</pre> In\u00a0[6]: Copied! <pre>TOKEN_S = get_token_for_site_client()\n\nprint(\"Operation summary:\")\npprint(get_json(f\"/operations/{operation_id}\"))\n\nprint(f\"Mark operation as done:\")\nresponse = patch_and_return_json(\n    f\"/operations/{operation_id}\",\n    json={\"done\": True, \"result\": \"code green\", \"metadata\": {\"a\": 3}},\n    headers=auth_header(TOKEN_S)\n)\npprint(response)\n</pre> TOKEN_S = get_token_for_site_client()  print(\"Operation summary:\") pprint(get_json(f\"/operations/{operation_id}\"))  print(f\"Mark operation as done:\") response = patch_and_return_json(     f\"/operations/{operation_id}\",     json={\"done\": True, \"result\": \"code green\", \"metadata\": {\"a\": 3}},     headers=auth_header(TOKEN_S) ) pprint(response) <pre>Bearer token expires in 30 minutes\nOperation summary:\n{'done': False,\n 'expire_time': '2022-09-15T20:22:52.487000',\n 'id': 'nmdc:sys05me5jk63',\n 'metadata': {'job': {'claims': [],\n                      'config': {},\n                      'created_at': None,\n                      'description': None,\n                      'id': 'nmdc:fk0jb83',\n                      'name': None,\n                      'workflow': {'capability_ids': None,\n                                   'created_at': None,\n                                   'description': None,\n                                   'id': 'test',\n                                   'name': None}},\n              'model': 'nmdc_runtime.api.models.job.JobOperationMetadata',\n              'site_id': 'dwinston-_SIO50Lu0Whd45uyX9xKm3t7VmVV1KVxk6HrTm27brE'},\n 'result': None}\nMark operation as done:\n{'done': True,\n 'expire_time': '2022-09-15T20:22:52.487000',\n 'id': 'nmdc:sys05me5jk63',\n 'metadata': {'a': 3,\n              'job': {'claims': [],\n                      'config': {},\n                      'created_at': None,\n                      'description': None,\n                      'id': 'nmdc:fk0jb83',\n                      'name': None,\n                      'workflow': {'capability_ids': None,\n                                   'created_at': None,\n                                   'description': None,\n                                   'id': 'test',\n                                   'name': None}},\n              'model': 'nmdc_runtime.api.models.job.JobOperationMetadata',\n              'site_id': 'dwinston-_SIO50Lu0Whd45uyX9xKm3t7VmVV1KVxk6HrTm27brE'},\n 'result': 'code green'}\n</pre> In\u00a0[7]: Copied! <pre>response = post_and_return_json(\n    \"/objects\",\n    json={\n        \"description\": \"a very fake object\",\n        \"checksums\": [{\"type\": \"sha256\", \"checksum\": secrets.token_hex()}],\n        \"created_time\": now(as_str=True),\n        \"size\": 1,\n        \"access_methods\": [\n            {\"access_url\": {\"url\": \"http://example.com/path/to/thing\"}},\n        ],\n    },\n    headers=auth_header(TOKEN_S)\n)\npprint(response)\nobject_id = response[\"id\"]\nprint(f\"Types associated with Object ID {object_id}:\")\npprint(get_json(f\"/objects/{object_id}/types\"))\n</pre> response = post_and_return_json(     \"/objects\",     json={         \"description\": \"a very fake object\",         \"checksums\": [{\"type\": \"sha256\", \"checksum\": secrets.token_hex()}],         \"created_time\": now(as_str=True),         \"size\": 1,         \"access_methods\": [             {\"access_url\": {\"url\": \"http://example.com/path/to/thing\"}},         ],     },     headers=auth_header(TOKEN_S) ) pprint(response) object_id = response[\"id\"] print(f\"Types associated with Object ID {object_id}:\") pprint(get_json(f\"/objects/{object_id}/types\")) <pre>{'access_methods': [{'access_id': None,\n                     'access_url': {'headers': None,\n                                    'url': 'http://example.com/path/to/thing'},\n                     'region': None,\n                     'type': 'https'}],\n 'aliases': None,\n 'checksums': [{'checksum': '0d5bc352bd9e947bcd1d88e9513ef4e8e0d4c81d7ae0f274a601ce36463e3f82',\n                'type': 'sha256'}],\n 'contents': None,\n 'created_time': '2022-08-16T20:22:58.963427+00:00',\n 'description': 'a very fake object',\n 'id': 'sys0n94fnf55',\n 'mime_type': None,\n 'name': None,\n 'self_uri': 'drs://drs.microbiomedata.org/sys0n94fnf55',\n 'size': 1,\n 'updated_time': None,\n 'version': None}\nTypes associated with Object ID sys0n94fnf55:\n[]\n</pre> In\u00a0[8]: Copied! <pre>response = put_and_return_json(\n    f\"/objects/{object_id}/types\",\n    json=[\"test\"],\n    headers=auth_header(TOKEN_S),\n)\npprint(get_json(f\"/objects/{object_id}/types\"))\n</pre> response = put_and_return_json(     f\"/objects/{object_id}/types\",     json=[\"test\"],     headers=auth_header(TOKEN_S), ) pprint(get_json(f\"/objects/{object_id}/types\")) <pre>[{'created_at': '2021-09-07T00:00:00',\n  'description': 'For use in unit and integration tests',\n  'id': 'test',\n  'name': 'A test object type'}]\n</pre> <p>Wait some time. Perhaps up to a minute. Then, see the claimable job:</p> In\u00a0[9]: Copied! <pre>def filter_jobs(filter_):\n    return get_json(\n        f\"/jobs/\",\n        headers=auth_header(TOKEN_U),\n        params={\"filter\": json.dumps(filter_)})\n\npprint(filter_jobs({\"workflow.id\": \"test\", \"config.object_id\": object_id}))\n</pre> def filter_jobs(filter_):     return get_json(         f\"/jobs/\",         headers=auth_header(TOKEN_U),         params={\"filter\": json.dumps(filter_)})  pprint(filter_jobs({\"workflow.id\": \"test\", \"config.object_id\": object_id})) <pre>{'resources': [{'config': {'object_id': 'sys0n94fnf55'},\n                'created_at': '2022-08-16T20:23:51.372491+00:00',\n                'id': 'nmdc:sys0s5mpmq50',\n                'workflow': {'id': 'test'}}]}\n</pre>"},{"location":"nb/queue_and_trigger_data_jobs/#queue-and-trigger-data-and-metadata-jobs","title":"Queue and Trigger Data and Metadata Jobs\u00b6","text":""},{"location":"nb/queue_and_trigger_data_jobs/#dependencies","title":"Dependencies\u00b6","text":""},{"location":"nb/queue_and_trigger_data_jobs/#understand-and-manage-queued-jobs","title":"Understand and Manage Queued Jobs\u00b6","text":""},{"location":"nb/queue_and_trigger_data_jobs/#use-case-create-a-new-logical-site-to-associate-with-job-executions","title":"Use case: create a new logical \"site\" to associate with job executions\u00b6","text":""},{"location":"nb/queue_and_trigger_data_jobs/#use-case-create-client-credentials-for-a-site-you-administer","title":"Use case: create client credentials for a site you administer\u00b6","text":""},{"location":"nb/queue_and_trigger_data_jobs/#use-case-filter-relevant-jobs-your-site-can-execute","title":"Use case: filter relevant jobs your site can execute\u00b6","text":""},{"location":"nb/queue_and_trigger_data_jobs/#use-case-claim-a-job-execution-to-keep-folks-in-the-loop","title":"Use case: claim a job execution to keep folks in the loop\u00b6","text":""},{"location":"nb/queue_and_trigger_data_jobs/#use-case-update-your-job-execution-operation-to-keep-folks-in-the-loop","title":"Use case: update your job-execution operation to keep folks in the loop\u00b6","text":""},{"location":"nb/queue_and_trigger_data_jobs/#understand-and-manage-triggered-jobs","title":"Understand and Manage Triggered Jobs\u00b6","text":""},{"location":"nb/queue_and_trigger_data_jobs/#use-case-register-an-object-that-may-trigger-a-job-via-a-type-annotation","title":"Use case: register an object that may trigger a job via a type annotation\u00b6","text":""},{"location":"nb/queue_and_trigger_data_jobs/#use-case-annotate-a-known-object-with-a-type-that-will-trigger-a-workflow","title":"Use case: annotate a known object with a type that will trigger a workflow\u00b6","text":""},{"location":"nb/wf_automation/","title":"Support for Workflow Automation","text":"<p>This notebook walks through existing functionality for (meta)data contributors to</p> <ol> <li><p>register workflows, to</p> </li> <li><p>install sensor logic for automated workflow execution, to</p> </li> <li><p>programmatically register workflow-run state transitions, i.e. run events, and to</p> </li> <li><p>programmatically register generated assets, i.e. data and metadata outputs, with any workflow run event.</p> </li> </ol> <p>In the <code>nmdc_runtime.api.boot.workflows</code> module, add an entry for your workflow to the <code>_raw</code> list. Examples:</p> <pre># nmdc_runtime/api/boot/workflows.py\n    {\n        \"id\": \"test\",\n        \"created_at\": datetime(2021, 9, 9, tzinfo=timezone.utc),\n        \"name\": \"A test workflow\",\n        \"description\": \"For use in unit and integration tests\",\n    },   \n    {\n        \"id\": \"metadata-in-1.0.0\",\n        \"created_at\": datetime(2021, 10, 12, tzinfo=timezone.utc),\n        \"name\": \"general metadata ETL\",\n        \"description\": \"Validate and ingest metadata from JSON files\",\n    },\n    {\n        \"id\": \"apply-changesheet-1.0.0\",\n        \"created_at\": datetime(2021, 9, 30, tzinfo=timezone.utc),\n        \"name\": \"apply metadata changesheet\",\n        \"description\": \"Validate and apply metadata changes from TSV/CSV files\",\n    },\n    {\n        \"id\": \"export-study-biosamples-as-csv-1.0.0\",\n        \"created_at\": datetime(2022, 6, 8, tzinfo=timezone.utc),\n        \"name\": \"export study biosamples metadata as CSV\",\n        \"description\": \"Export study biosamples metadata as CSV\",\n    },\n</pre> <p>That's it. The <code>id</code> field is a primary key under administration by workflow authors. That is, it is up to those who register a workflow by <code>id</code> here to ensure that it corresponds to a semantically invariant (minor and patch updates may vary if no <code>-x.y.z</code> suffix is part of the registered <code>id</code>) version of an unambiguously known workflow. Concretely, there is no requirement for e.g. a commmit-hash-including GitHub link to the workflow's entrypoint.</p> <p>Sensors are used to:</p> <ul> <li>orchestrate runs of runtime-site-executable workflows, e.g. validation and ingest of JSON objects and changesheets against the NMDC schema</li> <li>create new Job resources for external Sites to claim</li> </ul> <p>In the <code>nmdc_runtime.site.repository</code> module, you may add a function decorated with <code>dagster.sensor</code> (i.e. <code>@sensor</code> preceding the function's <code>def</code>), following the examples already installed.</p> <p>Alternatively, if your workflow needs to run if and only if a new data object of a certain type is detected by the runtime, then you may declaratively hook into the existing generic <code>nmdc_runtime.site.repository.process_workflow_job_triggers</code> sensor by registering appropriate entries in the <code>_raw</code> lists of <code>nmdc_runtime.api.boot.triggers</code> and <code>nmdc_runtime.api.boot.object_types</code>. See the next subsection for details.</p> <p>If your workflow needs to run if and only if a new data object of a certain type is detected by the runtime, you can add entries to two modules as per the following examples:</p> <pre># nmdc_runtime/api/boot/object_types.py\n    {\n        \"id\": \"test\",\n        \"created_at\": datetime(2021, 9, 7, tzinfo=timezone.utc),\n        \"name\": \"A test object type\",\n        \"description\": \"For use in unit and integration tests\",\n    },\n    {\n        \"id\": \"metadata-in\",\n        \"created_at\": datetime(2021, 6, 1, tzinfo=timezone.utc),\n        \"name\": \"metadata submission\",\n        \"description\": \"Input to the portal ETL process\",\n    },\n    {\n        \"id\": \"metadata-changesheet\",\n        \"created_at\": datetime(2021, 9, 30, tzinfo=timezone.utc),\n        \"name\": \"metadata changesheet\",\n        \"description\": \"Specification for changes to existing metadata\",\n    },\n</pre> <pre># nmdc_runtime/api/boot/triggers.py\n    {\n        \"created_at\": datetime(2021, 9, 9, tzinfo=timezone.utc),\n        \"object_type_id\": \"test\",\n        \"workflow_id\": \"test\",\n    },\n    {\n        \"created_at\": datetime(2021, 6, 1, tzinfo=timezone.utc),\n        \"object_type_id\": \"metadata-in\",\n        \"workflow_id\": \"metadata-in-1.0.0\",\n    },\n    {\n        \"created_at\": datetime(2021, 9, 30, tzinfo=timezone.utc),\n        \"object_type_id\": \"metadata-changesheet\",\n        \"workflow_id\": \"apply-changesheet-1.0.0\",\n    },\n</pre> <p>The corresponding sensor,</p> <pre># nmdc_runtime/site/repository.py\n@sensor(job=ensure_jobs.to_job(name=\"ensure_job_triggered\", **preset_normal))\ndef process_workflow_job_triggers(_context):\n</pre> <p>is activated approximately 30 seconds after the last time it ran, in perpetuity.</p> <p>There are currently two ways to register workflow-run state transitions:</p> <ol> <li>through claiming advertised Jobs and updating corresponding job Operation resources</li> <li>direct event registration with <code>/runs</code> API entrypoints</li> </ol> <p>If you have set up sensor logic to trigger the creation of a workflow Job resource when an appropriate input Object resource is available (see previous section), you may</p> <ol> <li><code>GET /jobs</code> to list and filter for relevant jobs</li> <li><code>POST /jobs/{job_id}:claim</code> to claim a job and receive the ID for a new Operation resource with which to register events regarding your workflow job execution.</li> <li><code>PATCH /operations/{op_id}</code> to report on job operation status, including whether it is <code>done</code> or not.</li> </ol> <p>You may <code>POST /runs/{run_id}/events</code> to post events relevant to your workflow execution. It is your responsibility to supply (1) a run id and (2) a job/workflow id with each posted representation so that events may be collated to recover run provenance. The OpenLineage schema is used for representations.</p> <p>If a workflow is registered with an executable by the runtime Site, you may <code>POST /runs</code> to request a run given workflow inputs/configuration. In this case, the runtime will return a run ID and will post run events that you may retrieve via <code>GET /runs/{run_id}/events</code> to list a run's events or <code>GET /runs/{run_id}</code> to get a summary of the run and its current status.</p> <p>Each mechanism for registering workflow-run state transitions (see previous section) includes facility for annotating transition representations with metadata about generated assets. Operation resources have <code>result</code> and <code>metadata</code> fields, and RunEvent resources (the representation schema for the <code>/runs</code> entrypoint suite) have <code>outputs</code> fields. The recommendation here is to include qualified references to nmdc:DataObject IDs.</p> <p>Note that such registration of assets within the representations of Operations and RunEvents is supplementary to but does not replace the primary requirement of provenance metadata embedded in submitted NMDC Schema nmdc:Activity representations, which also make reference to used and generated DataObjects.</p>"},{"location":"nb/wf_automation/#support-for-workflow-automation","title":"Support for Workflow Automation\u00b6","text":""},{"location":"nb/wf_automation/#register-workflows","title":"Register workflows\u00b6","text":""},{"location":"nb/wf_automation/#install-sensor-logic","title":"Install sensor logic\u00b6","text":""},{"location":"nb/wf_automation/#register-object-type-and-trigger-metadata","title":"Register object-type and trigger metadata\u00b6","text":""},{"location":"nb/wf_automation/#register-workflow-run-state-transitions","title":"Register workflow-run state transitions\u00b6","text":""},{"location":"nb/wf_automation/#claiming-a-job-and-updating-the-spawned-operation-resource","title":"Claiming a Job and updating the spawned Operation resource\u00b6","text":""},{"location":"nb/wf_automation/#direct-workflow-execution-event-registration-via-runs-entrypoints","title":"Direct workflow-execution event registration via <code>/runs</code> entrypoints\u00b6","text":""},{"location":"nb/wf_automation/#register-workflow-generated-assets","title":"Register workflow-generated assets\u00b6","text":""},{"location":"tutorials/auth/","title":"User/Site Login and Authentication","text":""},{"location":"tutorials/auth/#log-in-as-a-user","title":"Log in as a User","text":"<p>Open https://api.microbiomedata.org/docs and click on the <code>Authorize</code> button near the top of the page:</p> <p></p> <p>In the modal dialog, enter your given <code>username</code> and <code>password</code> in the first form, and click <code>Authorize</code>:</p> <p></p> <p>Once authorized, hit <code>Close</code> to get back to the interactive API documentation:</p> <p></p>"},{"location":"tutorials/auth/#create-a-user","title":"Create a User","text":"<p>Info</p> <p>You need to be already logged in to create a new user. Also, only users <code>aclum</code>, <code>dwinston</code>, <code>scanon</code>, and <code>scholia</code> can create new users at this time.</p> <p>Go to POST /users, and click the <code>Try it out</code> button. In the request body, the only required fields are <code>username</code> and <code>password</code>. If you know the <code>id</code>s of any sites you would like the new user to administer, enter those as an array value for the <code>site_admin</code> field.</p> <p>Share a new user's password securely with them. For example, a free instance of the open-source snappass web app is hosted by <code>dwinston</code> at https://snappass.polyneme.xyz/. This will generate a one-time link that you can email, send via Slack message, etc. to the new user.</p>"},{"location":"tutorials/auth/#create-a-site-client","title":"Create a Site Client","text":"<p>If you administer one or more sites, you can generate credentials for a site client that will act on behalf of the site. This is used for managing certain API resources -- rather than a person being responsible for a resource, a site is, and users that administer the site can come and go.</p> <p>Once logged in, you can use GET /users/me to see the <code>id</code>s of sites you can administer. Example response:</p> <pre><code>{\n  \"username\": \"dwinston\",\n  \"site_admin\": [\n    \"dwinston-laptop\",\n    \"nmdc-runtime-useradmin\",\n    \"dwinston-cloud\"\n  ]\n}\n</code></pre> <p>You can create your own sites via POST /sites. If the <code>id</code> you request already exists, you will get an error response. Do not worry about setting <code>capability_ids</code>; those can be set later.</p> <p>Once you have identified a site for which to generate credentials, use POST /sites/{site_id}:generateCredentials to do so. The response will look something like</p> <pre><code>{\n  \"client_id\": \"sys0***\",\n  \"client_secret\": \"***\"\n}\n</code></pre> <p>Save this information like you saved your username and password.</p>"},{"location":"tutorials/auth/#log-in-as-a-site","title":"Log in as a Site","text":"<p>Click on the <code>Authorize</code> button near the top of https://api.microbiomedata.org/docs. You may need to <code>Logout</code> first. Scroll all the way down to the second form, the one with only two fields, <code>client_id</code> and <code>client_secret</code>:</p> <p></p> <p>Note that the form is for the <code>clientCredentials</code> flow, whereas the first form is for the <code>password</code> flow. In this clientCredentials form, enter your site client credentials and <code>Authorize</code>. You are now logged in as a site client.</p>"},{"location":"tutorials/exporters/","title":"Metadata Export Pipelines (Exporters)","text":""},{"location":"tutorials/exporters/#introduction","title":"Introduction","text":"<p>Metadata export pipelines (often referred to as exporters) are pipelines that are run when we want to export out metadata (biosample, omics/sequencing, etc.) from the NMDC database into a format/serialization that is readable by another system (ex. NCBI Submission Portal). Target systems/databases that are currently supported, i.e., systems that we currently have pipelines for are:</p> <ul> <li>NCBI Submission Portal</li> </ul>"},{"location":"tutorials/exporters/#procedure","title":"Procedure","text":"<p>The steps below describe how to run the metadata export pipelines (exporters).</p> <ol> <li>Go to either the development or production Dagit interface: https://dagit-dev.microbiomedata.org/ or https://dagit.microbiomedata.org/. If you're running Dagster locally through Docker containers, then go to http://localhost:3000/.</li> <li>Find the hamburger button (<code>\u2630</code>) to the left, and a side panel should open up.</li> <li>You will see a section in that panel called <code>biosample_export@nmdc_runtime.site.repository:biosample_export</code> under which you will see all the exporter pipelines that you can run.</li> <li></li> <li>Select any of the above pipelines, and click on the Launchpad tab/section.</li> <li>You will see different configuration parameters that you need to fill out for different pipelines.</li> <li>Once you're done filling out the configuration parameters, you can hit the Launch Run button at the bottom right of the screen, and monitor the progress of your run.</li> <li>You can also track the history of all your past runs in the Runs tab/section.</li> <li>You can retrieve the output of your pipeline by clicking on the right step in the Dagit console log panel which will allow you to preview the output.</li> <li>For example, for the NCBI XML exporter you will see an <code>ASSET_MATERIALIZATION</code> step in the Dagit console log that can click to preview the output and copy it into an appropriately named file, i.e., with the right extension.</li> </ol>"},{"location":"tutorials/exporters/#additional-details","title":"Additional Details","text":"<ul> <li><code>nmdc_study_to_ncbi_submission_export</code>: exporter to help you export metadata related to a given study in NMDC into an XML Submission file that is compliant with (i.e., can be submitted to) the NCBI Submission Portal.</li> </ul>"},{"location":"tutorials/json/","title":"Validate JSON and Fetch JSON","text":"<p>Let's dive in and get acquainted with the NMDC Runtime API.</p>"},{"location":"tutorials/json/#validate-json","title":"Validate JSON","text":"<p>Already? Yes. Let's do this. Here is a tiny nmdc:Database JSON object:</p> <pre><code>{\"biosample_set\": [{\"id\": 42}]}\n</code></pre> <p>This represents a set of nmdc:Biosample objects. There is just one, with an <code>id</code> of <code>42</code>.</p> <p>Let's validate it. Go to the POST /metadata/json:validate endpoint at https://api.microbiomedata.org/docs and click \"Try it out\":</p> <p></p> <p>Now, copy the above JSON object, paste it into the <code>Request body</code> field, and hit <code>Execute</code>:</p> <p></p> <p>This gives us a response where the result is \"errors\". Looks like a biosample <code>id</code> needs to be a string value, and we are missing required properties. We also get a display of a <code>curl</code> command to reproduce the request on the command line:</p> <p></p> <p>Let's see what a \"valid\" response looks like. The GET /nmdcschema/{collection_name}/{doc_id} endpoint allows us to get the NMDC-schema-validated JSON object for one of the NMDC metadata collections:</p> <p></p> <p>For example, https://api.microbiomedata.org/nmdcschema/biosample_set/gold:Gb0115217 is</p> <pre><code>{\n  \"location\": \"groundwater-surface water interaction zone in Washington, USA\",\n  \"env_medium\": {\n    \"has_raw_value\": \"ENVO:01000017\"\n  },\n  \"depth2\": {\n    \"has_raw_value\": \"1.0\",\n    \"has_numeric_value\": 1,\n    \"has_unit\": \"meter\"\n  },\n  \"env_broad_scale\": {\n    \"has_raw_value\": \"ENVO:01000253\"\n  },\n  \"alternative_identifiers\": [\n    \"img.taxon:3300042741\"\n  ],\n  \"ecosystem\": \"Engineered\",\n  \"ecosystem_category\": \"Artificial ecosystem\",\n  \"id\": \"gold:Gb0115217\",\n  \"env_local_scale\": {\n    \"has_raw_value\": \"ENVO:01000621\"\n  },\n  \"community\": \"microbial communities\",\n  \"mod_date\": \"2021-06-17\",\n  \"ecosystem_subtype\": \"Unclassified\",\n  \"INSDC_biosample_identifiers\": [\n    \"biosample:SAMN06343863\"\n  ],\n  \"description\": \"Sterilized sand packs were incubated back in the ground and collected at time point T2.\",\n  \"collection_date\": {\n    \"has_raw_value\": \"2014-09-23\"\n  },\n  \"ecosystem_type\": \"Sand microcosm\",\n  \"sample_collection_site\": \"sand microcosm\",\n  \"name\": \"Sand microcosm microbial communities from a hyporheic zone in Columbia River, Washington, USA - GW-RW T2_23-Sept-14\",\n  \"lat_lon\": {\n    \"has_raw_value\": \"46.37228379 -119.2717467\",\n    \"latitude\": 46.37228379,\n    \"longitude\": -119.2717467\n  },\n  \"specific_ecosystem\": \"Unclassified\",\n  \"identifier\": \"GW-RW T2_23-Sept-14\",\n  \"GOLD_sample_identifiers\": [\n    \"gold:Gb0115217\"\n  ],\n  \"add_date\": \"2015-05-28\",\n  \"habitat\": \"sand microcosm\",\n  \"type\": \"nmdc:Biosample\",\n  \"depth\": {\n    \"has_raw_value\": \"0.5\",\n    \"has_numeric_value\": 0.5,\n    \"has_unit\": \"meter\"\n  },\n  \"part_of\": [\n    \"gold:Gs0114663\"\n  ],\n  \"ncbi_taxonomy_name\": \"sediment metagenome\",\n  \"geo_loc_name\": {\n    \"has_raw_value\": \"USA: Columbia River, Washington\"\n  }\n}\n</code></pre> <p>Now, copying and paste the above into the request body for <code>POST /metadata/json:validate</code>. Remember, the body needs to be a nmdc:Database object, in this case with a single member of the biosample_set collection, so copy and paste the <code>{\"biosample_set\": [</code> and <code>]}</code> parts to book-end the document JSON:</p> <pre><code>{\"biosample_set\": [\n\"PASTE_JSON_DOCUMENT_HERE\"\n]}\n</code></pre> <p>Now, when you execute the request, the response body will be</p> <pre><code>{\n  \"result\": \"All Okay!\"\n}\n</code></pre> <p>Hooray!</p>"},{"location":"tutorials/json/#get-a-list-of-nmdc-schema-compliant-documents","title":"Get a List of NMDC-Schema-Compliant Documents","text":"<p>The GET /nmdcschema/{collection_name} endpoint allows you to get a filtered list of documents from one of the NMDC Schema collections:</p> <p></p> <p>The <code>collection_name</code> must be one defined for a nmdc:Database, in the form expected by the JSON Schema, nmdc.schema.json. This typically means that any spaces in the name should be entered as underscores (<code>_</code>) instead.</p> <p>The <code>filter</code>, if provided, is a JSON document in the form of the MongoDB Query Language. For example, the filter <code>{\"part_of\": \"gold:Gs0114663\"}</code> on collection_name <code>biosample_set</code> will list biosamples that are part of the <code>gold:Gs0114663</code> study:</p> <p></p> <p>When I execute that query, I use the default <code>max_page_size</code> of 20, meaning at most 20 documents are returned at a time. A much larger <code>max_page_size</code> is fine for programs/scripts, but can make your web browser less responsive when using the interactive documentation.</p> <p>The response body for our request has two fields, <code>resources</code> and <code>next_page_token</code>:</p> <pre><code>{\n  \"resources\": [\n    ...\n  ],\n  \"next_page_token\": \"nmdc:sys0s8f846\"\n}\n</code></pre> <p><code>resources</code> is a list of documents. <code>next_page_token</code> is a value you can plug into a subsequent request as the <code>page_token</code> parameter:</p> <p></p> <p>This will return the next page of results. You do need to keep the other request parameters the same. In this way, you can page through and retrieve all documents that match a given filter (or no filter) for a given collection. Page tokens are ephemeral: once you use one in a request, it is removed from the system's memory.</p>"},{"location":"tutorials/metadata-in/","title":"Submit Metadata as JSON","text":""},{"location":"tutorials/metadata-in/#create-a-data-repository-service-drs-object","title":"Create a Data Repository Service (DRS) Object","text":"<p>I have created a public gist on Github via https://gist.github.com/ (a free service) for a fake biosample:</p> <pre><code>{\"biosample_set\": [{\n    \"id\": \"fake\",\n    \"env_broad_scale\" : {\n        \"term\" : {\"id\": \"ENVO:01000253\"}\n    }, \n    \"env_local_scale\" : {\n        \"term\" : {\"id\": \"ENVO:01000621\"}\n    }, \n    }, \n    \"env_medium\" : {\n        \"term\" : {\"id\": \"ENVO:01000017\"}\n    }}\n]}\n</code></pre> <p>We will use this example for the tutorial. The link to the gist is https://gist.github.com/dwinston/591afa6de4216d8fc4164c39f6418866, and if you click on the <code>Raw</code> button at the top-right corner of the code display window, you will get a URL for the raw data.</p> <p>To create an object in the API, we need at least a URL, file checksum, file size, and the time the file was created / last modified prior to registration with the API. It's also recommended that you include a file name (with file extension \".json\" in this case) and description.</p> <pre><code># Example of obtaining sha256 checksum and file size in bytes on the command line: \n$ wget https://gist.githubusercontent.com/dwinston/591afa6de4216d8fc4164c39f6418866/raw/4cc38cdf7b5edd9bb6a08897733346b62730002c/fake_biosample.json\n...\u2018fake_biosample.json\u2019 saved...\n$ openssl dgst -sha256 fake_biosample.json\nSHA256(fake_biosample.json)= c65ae13038ac980662472487a5a36cae23097e9c164fcbf0877b52b957d7faa7\n$ stat -f \"%z bytes\" fake_biosample.json\n260 bytes\n</code></pre> <pre><code>{\n  \"description\": \"A fake biosample.\",\n  \"name\": \"fake_biosample.json\",\n  \"access_methods\": [\n    {\n      \"access_url\": {\n        \"url\": \"https://gist.githubusercontent.com/dwinston/591afa6de4216d8fc4164c39f6418866/raw/4cc38cdf7b5edd9bb6a08897733346b62730002c/fake_biosample.json\"\n      }\n    }\n  ],\n  \"checksums\": [\n    {\n      \"checksum\": \"c65ae13038ac980662472487a5a36cae23097e9c164fcbf0877b52b957d7faa7\",\n      \"type\": \"sha256\"\n    }\n  ],\n  \"size\": 260,\n  \"created_time\": \"2021-11-17T10:30:00-05:00\"\n}\n</code></pre> <p>After a POST /objects with the above as the request body, I get back a response object with an <code>id</code> that looks like <code>sys0***</code>.</p>"},{"location":"tutorials/metadata-in/#annotate-the-drs-object-with-the-metadata-in-type","title":"Annotate the DRS object with the \"metadata-in\" type","text":"<p>Now, go to PUT /objects/{object_id}/types and ensure the types array for the object is <code>[\"metadata-in\"]</code>. This lets the Runtime know that you intend for this object to be ingested as NMDC metadata.</p> <p></p>"},{"location":"tutorials/metadata-in/#monitor-the-progress-of-metadata-ingest","title":"Monitor the progress of metadata ingest","text":"<p>After tagging the DRS Object as \"metadata-in\", the NMDC Runtime site senses that a new \"metadata-in-1.0.0\" job should be run given your DRS object <code>id</code> as input. You can then monitor runs of the job.</p> <p>In general, https://dagit-readonly.nmdc-runtime-dev.polyneme.xyz/instance/runs will give you an overview of the NMDC Runtime site's job runs. If you have the username and password (ask <code>dehays</code>), you can administer the underlying Dagster orchestrator via its Dagit web UI via https://dagit.dev.microbiomedata.org/.</p> <p>Note</p> <p>The read-only version is hosted at NERSC in the same kubernetes pod as the read-write version -- we just haven't gotten around to getting a SSL certificate for a <code>*.mirobiomedata.org</code> subdomain.</p> <p>Here's an example of the general Runs view after our new metadata has been ingested:</p> <p></p> <p>And indeed it has been ingested! See https://api.microbiomedata.org/nmdcschema/biosample_set/fake (unless we already deleted it -- see section below).</p>"},{"location":"tutorials/metadata-in/#removing-a-json-document","title":"Removing a JSON document","text":"<p>Info</p> <p>You must be authorized to do this. Specifically, a document of the form  <pre><code>{\n    \"username\" : &lt;YOUR_USERNAME&gt;,\n    \"action\" : \"/queries:run(query_cmd:DeleteCommand)\",\n}\n</code></pre> must be present in the <code>_runtime.api.allow</code> database collection. Ask a database administrator to be added. Any to-be-deleted documents are backed up to a separate database immediately prior to deletion.</p> <p>A call to POST /queries:run with the body</p> <pre><code>{\n  \"delete\": \"biosample_set\",\n  \"deletes\": [{\"q\": {\"id\": \"fake\"}, \"limit\": 1}]\n}\n</code></pre> <p>will remove our fake document. Note that you need to be logged in as a user, NOT as a site client, to execute such a request. The syntax for the request is a subset of the MongoDB <code>delete</code> command syntax.</p> <p>Danger</p> <p>People who can delete documents can currently delete a document from any collection.  <pre><code>{\n  \"delete\": \"users\",\n  \"deletes\": [{\"q\": {\"username\": \"useridontlike\"}, \"limit\": 1}]\n}\n</code></pre></p>"},{"location":"tutorials/translators/","title":"Metadata Ingest Pipelines (Translators)","text":""},{"location":"tutorials/translators/#introduction","title":"Introduction","text":"<p>Metadata ingest pipelines (often referred to as translators) are pipelines that are run when we want to bring in (ingest) metadata (biosample, lab processing metadata, etc.) into the NMDC database. Ingest sources that are currently supported, i.e., sources that we currently have pipelines for are:</p> <ul> <li>NMDC Submission Portal</li> <li>JGI GOLD</li> <li>NSF NEON<ul> <li>Soil Data Product (DP1.10086.001 and DP1.10107.001)</li> <li>Benthic Water Data Product (DP1.20279.001)</li> <li>Surface Water Data Product (DP1.20281.001)</li> </ul> </li> </ul>"},{"location":"tutorials/translators/#procedure","title":"Procedure","text":"<p>The steps below describe how to run the metadata ingest pipelines (translators).</p> <ol> <li>Go to either the development or production Dagit interface: https://dagit-dev.microbiomedata.org/ or https://dagit.microbiomedata.org/. If you're running Dagster locally through Docker containers, then go to http://localhost:3000/.</li> <li>Find the hamburger button (<code>\u2630</code>) to the left, and a side panel should open up.</li> <li>You will see a section in that panel called <code>biosample_submission_ingest@nmdc_runtime.site.repository:biosample_submission_ingest</code> under which you will see all the translator pipelines that you can run.</li> <li></li> <li>Select any of the above pipelines, and click on the Launchpad tab/section.</li> <li>You will see different configuration parameters that you need to fill out for different pipelines.</li> <li>Once you're done filling out the configuration parameters, you can hit the Launch Run button at the bottom right of the screen, and monitor the progress of your run.</li> <li>You can also track the history of all your past runs in the Runs tab/section.</li> <li>For some of the translators you will notice that they are in pairs (prefixed with translate_ and ingest_). The reason is that the translate_ prefixed pipelines are intended to be run so that we have the output (JSON, etc.) rendered in Dagit and we can look at it there. As for the ingest_ prefixed pipelines, the idea is that when they are run, the produced output is also submitted/ingested into the NMDC database.</li> <li>If we are running the translate_ prefixed pipelines, the output can normally be previewed by finding and clicking the preview link on the right console log step. You can copy the text from the preview pane and either paste it directy into the request body box for any of the endpoints in the Swagger UI. Or you can paste the text into a file with the right extension and submit (\"upload\") it to the NMDC database using an appropriate endpoint.</li> <li>Like for example, if the translator output is JSON text, you can copy and paste the result into a JSON file. Then you can use the /metadata/json:submit endpoint to submit that JSON data to the database.</li> </ol>"},{"location":"tutorials/translators/#additional-details","title":"Additional Details","text":"<ul> <li><code>gold_study_to_database</code>: translator to help you ingest metadata from a given study in GOLD. GOLD studies are prefixed with Gs, for example, <code>Gs0156736</code>. You need to plug in this study id in the configuration file for the GOLD translator and launch the job/run.</li> <li><code>translate_metadata_submission_to_nmdc_schema_database</code>: translator to help you ingest metadata from the NMDC Submission Portal. To run this you need to know the submission id of the submission you are trying to ingest. Typically that can be found in the URL bar of the study you are editing.</li> <li><code>ingest_metadata_submission</code>: this translator is the direct ingest into database equivalent of the above translator.</li> <li><code>translate_neon_api_soil_metadata_to_nmdc_schema_database</code>: translator to help you ingest metadata from the NEON soil data products DP1.10086.001 and DP1.10107.001.</li> <li><code>ingest_neon_soil_metadata</code>: this translator is the direct ingest into database equivalent of the above translator.</li> <li><code>translate_neon_api_benthic_metadata_to_nmdc_schema_database</code>: translator to help you ingest metadata from the NEON benthic data product DP1.20279.001.</li> <li><code>ingest_neon_benthic_metadata</code>: this translator is the direct ingest into database equivalent of the above translator.</li> <li><code>translate_neon_api_surface_water_metadata_to_nmdc_schema_database</code>: translator to help you ingest metadata from the NEON surface water data product DP1.20281.001.</li> <li><code>ingest_neon_surface_water_metadata</code>: this translator is the direct ingest into database equivalent of the above translator.</li> </ul>"}]}